{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"frontend/requirements/","text":"Requirements \u00b6 To use Pinferencia's frontend with your model, there are some requirements of your model's predict function. Templates \u00b6 Currently, there are mainly two major catogory of the template's inputs and outputs. More (audio and video) will be supported in the future. Base Templates \u00b6 Template Input Output Text to Text Text Text Text to Image Text Image Image to Text Image Text Camera to Text Image Text Image to Image Image Image Camera to Image Image Image Derived Templates \u00b6 Template Input Output Translation Text Text Image Classification Image Text Image Style Transfer Image Image Input \u00b6 The predict function must be able to accept a list of data as inputs. For text input, the input will be a list of strings. For image input, the input will be a list of strings representing the base64 encoded images. Output \u00b6 The predict function must produce a list of data as outputs. For text output, the output must be a list. For image output, the output must be a list of strings representing the base64 encoded images. Text Output The frontend will try to parse the outputs into table, json or pure text. Table Text JSON If the output is similar to below: [ [ { \"a\" : 1 , \"b\" : 2 }, { \"a\" : 3 , \"b\" : 4 }, { \"a\" : 5 , \"b\" : 6 } ] ] It will be displayed as a table. If the output is similar to below: [ \"Text output.\" ] It will be displayed as a text. All other format of outputs will be displayed as a JSON.","title":"Frontend Requirements"},{"location":"frontend/requirements/#requirements","text":"To use Pinferencia's frontend with your model, there are some requirements of your model's predict function.","title":"Requirements"},{"location":"frontend/requirements/#templates","text":"Currently, there are mainly two major catogory of the template's inputs and outputs. More (audio and video) will be supported in the future.","title":"Templates"},{"location":"frontend/requirements/#base-templates","text":"Template Input Output Text to Text Text Text Text to Image Text Image Image to Text Image Text Camera to Text Image Text Image to Image Image Image Camera to Image Image Image","title":"Base Templates"},{"location":"frontend/requirements/#derived-templates","text":"Template Input Output Translation Text Text Image Classification Image Text Image Style Transfer Image Image","title":"Derived Templates"},{"location":"frontend/requirements/#input","text":"The predict function must be able to accept a list of data as inputs. For text input, the input will be a list of strings. For image input, the input will be a list of strings representing the base64 encoded images.","title":"Input"},{"location":"frontend/requirements/#output","text":"The predict function must produce a list of data as outputs. For text output, the output must be a list. For image output, the output must be a list of strings representing the base64 encoded images. Text Output The frontend will try to parse the outputs into table, json or pure text. Table Text JSON If the output is similar to below: [ [ { \"a\" : 1 , \"b\" : 2 }, { \"a\" : 3 , \"b\" : 4 }, { \"a\" : 5 , \"b\" : 6 } ] ] It will be displayed as a table. If the output is similar to below: [ \"Text output.\" ] It will be displayed as a text. All other format of outputs will be displayed as a JSON.","title":"Output"},{"location":"get-started/home/","text":"Get Started \u00b6 In this series of tutorials, you will master how to serve: a custom model : a simple JSON model a custom function PyTorch MNIST model with two methods and some fun with the MNIST model","title":"Introduction"},{"location":"get-started/home/#get-started","text":"In this series of tutorials, you will master how to serve: a custom model : a simple JSON model a custom function PyTorch MNIST model with two methods and some fun with the MNIST model","title":"Get Started"},{"location":"get-started/other-models/","text":"What's Next \u00b6 Well, I bet you had some fun with the PyTorch MNIST model in the previous tutorial. I think you are familiar with Pinferencia now. Pinferencia can serve any callable object in a very straight-forward way. No complications. And it's easy to integrate with your existing codes. That is what Pinferencia is designed for. Minimum codes modifications . Now you can serve models from any framework , and you can even mix up them together . You can have an API using different models from different frameworks at the same time ! Enjoy yourself! If you like Pinferencia , don't forget to go to Github and give a star. Thank you. If you want to explore more examples on different machine learning models, you can find the inside the Example section from the navigation panel on your left.","title":"What's Next"},{"location":"get-started/other-models/#whats-next","text":"Well, I bet you had some fun with the PyTorch MNIST model in the previous tutorial. I think you are familiar with Pinferencia now. Pinferencia can serve any callable object in a very straight-forward way. No complications. And it's easy to integrate with your existing codes. That is what Pinferencia is designed for. Minimum codes modifications . Now you can serve models from any framework , and you can even mix up them together . You can have an API using different models from different frameworks at the same time ! Enjoy yourself! If you like Pinferencia , don't forget to go to Github and give a star. Thank you. If you want to explore more examples on different machine learning models, you can find the inside the Example section from the navigation panel on your left.","title":"What's Next"},{"location":"get-started/pytorch-mnist/","text":"Serve PyTorch MNIST Model \u00b6 In this tutorial, we will serve a PyTorch MNIST model. It receives a Base64 encoded image as request data, and return the prediction in the response. Prerequisite \u00b6 Visit PyTorch Examples - MNIST , download the files. Run below commands to install and train the model: pip install -r requirements.txt python main.py --save-model After the training is finished, you will have a folder structure as below. A mnist_cnn.pt file is created . \u251c\u2500\u2500 README.md \u251c\u2500\u2500 main.py \u251c\u2500\u2500 mnist_cnn.pt \u2514\u2500\u2500 requirements.txt Deploy Methods \u00b6 There are two methods you can deploy the model. Directly register a function. Only register a model path, with an additioanl handler. We will cover both methods step by step in this tutorial. Directly Register a Function \u00b6 Create the App \u00b6 Let's create a file func_app.py in the same folder. func_app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 import base64 from io import BytesIO import torch from main import Net # (1) from PIL import Image from torchvision import transforms from pinferencia import Server use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) # (2) model = Net () . to ( device ) # (3) model . load_state_dict ( torch . load ( \"mnist_cnn.pt\" )) model . eval () def preprocessing ( img_str ): image = Image . open ( BytesIO ( base64 . b64decode ( img_str ))) tensor = transform ( image ) return torch . stack ([ tensor ]) . to ( device ) def predict ( data ): return model ( preprocessing ( data )) . argmax ( 1 ) . tolist ()[ 0 ] service = Server () # (4) service . register ( model_name = \"mnist\" , model = predict ) Make suer you can import the Net Model. Preprocessing transformation codes. The example script only save the state_dict . Here we need to initialize the model and load the state_dict . Get ready, 3, 2, 1. GO! Start the Service \u00b6 $ uvicorn func_app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. Test the Service \u00b6 Test Data? Because our input is a base64 encoded MNIST image, where can we get these data? You can make use of PyTorch's datasets. Create a file with in the same folder named get-base64-img . get-base64-img.py import base64 import random from io import BytesIO from PIL import Image from torchvision import datasets dataset = datasets . MNIST ( # (1) \"./data\" , train = True , download = True , transform = None , ) index = random . randint ( 0 , len ( dataset . data )) # (2) img = dataset . data [ index ] img = Image . fromarray ( img . numpy (), mode = \"L\" ) buffered = BytesIO () img . save ( buffered , format = \"JPEG\" ) base64_img_str = base64 . b64encode ( buffered . getvalue ()) . decode () print ( \"Base64 String:\" , base64_img_str ) # (3) print ( \"target:\" , dataset . targets [ index ] . tolist ()) This is the MNIST dataset used during training. Let's use a random image. The string and the target are printed to stdout. Run the script and copy the string. python get-base64-img.py Output: Base64 String: /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+uhfwXqy2Ph25VYnPiB3SzhUkPlXCfNkAAEsCCCeOeKx9RsLjStUu9Ou1C3NpM8Eqg5AdSVIz35FVqK9xl0HXhb/C20sdMubjTLMQXs11AhkRXmmDsCwzgAYPpz+XI/GrSLrTfiVqNzPapbw3xE8AWQNvUAKXOOmWVjg+teeUV2fgXxd4hsPE2hWEGuX8Vh9uhja3Fw3lbGcBhtzjGCad8XI7iL4p68twHDGcMm45+QqCuPbBFcVRRU97fXepXb3d9dT3VzJjfNPIXdsAAZY8nAAH4VBX/9k = target: 4 Let's create a file test.py test.py 1 2 3 4 5 6 7 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/mnist/predict\" , json = { \"data\" : \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+uhfwXqy2Ph25VYnPiB3SzhUkPlXCfNkAAEsCCCeOeKx9RsLjStUu9Ou1C3NpM8Eqg5AdSVIz35FVqK9xl0HXhb/C20sdMubjTLMQXs11AhkRXmmDsCwzgAYPpz+XI/GrSLrTfiVqNzPapbw3xE8AWQNvUAKXOOmWVjg+teeUV2fgXxd4hsPE2hWEGuX8Vh9uhja3Fw3lbGcBhtzjGCad8XI7iL4p68twHDGcMm45+QqCuPbBFcVRRU97fXepXb3d9dT3VzJjfNPIXdsAAZY8nAAH4VBX/9k=\" }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) Run the test: $ python test.py Prediction: 4 You can try out the API with more images, or even using the interactive API documentation page http://127.0.0.1 Register a Model Path, with a Handler \u00b6 Handler If you prefer the old classical way of serving a model with a file, using a handler is your choice. For details of handlers, please visit Handlers Create the App \u00b6 Let's create a file func_app.py in the same folder. The codes below are refactored into a handle class. It looks cleaner! path_app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 import base64 import pathlib from io import BytesIO import torch from main import Net from PIL import Image from torchvision import transforms from pinferencia import Server from pinferencia.handlers import BaseHandler class MNISTHandler ( BaseHandler ): transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) def load_model ( self ): # (1) model = Net () . to ( self . device ) model . load_state_dict ( torch . load ( self . model_path )) model . eval () return model def predict ( self , data ): # (2) image = Image . open ( BytesIO ( base64 . b64decode ( data ))) tensor = self . transform ( image ) input_data = torch . stack ([ tensor ]) . to ( self . device ) return self . model ( input_data ) . argmax ( 1 ) . tolist ()[ 0 ] service = Server ( model_dir = pathlib . Path ( __file__ ) . parent . resolve ()) # (3) service . register ( model_name = \"mnist\" , model = \"mnist_cnn.pt\" , handler = MNISTHandler , load_now = True , # (4) ) We move the codes of loading the model into the load_model function. The model path can be accessed by self.model_path . We move the codes of predicting into the predict function. The model can be accessed by self.model . model_dir is where Pinferencia will look for your model files. Set the model_dir to the folder having the mnist_cnn.pt and this script. load_now determine if the model will be get loaded immediately during registration. The default is True . If set to False , you need to call the load API to load the model before prediction. Start the Service \u00b6 $ uvicorn path_app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. Test the Service \u00b6 Run the test: $ python test.py Prediction: 4 No suprise, the same result. Finally \u00b6 Using Pinferencia , you can serve any model. You can load the models by yourself, just what you have done to do a offline prediction. The codes are already there. Then, just register the model using Pinferencia , and your model is alive. Alternatively, you can choose to refactor your codes into a Handler Class . The old classic way also works with Pinferencia . Both worlds work for your model, classic music and rock'n'roll . Isn't it great! Now you have mastered how to use Pinferencia to: Register any model, any function and serve them. Use your custom handler to serve your machine learning model. If you still have time, let's try something fun. Extra: Sum Up the MNIST Images \u00b6 Let's create a sum_mnist.py . It accepts an array of images, predicts their digits and sum up them. sum_mnist.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 import base64 import pathlib from io import BytesIO import torch from main import Net from PIL import Image from torchvision import transforms from pinferencia import Server from pinferencia.handlers import BaseHandler class MNISTHandler ( BaseHandler ): transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) def load_model ( self ): model = Net () . to ( self . device ) model . load_state_dict ( torch . load ( self . model_path )) model . eval () return model def predict ( self , data ): tensors = [] # (1) for img in data : image = Image . open ( BytesIO ( base64 . b64decode ( img ))) tensors . append ( self . transform ( image )) input_data = torch . stack ( tensors ) . to ( self . device ) return sum ( self . model ( input_data ) . argmax ( 1 ) . tolist ()) service = Server ( model_dir = pathlib . Path ( __file__ ) . parent . resolve ()) service . register ( model_name = \"mnist\" , model = \"mnist_cnn.pt\" , handler = MNISTHandler , load_now = True , ) Here we pre-process each image, predict its digit and sum up. Have fun with Pinferencia !","title":"Serve PyTorch MNIST Model"},{"location":"get-started/pytorch-mnist/#serve-pytorch-mnist-model","text":"In this tutorial, we will serve a PyTorch MNIST model. It receives a Base64 encoded image as request data, and return the prediction in the response.","title":"Serve PyTorch MNIST Model"},{"location":"get-started/pytorch-mnist/#prerequisite","text":"Visit PyTorch Examples - MNIST , download the files. Run below commands to install and train the model: pip install -r requirements.txt python main.py --save-model After the training is finished, you will have a folder structure as below. A mnist_cnn.pt file is created . \u251c\u2500\u2500 README.md \u251c\u2500\u2500 main.py \u251c\u2500\u2500 mnist_cnn.pt \u2514\u2500\u2500 requirements.txt","title":"Prerequisite"},{"location":"get-started/pytorch-mnist/#deploy-methods","text":"There are two methods you can deploy the model. Directly register a function. Only register a model path, with an additioanl handler. We will cover both methods step by step in this tutorial.","title":"Deploy Methods"},{"location":"get-started/pytorch-mnist/#directly-register-a-function","text":"","title":"Directly Register a Function"},{"location":"get-started/pytorch-mnist/#create-the-app","text":"Let's create a file func_app.py in the same folder. func_app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 import base64 from io import BytesIO import torch from main import Net # (1) from PIL import Image from torchvision import transforms from pinferencia import Server use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) # (2) model = Net () . to ( device ) # (3) model . load_state_dict ( torch . load ( \"mnist_cnn.pt\" )) model . eval () def preprocessing ( img_str ): image = Image . open ( BytesIO ( base64 . b64decode ( img_str ))) tensor = transform ( image ) return torch . stack ([ tensor ]) . to ( device ) def predict ( data ): return model ( preprocessing ( data )) . argmax ( 1 ) . tolist ()[ 0 ] service = Server () # (4) service . register ( model_name = \"mnist\" , model = predict ) Make suer you can import the Net Model. Preprocessing transformation codes. The example script only save the state_dict . Here we need to initialize the model and load the state_dict . Get ready, 3, 2, 1. GO!","title":"Create the App"},{"location":"get-started/pytorch-mnist/#start-the-service","text":"$ uvicorn func_app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete.","title":"Start the Service"},{"location":"get-started/pytorch-mnist/#test-the-service","text":"Test Data? Because our input is a base64 encoded MNIST image, where can we get these data? You can make use of PyTorch's datasets. Create a file with in the same folder named get-base64-img . get-base64-img.py import base64 import random from io import BytesIO from PIL import Image from torchvision import datasets dataset = datasets . MNIST ( # (1) \"./data\" , train = True , download = True , transform = None , ) index = random . randint ( 0 , len ( dataset . data )) # (2) img = dataset . data [ index ] img = Image . fromarray ( img . numpy (), mode = \"L\" ) buffered = BytesIO () img . save ( buffered , format = \"JPEG\" ) base64_img_str = base64 . b64encode ( buffered . getvalue ()) . decode () print ( \"Base64 String:\" , base64_img_str ) # (3) print ( \"target:\" , dataset . targets [ index ] . tolist ()) This is the MNIST dataset used during training. Let's use a random image. The string and the target are printed to stdout. Run the script and copy the string. python get-base64-img.py Output: Base64 String: /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+uhfwXqy2Ph25VYnPiB3SzhUkPlXCfNkAAEsCCCeOeKx9RsLjStUu9Ou1C3NpM8Eqg5AdSVIz35FVqK9xl0HXhb/C20sdMubjTLMQXs11AhkRXmmDsCwzgAYPpz+XI/GrSLrTfiVqNzPapbw3xE8AWQNvUAKXOOmWVjg+teeUV2fgXxd4hsPE2hWEGuX8Vh9uhja3Fw3lbGcBhtzjGCad8XI7iL4p68twHDGcMm45+QqCuPbBFcVRRU97fXepXb3d9dT3VzJjfNPIXdsAAZY8nAAH4VBX/9k = target: 4 Let's create a file test.py test.py 1 2 3 4 5 6 7 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/mnist/predict\" , json = { \"data\" : \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+uhfwXqy2Ph25VYnPiB3SzhUkPlXCfNkAAEsCCCeOeKx9RsLjStUu9Ou1C3NpM8Eqg5AdSVIz35FVqK9xl0HXhb/C20sdMubjTLMQXs11AhkRXmmDsCwzgAYPpz+XI/GrSLrTfiVqNzPapbw3xE8AWQNvUAKXOOmWVjg+teeUV2fgXxd4hsPE2hWEGuX8Vh9uhja3Fw3lbGcBhtzjGCad8XI7iL4p68twHDGcMm45+QqCuPbBFcVRRU97fXepXb3d9dT3VzJjfNPIXdsAAZY8nAAH4VBX/9k=\" }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) Run the test: $ python test.py Prediction: 4 You can try out the API with more images, or even using the interactive API documentation page http://127.0.0.1","title":"Test the Service"},{"location":"get-started/pytorch-mnist/#register-a-model-path-with-a-handler","text":"Handler If you prefer the old classical way of serving a model with a file, using a handler is your choice. For details of handlers, please visit Handlers","title":"Register a Model Path, with a Handler"},{"location":"get-started/pytorch-mnist/#create-the-app_1","text":"Let's create a file func_app.py in the same folder. The codes below are refactored into a handle class. It looks cleaner! path_app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 import base64 import pathlib from io import BytesIO import torch from main import Net from PIL import Image from torchvision import transforms from pinferencia import Server from pinferencia.handlers import BaseHandler class MNISTHandler ( BaseHandler ): transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) def load_model ( self ): # (1) model = Net () . to ( self . device ) model . load_state_dict ( torch . load ( self . model_path )) model . eval () return model def predict ( self , data ): # (2) image = Image . open ( BytesIO ( base64 . b64decode ( data ))) tensor = self . transform ( image ) input_data = torch . stack ([ tensor ]) . to ( self . device ) return self . model ( input_data ) . argmax ( 1 ) . tolist ()[ 0 ] service = Server ( model_dir = pathlib . Path ( __file__ ) . parent . resolve ()) # (3) service . register ( model_name = \"mnist\" , model = \"mnist_cnn.pt\" , handler = MNISTHandler , load_now = True , # (4) ) We move the codes of loading the model into the load_model function. The model path can be accessed by self.model_path . We move the codes of predicting into the predict function. The model can be accessed by self.model . model_dir is where Pinferencia will look for your model files. Set the model_dir to the folder having the mnist_cnn.pt and this script. load_now determine if the model will be get loaded immediately during registration. The default is True . If set to False , you need to call the load API to load the model before prediction.","title":"Create the App"},{"location":"get-started/pytorch-mnist/#start-the-service_1","text":"$ uvicorn path_app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete.","title":"Start the Service"},{"location":"get-started/pytorch-mnist/#test-the-service_1","text":"Run the test: $ python test.py Prediction: 4 No suprise, the same result.","title":"Test the Service"},{"location":"get-started/pytorch-mnist/#finally","text":"Using Pinferencia , you can serve any model. You can load the models by yourself, just what you have done to do a offline prediction. The codes are already there. Then, just register the model using Pinferencia , and your model is alive. Alternatively, you can choose to refactor your codes into a Handler Class . The old classic way also works with Pinferencia . Both worlds work for your model, classic music and rock'n'roll . Isn't it great! Now you have mastered how to use Pinferencia to: Register any model, any function and serve them. Use your custom handler to serve your machine learning model. If you still have time, let's try something fun.","title":"Finally"},{"location":"get-started/pytorch-mnist/#extra-sum-up-the-mnist-images","text":"Let's create a sum_mnist.py . It accepts an array of images, predicts their digits and sum up them. sum_mnist.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 import base64 import pathlib from io import BytesIO import torch from main import Net from PIL import Image from torchvision import transforms from pinferencia import Server from pinferencia.handlers import BaseHandler class MNISTHandler ( BaseHandler ): transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) def load_model ( self ): model = Net () . to ( self . device ) model . load_state_dict ( torch . load ( self . model_path )) model . eval () return model def predict ( self , data ): tensors = [] # (1) for img in data : image = Image . open ( BytesIO ( base64 . b64decode ( img ))) tensors . append ( self . transform ( image )) input_data = torch . stack ( tensors ) . to ( self . device ) return sum ( self . model ( input_data ) . argmax ( 1 ) . tolist ()) service = Server ( model_dir = pathlib . Path ( __file__ ) . parent . resolve ()) service . register ( model_name = \"mnist\" , model = \"mnist_cnn.pt\" , handler = MNISTHandler , load_now = True , ) Here we pre-process each image, predict its digit and sum up. Have fun with Pinferencia !","title":"Extra: Sum Up the MNIST Images"},{"location":"get-started/serve-a-function/","text":"Serve a Function \u00b6 Well, serving a function? Is it useful? Of course it is. If you have a whole workflow of inferences , it consists of many steps. Most of the time, you will implement a function to do this job. Now you can register the function immediately. If you want to share some pre-processing or post-processing functions, now you've got your Robin, Batman ! Or a function is just enough for your job. Mission \u00b6 We're given a list of mountains' heights. We need to find out the highest, the loweset, and the difference between the highest and the lowest. It's a simple problem, let's solve it in a function to get you familiar with the concept: graph LR heights(Mountains' Heights) --> max(Find Out the Highest) heights --> min(Find Out the Lowest) min --> diff(Calculate the Difference) max --> diff diff --> output(Output) subgraph Workflow max min diff end Create the Service and Register the Model \u00b6 Save the following codes in app.py . app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from typing import List from pinferencia import Server def calc ( data : List [ int ]) -> int : highest = max ( data ) lowest = min ( data ) return highest - lowest service = Server () service . register ( model_name = \"mountain\" , model = calc ) Start the Server \u00b6 $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) Test the API \u00b6 Create a test.py with the codes below. Tips You need to have requests installed. pip install requests test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/mountain/predict\" , json = { \"data\" : [ 1000 , 2000 , 3000 ]}, ) difference = response . json ()[ \"data\" ] print ( f \"Difference between the highest and lowest is { difference } m.\" ) Run the script and check the result. $ python test.py Difference between the highest and lowest is 2000m. Further more \u00b6 So now you have learned how to serve a model define as a Class or a Function . If you have just a single model to serve, it's easy-peasy. But in real world, you have custom codes like pre-processing and post-processing. And some tasks need multiple models to work together. For example, if you want to predict an animal's breed, you may need the below workflow: graph LR pic(Picture) --> species(Species Classification) species --> cat(Cat) --> cat_breed(Cat Breed Classification) --> Persian(Persian) species --> dog(Dog) --> dog_breed(Dog Breed Classification) --> Labrador(Labrador) species --> monkey(Monkey) --> monkey_breed(Monkey Breed Classification) --> spider(Spider Monkeys) Deploying this on many platform or tools aren't that easy. However, now you have Pinferencia , you have a choice!","title":"Serve a Function"},{"location":"get-started/serve-a-function/#serve-a-function","text":"Well, serving a function? Is it useful? Of course it is. If you have a whole workflow of inferences , it consists of many steps. Most of the time, you will implement a function to do this job. Now you can register the function immediately. If you want to share some pre-processing or post-processing functions, now you've got your Robin, Batman ! Or a function is just enough for your job.","title":"Serve a Function"},{"location":"get-started/serve-a-function/#mission","text":"We're given a list of mountains' heights. We need to find out the highest, the loweset, and the difference between the highest and the lowest. It's a simple problem, let's solve it in a function to get you familiar with the concept: graph LR heights(Mountains' Heights) --> max(Find Out the Highest) heights --> min(Find Out the Lowest) min --> diff(Calculate the Difference) max --> diff diff --> output(Output) subgraph Workflow max min diff end","title":"Mission"},{"location":"get-started/serve-a-function/#create-the-service-and-register-the-model","text":"Save the following codes in app.py . app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from typing import List from pinferencia import Server def calc ( data : List [ int ]) -> int : highest = max ( data ) lowest = min ( data ) return highest - lowest service = Server () service . register ( model_name = \"mountain\" , model = calc )","title":"Create the Service and Register the Model"},{"location":"get-started/serve-a-function/#start-the-server","text":"$ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)","title":"Start the Server"},{"location":"get-started/serve-a-function/#test-the-api","text":"Create a test.py with the codes below. Tips You need to have requests installed. pip install requests test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/mountain/predict\" , json = { \"data\" : [ 1000 , 2000 , 3000 ]}, ) difference = response . json ()[ \"data\" ] print ( f \"Difference between the highest and lowest is { difference } m.\" ) Run the script and check the result. $ python test.py Difference between the highest and lowest is 2000m.","title":"Test the API"},{"location":"get-started/serve-a-function/#further-more","text":"So now you have learned how to serve a model define as a Class or a Function . If you have just a single model to serve, it's easy-peasy. But in real world, you have custom codes like pre-processing and post-processing. And some tasks need multiple models to work together. For example, if you want to predict an animal's breed, you may need the below workflow: graph LR pic(Picture) --> species(Species Classification) species --> cat(Cat) --> cat_breed(Cat Breed Classification) --> Persian(Persian) species --> dog(Dog) --> dog_breed(Dog Breed Classification) --> Labrador(Labrador) species --> monkey(Monkey) --> monkey_breed(Monkey Breed Classification) --> spider(Spider Monkeys) Deploying this on many platform or tools aren't that easy. However, now you have Pinferencia , you have a choice!","title":"Further more"},{"location":"get-started/serve-a-json-model/","text":"Run a JSON Model \u00b6 Now let's first try something easy to get you familiar with Pinferecia . TL;DR It's important for you to understand how to register and serve a model in Pinferencia . However, if you want to try machine learning model now, you can jump to Serve Pytorch MNIST Model Define the JSON Model \u00b6 Let's create a file named app.py . Below is a JSON Model. It simply return 1 for input a , 2 for input b , and 0 for other inputs. app.py 1 2 3 4 class JSONModel : def predict ( self , data ): knowledge = { \"a\" : 1 , \"b\" : 2 } return knowledge . get ( data , 0 ) Create the Service and Register the Model \u00b6 First we import Server from pinferencia , then create an instance and register a instance of our JSON Model . app.py 1 2 3 4 5 6 7 8 9 10 11 12 from pinferencia import Server class JSONModel : def predict ( self , data : str ) -> int : knowledge = { \"a\" : 1 , \"b\" : 2 } return knowledge . get ( data , 0 ) model = JSONModel () service = Server () service . register ( model_name = \"json\" , model = model , entrypoint = \"predict\" ) What are the model_name and entrypoint here? model_name is the name you give to the model for later API access. Here we give the model a name json , and the url for this model is http://127.0.0.1:8000/v1/models/json . If you have any confusion about the APIs, you can always visit the documentation page mentioned in the next part. The entrypoint predict means we will use the predict function of JSON Model to predict the data. Start the Server \u00b6 $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) Open your browser and visit http://127.0.0.1:8000 , and now you have an automatically generated API Documentation page! FastAPI and Starlette Pinferencia builds on FastAPI which is built on Starlette . Thanks to them, you will have an API with OpenAPI Specification. It means you will have an automatic documentation webpage and client codes can also be generated automatically. Tips There are two API documentation endpoints: http://127.0.0.1:8000 or http://127.0.0.1:8000/docs http://127.0.0.1:8000/redoc You can view the API specifiacations and even try out the API by yourself! Test the API \u00b6 Create a test.py with the codes below. Tips You need to have requests installed. pip install requests test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/json/predict\" , json = { \"data\" : \"a\" }, ) print ( response . json ()) Run the script and check the result. $ python test.py {'model_name': 'json', 'data': 1} Now let's add two more inputs and make the print pretty. test.py 1 2 3 4 5 6 7 8 9 10 11 import requests print ( \"| {:^10} | {:^15} |\" . format ( \"Input\" , \"Prediction\" )) print ( \"| {:^10} | {:^15} |\" . format ( \"-\" * 10 , \"-\" * 15 )) for character in [ \"a\" , \"b\" , \"c\" ]: response = requests . post ( url = \"http://localhost:8000/v1/models/json/predict\" , json = { \"data\" : character }, ) print ( f \"| { character : ^10 } | { str ( response . json ()[ 'data' ]) : ^15 } |\" ) Run the script again and check the result. $ python test.py | Input | Prediction | |----------|---------------| | a | 1 | | b | 2 | | c | 0 |","title":"Serve a Simple JSON Model"},{"location":"get-started/serve-a-json-model/#run-a-json-model","text":"Now let's first try something easy to get you familiar with Pinferecia . TL;DR It's important for you to understand how to register and serve a model in Pinferencia . However, if you want to try machine learning model now, you can jump to Serve Pytorch MNIST Model","title":"Run a JSON Model"},{"location":"get-started/serve-a-json-model/#define-the-json-model","text":"Let's create a file named app.py . Below is a JSON Model. It simply return 1 for input a , 2 for input b , and 0 for other inputs. app.py 1 2 3 4 class JSONModel : def predict ( self , data ): knowledge = { \"a\" : 1 , \"b\" : 2 } return knowledge . get ( data , 0 )","title":"Define the JSON Model"},{"location":"get-started/serve-a-json-model/#create-the-service-and-register-the-model","text":"First we import Server from pinferencia , then create an instance and register a instance of our JSON Model . app.py 1 2 3 4 5 6 7 8 9 10 11 12 from pinferencia import Server class JSONModel : def predict ( self , data : str ) -> int : knowledge = { \"a\" : 1 , \"b\" : 2 } return knowledge . get ( data , 0 ) model = JSONModel () service = Server () service . register ( model_name = \"json\" , model = model , entrypoint = \"predict\" ) What are the model_name and entrypoint here? model_name is the name you give to the model for later API access. Here we give the model a name json , and the url for this model is http://127.0.0.1:8000/v1/models/json . If you have any confusion about the APIs, you can always visit the documentation page mentioned in the next part. The entrypoint predict means we will use the predict function of JSON Model to predict the data.","title":"Create the Service and Register the Model"},{"location":"get-started/serve-a-json-model/#start-the-server","text":"$ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) Open your browser and visit http://127.0.0.1:8000 , and now you have an automatically generated API Documentation page! FastAPI and Starlette Pinferencia builds on FastAPI which is built on Starlette . Thanks to them, you will have an API with OpenAPI Specification. It means you will have an automatic documentation webpage and client codes can also be generated automatically. Tips There are two API documentation endpoints: http://127.0.0.1:8000 or http://127.0.0.1:8000/docs http://127.0.0.1:8000/redoc You can view the API specifiacations and even try out the API by yourself!","title":"Start the Server"},{"location":"get-started/serve-a-json-model/#test-the-api","text":"Create a test.py with the codes below. Tips You need to have requests installed. pip install requests test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/json/predict\" , json = { \"data\" : \"a\" }, ) print ( response . json ()) Run the script and check the result. $ python test.py {'model_name': 'json', 'data': 1} Now let's add two more inputs and make the print pretty. test.py 1 2 3 4 5 6 7 8 9 10 11 import requests print ( \"| {:^10} | {:^15} |\" . format ( \"Input\" , \"Prediction\" )) print ( \"| {:^10} | {:^15} |\" . format ( \"-\" * 10 , \"-\" * 15 )) for character in [ \"a\" , \"b\" , \"c\" ]: response = requests . post ( url = \"http://localhost:8000/v1/models/json/predict\" , json = { \"data\" : character }, ) print ( f \"| { character : ^10 } | { str ( response . json ()[ 'data' ]) : ^15 } |\" ) Run the script again and check the result. $ python test.py | Input | Prediction | |----------|---------------| | a | 1 | | b | 2 | | c | 0 |","title":"Test the API"},{"location":"handlers/","text":"Handlers \u00b6 BaseHandler \u00b6 BaseHandler is only an abstract base class. You can't use it directly. Let's Take a look at some of its functions: BaseHandler 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class BaseHandler ( abc . ABC ): def preprocess ( self , data : object , parameters : object = None ): return data # (1) def postprocess ( self , data : object , parameters : object = None ): return data # (2) def predict ( self , data : object ): if not getattr ( self , \"model\" , None ): raise Exception ( \"Model is not loaded.\" ) predict_func = ( # (3) getattr ( self . model , self . entrypoint ) if self . entrypoint else self . model ) return predict_func ( data ) @abc . abstractmethod def load_model ( self ): return NotImplemented # (4) The default codes do nothing . You can override this function to provide your own pre-processing codes. The default codes do nothing . You can override this function to provide your own post-processing codes. Get predict function from entrypoint name and the model object. Model can be accessed by self.model , the entrypoint registered can be accessed by self.entrypoint . You need to implement this function. Model path can be accessed by self.model_path PickleHandler \u00b6 The default handler is PickleHandler . PickleHandler 1 2 3 4 5 6 7 8 class PickleHandler ( BaseHandler ): \"\"\"Pickle Handler for Models Saved through Pickle\"\"\" def load_model ( self ): if not getattr ( self , \"model_path\" , None ): raise Exception ( \"Model path not provided.\" ) with open ( self . model_path , \"rb\" ) as f : return pickle . load ( f )","title":"Handlers"},{"location":"handlers/#handlers","text":"","title":"Handlers"},{"location":"handlers/#basehandler","text":"BaseHandler is only an abstract base class. You can't use it directly. Let's Take a look at some of its functions: BaseHandler 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class BaseHandler ( abc . ABC ): def preprocess ( self , data : object , parameters : object = None ): return data # (1) def postprocess ( self , data : object , parameters : object = None ): return data # (2) def predict ( self , data : object ): if not getattr ( self , \"model\" , None ): raise Exception ( \"Model is not loaded.\" ) predict_func = ( # (3) getattr ( self . model , self . entrypoint ) if self . entrypoint else self . model ) return predict_func ( data ) @abc . abstractmethod def load_model ( self ): return NotImplemented # (4) The default codes do nothing . You can override this function to provide your own pre-processing codes. The default codes do nothing . You can override this function to provide your own post-processing codes. Get predict function from entrypoint name and the model object. Model can be accessed by self.model , the entrypoint registered can be accessed by self.entrypoint . You need to implement this function. Model path can be accessed by self.model_path","title":"BaseHandler"},{"location":"handlers/#picklehandler","text":"The default handler is PickleHandler . PickleHandler 1 2 3 4 5 6 7 8 class PickleHandler ( BaseHandler ): \"\"\"Pickle Handler for Models Saved through Pickle\"\"\" def load_model ( self ): if not getattr ( self , \"model_path\" , None ): raise Exception ( \"Model path not provided.\" ) with open ( self . model_path , \"rb\" ) as f : return pickle . load ( f )","title":"PickleHandler"},{"location":"install/","text":"Install Pinferencia \u00b6 Recommended \u00b6 It's recommended to install Pinferencia with uvicorn . $ pip install \"pinferencia[uvicorn]\" ---> 100% Alternatively \u00b6 You can also choose install Pinferencia without uvicorn , and install other ASGI server you prefer later. $ pip install pinferencia ---> 100%","title":"Install"},{"location":"install/#install-pinferencia","text":"","title":"Install Pinferencia"},{"location":"install/#recommended","text":"It's recommended to install Pinferencia with uvicorn . $ pip install \"pinferencia[uvicorn]\" ---> 100%","title":"Recommended"},{"location":"install/#alternatively","text":"You can also choose install Pinferencia without uvicorn , and install other ASGI server you prefer later. $ pip install pinferencia ---> 100%","title":"Alternatively"},{"location":"ml/huggingface/dependencies/","text":"For mac users \u00b6 If you're working on a M1 Mac like me, you need install cmake and rust brew install cmake curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh Install dependencies \u00b6 You can install dependencies using pip. pip install tqdm boto3 requests regex sentencepiece sacremoses or you can use a docker image instead: docker run -it -p 8000 :8000 -v $( pwd ) :/opt/workspace huggingface/transformers-pytorch-cpu:4.18.0 bash","title":"Install Dependencies"},{"location":"ml/huggingface/dependencies/#for-mac-users","text":"If you're working on a M1 Mac like me, you need install cmake and rust brew install cmake curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh","title":"For mac users"},{"location":"ml/huggingface/dependencies/#install-dependencies","text":"You can install dependencies using pip. pip install tqdm boto3 requests regex sentencepiece sacremoses or you can use a docker image instead: docker run -it -p 8000 :8000 -v $( pwd ) :/opt/workspace huggingface/transformers-pytorch-cpu:4.18.0 bash","title":"Install dependencies"},{"location":"ml/huggingface/pipeline/nlp/bert/","text":"Many of you must have heard of Bert , or transformers . And you may also know huggingface. In this tutorial, let's play with its pytorch transformer model and serve it through REST API How does the model work? \u00b6 With an input of an incomplete sentence, the model will give its prediction: Input Output Paris is the [MASK] of France. Paris is the capital of France. Let's try it out now Prerequisite \u00b6 Please visit Dependencies Serve the Model \u00b6 Install Pinferencia \u00b6 First, let's install Pinferencia . pip install \"pinferencia[uvicorn]\" Create app.py \u00b6 Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 from transformers import pipeline from pinferencia import Server bert = pipeline ( \"fill-mask\" , model = \"bert-base-uncased\" ) service = Server () service . register ( model_name = \"bert\" , model = lambda text : bert ( text )) Run the service, and wait for it to load the model and start the server: $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) Test the service \u00b6 curl Python Requests Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/bert/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"Paris is the [MASK] of France.\" }' Response { \"model_name\":\"bert\", \"data\":\"Paris is the capital of France.\" } Create the test.py . test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/bert/predict\" , json = { \"data\" : \"Paris is the [MASK] of France.\" }, ) print ( response . json ()) Run the script and check the result. $ python test.py {'model_name': 'bert', 'data': 'Paris is the capital of France.'}","title":"Bert"},{"location":"ml/huggingface/pipeline/nlp/bert/#how-does-the-model-work","text":"With an input of an incomplete sentence, the model will give its prediction: Input Output Paris is the [MASK] of France. Paris is the capital of France. Let's try it out now","title":"How does the model work?"},{"location":"ml/huggingface/pipeline/nlp/bert/#prerequisite","text":"Please visit Dependencies","title":"Prerequisite"},{"location":"ml/huggingface/pipeline/nlp/bert/#serve-the-model","text":"","title":"Serve the Model"},{"location":"ml/huggingface/pipeline/nlp/bert/#install-pinferencia","text":"First, let's install Pinferencia . pip install \"pinferencia[uvicorn]\"","title":"Install Pinferencia"},{"location":"ml/huggingface/pipeline/nlp/bert/#create-apppy","text":"Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 from transformers import pipeline from pinferencia import Server bert = pipeline ( \"fill-mask\" , model = \"bert-base-uncased\" ) service = Server () service . register ( model_name = \"bert\" , model = lambda text : bert ( text )) Run the service, and wait for it to load the model and start the server: $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)","title":"Create app.py"},{"location":"ml/huggingface/pipeline/nlp/bert/#test-the-service","text":"curl Python Requests Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/bert/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"Paris is the [MASK] of France.\" }' Response { \"model_name\":\"bert\", \"data\":\"Paris is the capital of France.\" } Create the test.py . test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/bert/predict\" , json = { \"data\" : \"Paris is the [MASK] of France.\" }, ) print ( response . json ()) Run the script and check the result. $ python test.py {'model_name': 'bert', 'data': 'Paris is the capital of France.'}","title":"Test the service"},{"location":"ml/huggingface/pipeline/nlp/text-generation/","text":"GPT2\u200a-\u200aText Generation Transformer: How to Use & How to Serve \u00b6 What is text generation? Input some texts, and the model will predict what the following texts will be. Sounds interesting. How can it be interesting without trying out the model by ourself? How to Use \u00b6 The model will be downloaded automatically from transformers import pipeline , set_seed generator = pipeline ( \"text-generation\" , model = \"gpt2\" ) set_seed ( 42 ) def predict ( text ): return generator ( text , max_length = 50 , num_return_sequences = 3 ) That's it! Let's try it out a little bit: predict ( \"You look amazing today,\" ) And the result: [{'generated_text': 'You look amazing today, guys. If you\\'re still in school and you still have a job where you work in the field\u2026 you\\'re going to look ridiculous by now, you\\'re going to look really ridiculous.\"\\n\\nHe turned to his friends'}, {'generated_text': 'You look amazing today, aren\\'t you?\"\\n\\nHe turned and looked at me. He had an expression that was full of worry as he looked at me. Even before he told me I\\'d have sex, he gave up after I told him'}, {'generated_text': 'You look amazing today, and look amazing in the sunset.\"\\n\\nGarry, then 33, won the London Marathon at age 15, and the World Triathlon in 2007, the two youngest Olympians to ride 100-meters. He also'}] Let's have a look at the first result. You look amazing today, guys. If you're still in school and you still have a job where you work in the field\u2026 you're going to look ridiculous by now, you're going to look really ridiculous.\" He turned to his friends \ud83e\udd23 That's the thing we're looking for! If you run the prediction again, it'll give different results every time. How to Deploy \u00b6 Install Pinferencia \u00b6 $ pip install \"pinferencia[uvicorn]\" ---> 100% Create the Service \u00b6 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from transformers import pipeline , set_seed from pinferencia import Server generator = pipeline ( \"text-generation\" , model = \"gpt2\" ) set_seed ( 42 ) def predict ( text ): return generator ( text , max_length = 50 , num_return_sequences = 3 ) service = Server () service . register ( model_name = \"gpt2\" , model = predict ) Start the Server \u00b6 $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) Test the Service \u00b6 Curl Python requests curl -X 'POST' \\ 'http://127.0.0.1:8000/v1/models/gpt2/predict' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"id\": \"string\", \"parameters\": {}, \"data\": \"You look amazing today,\" }' Result: { \"id\" : \"string\" , \"model_name\" : \"gpt2\" , \"data\" : [ { \"generated_text\" : \"You look amazing today, I was in front of my friends. I wanted everyone to see me. But that's all. No one really cares about me in the eyes of the whole world unless I love them.\\\"\\n\\nIn a second Facebook post\" }, { \"generated_text\" : \"You look amazing today, and I know I am going to get the job done! So thank you all for all those donations, money, help, and hugs. I hope to see you again soon.\" }, { \"generated_text\" : \"You look amazing today, but I will have to wait until early June for what will go down as the first NBA championship (a thing I had been expecting). If it's not the biggest, it is also not great. Now let's look at\" } ] } test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/gpt2/predict\" , json = { \"data\" : \"You look amazing today,\" }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) Run python test.py and print the result: Prediction: [ { \"generated_text\": \"You look amazing today, I was in front of my friends. I wanted everyone to see me. But that's all. No one really cares about me in the eyes of the whole world unless I love them.\\\"\\n\\nIn a second Facebook post\" }, { \"generated_text\": \"You look amazing today, and I know I am going to get the job done! So thank you all for all those donations, money, help, and hugs. I hope to see you again soon.\" }, { \"generated_text\": \"You look amazing today, but I will have to wait until early June for what will go down as the first NBA championship (a thing I had been expecting). If it's not the biggest, it is also not great. Now let's look at\" } ] Even cooler, go to http://127.0.0.1:8000 , and you will have an interactive ui. You can send predict requests just there!","title":"Text Generation - GPT2"},{"location":"ml/huggingface/pipeline/nlp/text-generation/#gpt2-text-generation-transformer-how-to-use-how-to-serve","text":"What is text generation? Input some texts, and the model will predict what the following texts will be. Sounds interesting. How can it be interesting without trying out the model by ourself?","title":"GPT2\u200a-\u200aText Generation Transformer: How to Use &amp; How to\u00a0Serve"},{"location":"ml/huggingface/pipeline/nlp/text-generation/#how-to-use","text":"The model will be downloaded automatically from transformers import pipeline , set_seed generator = pipeline ( \"text-generation\" , model = \"gpt2\" ) set_seed ( 42 ) def predict ( text ): return generator ( text , max_length = 50 , num_return_sequences = 3 ) That's it! Let's try it out a little bit: predict ( \"You look amazing today,\" ) And the result: [{'generated_text': 'You look amazing today, guys. If you\\'re still in school and you still have a job where you work in the field\u2026 you\\'re going to look ridiculous by now, you\\'re going to look really ridiculous.\"\\n\\nHe turned to his friends'}, {'generated_text': 'You look amazing today, aren\\'t you?\"\\n\\nHe turned and looked at me. He had an expression that was full of worry as he looked at me. Even before he told me I\\'d have sex, he gave up after I told him'}, {'generated_text': 'You look amazing today, and look amazing in the sunset.\"\\n\\nGarry, then 33, won the London Marathon at age 15, and the World Triathlon in 2007, the two youngest Olympians to ride 100-meters. He also'}] Let's have a look at the first result. You look amazing today, guys. If you're still in school and you still have a job where you work in the field\u2026 you're going to look ridiculous by now, you're going to look really ridiculous.\" He turned to his friends \ud83e\udd23 That's the thing we're looking for! If you run the prediction again, it'll give different results every time.","title":"How to\u00a0Use"},{"location":"ml/huggingface/pipeline/nlp/text-generation/#how-to-deploy","text":"","title":"How to Deploy"},{"location":"ml/huggingface/pipeline/nlp/text-generation/#install-pinferencia","text":"$ pip install \"pinferencia[uvicorn]\" ---> 100%","title":"Install Pinferencia"},{"location":"ml/huggingface/pipeline/nlp/text-generation/#create-the-service","text":"app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from transformers import pipeline , set_seed from pinferencia import Server generator = pipeline ( \"text-generation\" , model = \"gpt2\" ) set_seed ( 42 ) def predict ( text ): return generator ( text , max_length = 50 , num_return_sequences = 3 ) service = Server () service . register ( model_name = \"gpt2\" , model = predict )","title":"Create the Service"},{"location":"ml/huggingface/pipeline/nlp/text-generation/#start-the-server","text":"$ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)","title":"Start the Server"},{"location":"ml/huggingface/pipeline/nlp/text-generation/#test-the-service","text":"Curl Python requests curl -X 'POST' \\ 'http://127.0.0.1:8000/v1/models/gpt2/predict' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"id\": \"string\", \"parameters\": {}, \"data\": \"You look amazing today,\" }' Result: { \"id\" : \"string\" , \"model_name\" : \"gpt2\" , \"data\" : [ { \"generated_text\" : \"You look amazing today, I was in front of my friends. I wanted everyone to see me. But that's all. No one really cares about me in the eyes of the whole world unless I love them.\\\"\\n\\nIn a second Facebook post\" }, { \"generated_text\" : \"You look amazing today, and I know I am going to get the job done! So thank you all for all those donations, money, help, and hugs. I hope to see you again soon.\" }, { \"generated_text\" : \"You look amazing today, but I will have to wait until early June for what will go down as the first NBA championship (a thing I had been expecting). If it's not the biggest, it is also not great. Now let's look at\" } ] } test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/gpt2/predict\" , json = { \"data\" : \"You look amazing today,\" }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) Run python test.py and print the result: Prediction: [ { \"generated_text\": \"You look amazing today, I was in front of my friends. I wanted everyone to see me. But that's all. No one really cares about me in the eyes of the whole world unless I love them.\\\"\\n\\nIn a second Facebook post\" }, { \"generated_text\": \"You look amazing today, and I know I am going to get the job done! So thank you all for all those donations, money, help, and hugs. I hope to see you again soon.\" }, { \"generated_text\": \"You look amazing today, but I will have to wait until early June for what will go down as the first NBA championship (a thing I had been expecting). If it's not the biggest, it is also not great. Now let's look at\" } ] Even cooler, go to http://127.0.0.1:8000 , and you will have an interactive ui. You can send predict requests just there!","title":"Test the Service"},{"location":"ml/huggingface/pipeline/nlp/translation/","text":"Google T5 Translation as a Service with Just 7 lines of Codes \u00b6 What is T5? Text-To-Text Transfer Transformer (T5) from Google gives the power of translation. In the article, we will deploy Google T5 model as a REST API service. Difficult? What about I\u2019ll tell you: you just need to write 7 lines of codes? Install Dependencies \u00b6 HuggingFace \u00b6 pip install \"transformers[pytorch]\" If it doesn\u2019t work, please visit Installation and check their official documentations. Pinferencia \u00b6 pip install \"pinferencia[uvicorn]\" Define the Service \u00b6 First let\u2019s create the app.py to define the service: app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from transformers import pipeline from pinferencia import Server t5 = pipeline ( model = \"t5-base\" , tokenizer = \"t5-base\" ) def translate ( text ): return t5 ( text ) service = Server () service . register ( model_name = \"t5\" , model = translate ) Start the Service \u00b6 $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) Test the Service \u00b6 Curl Python requests curl -X 'POST' \\ 'http://localhost:8000/v1/models/t5/predict' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"parameters\": {}, \"data\": \"translate English to German: Good morning, my love.\" }' Result: { \"model_name\" : \"t5\" , \"data\" : [ { \"translation_text\" : \"Guten Morgen, liebe Liebe.\" } ] } test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/gpt2/predict\" , json = { \"data\" : \"translate English to German: Good morning, my love.\" }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) Run python test.py and print the result: Prediction: { \"translation_text\": \"Guten Morgen, liebe Liebe.\" } Even cooler, go to http://127.0.0.1:8000 , and you will have an interactive ui. You can send predict requests just there!","title":"Translation - Google T5"},{"location":"ml/huggingface/pipeline/nlp/translation/#google-t5-translation-as-a-service-with-just-7-lines-of-codes","text":"What is T5? Text-To-Text Transfer Transformer (T5) from Google gives the power of translation. In the article, we will deploy Google T5 model as a REST API service. Difficult? What about I\u2019ll tell you: you just need to write 7 lines of codes?","title":"Google T5 Translation as a Service with Just 7 lines of Codes"},{"location":"ml/huggingface/pipeline/nlp/translation/#install-dependencies","text":"","title":"Install Dependencies"},{"location":"ml/huggingface/pipeline/nlp/translation/#huggingface","text":"pip install \"transformers[pytorch]\" If it doesn\u2019t work, please visit Installation and check their official documentations.","title":"HuggingFace"},{"location":"ml/huggingface/pipeline/nlp/translation/#pinferencia","text":"pip install \"pinferencia[uvicorn]\"","title":"Pinferencia"},{"location":"ml/huggingface/pipeline/nlp/translation/#define-the-service","text":"First let\u2019s create the app.py to define the service: app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from transformers import pipeline from pinferencia import Server t5 = pipeline ( model = \"t5-base\" , tokenizer = \"t5-base\" ) def translate ( text ): return t5 ( text ) service = Server () service . register ( model_name = \"t5\" , model = translate )","title":"Define the Service"},{"location":"ml/huggingface/pipeline/nlp/translation/#start-the-service","text":"$ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)","title":"Start the Service"},{"location":"ml/huggingface/pipeline/nlp/translation/#test-the-service","text":"Curl Python requests curl -X 'POST' \\ 'http://localhost:8000/v1/models/t5/predict' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"parameters\": {}, \"data\": \"translate English to German: Good morning, my love.\" }' Result: { \"model_name\" : \"t5\" , \"data\" : [ { \"translation_text\" : \"Guten Morgen, liebe Liebe.\" } ] } test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/gpt2/predict\" , json = { \"data\" : \"translate English to German: Good morning, my love.\" }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) Run python test.py and print the result: Prediction: { \"translation_text\": \"Guten Morgen, liebe Liebe.\" } Even cooler, go to http://127.0.0.1:8000 , and you will have an interactive ui. You can send predict requests just there!","title":"Test the Service"},{"location":"ml/huggingface/pipeline/vision/","text":"In this tutorial, we will explore how to use Hugging Face pipeline, and how to deploy it with Pinferencia as REST API. Prerequisite \u00b6 Please visit Dependencies Download the model and predict \u00b6 The model will be automatically downloaded. 1 2 3 4 5 6 from transformers import pipeline vision_classifier = pipeline ( task = \"image-classification\" ) vision_classifier ( images = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\" ) Result: [{ 'label' : 'lynx, catamount' , 'score' : 0.4403027892112732 }, { 'label' : 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor' , 'score' : 0.03433405980467796 }, { 'label' : 'snow leopard, ounce, Panthera uncia' , 'score' : 0.032148055732250214 }, { 'label' : 'Egyptian cat' , 'score' : 0.02353910356760025 }, { 'label' : 'tiger cat' , 'score' : 0.023034192621707916 }] Let's try another image, and let's try predict two image in one batch: 1 2 3 4 image = \"https://cdn.pixabay.com/photo/2018/08/12/16/59/parrot-3601194_1280.jpg\" vision_classifier ( images = [ image , image ] ) Result: [[{ 'score' : 0.9489120244979858 , 'label' : 'macaw' }, { 'score' : 0.014800671488046646 , 'label' : 'broom' }, { 'score' : 0.009150494821369648 , 'label' : 'swab, swob, mop' }, { 'score' : 0.0018255198374390602 , 'label' : \"plunger, plumber's helper\" }, { 'score' : 0.0017631321679800749 , 'label' : 'African grey, African gray, Psittacus erithacus' }], [{ 'score' : 0.9489120244979858 , 'label' : 'macaw' }, { 'score' : 0.014800671488046646 , 'label' : 'broom' }, { 'score' : 0.009150494821369648 , 'label' : 'swab, swob, mop' }, { 'score' : 0.0018255198374390602 , 'label' : \"plunger, plumber's helper\" }, { 'score' : 0.0017631321679800749 , 'label' : 'African grey, African gray, Psittacus erithacus' }]] Amazingly easy! Now let's try: Deploy the model \u00b6 Without deployment, how could a machine learning tutorial be complete? First, let's install Pinferencia . pip install \"pinferencia[uvicorn]\" Now let's create an app.py file with the codes: app.py 1 2 3 4 5 6 7 8 9 10 11 from transformers import pipeline from pinferencia import Server vision_classifier = pipeline ( task = \"image-classification\" ) def classify ( data ): return vision_classifier ( images = data ) service = Server () service . register ( model_name = \"vision\" , model = classify ) Easy, right? Predict \u00b6 Curl Python requests curl --location --request POST 'http://127.0.0.1:8000/v1/models/vision/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"https://cdn.pixabay.com/photo/2018/08/12/16/59/parrot-3601194_1280.jpg\" }' Result: Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/vision/predict\" , json = { \"data\" : \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\" # noqa }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) Run python test.py and result: Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] Even cooler, go to http://127.0.0.1:8000 , and you will have an interactive ui. You can send predict request just there! Improve it \u00b6 However, using the url of the image to predict sometimes is not appropriate. Let's modify the app.py a little bit to accept Base64 Encoded String as the input. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import base64 from io import BytesIO from PIL import Image from transformers import pipeline from pinferencia import Server vision_classifier = pipeline ( task = \"image-classification\" ) def classify ( image_base64_str ): image = Image . open ( BytesIO ( base64 . b64decode ( image_base64_str ))) return vision_classifier ( images = image ) service = Server () service . register ( model_name = \"vision\" , model = classify ) Predict Again \u00b6 Curl Python requests curl --location --request POST 'http://127.0.0.1:8000/v1/models/vision/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"...\" }' Result: Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/vision/predict\" , json = { \"data\" : \"...\" # noqa }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) Run python test.py and result: Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ]","title":"Image Classification"},{"location":"ml/huggingface/pipeline/vision/#prerequisite","text":"Please visit Dependencies","title":"Prerequisite"},{"location":"ml/huggingface/pipeline/vision/#download-the-model-and-predict","text":"The model will be automatically downloaded. 1 2 3 4 5 6 from transformers import pipeline vision_classifier = pipeline ( task = \"image-classification\" ) vision_classifier ( images = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\" ) Result: [{ 'label' : 'lynx, catamount' , 'score' : 0.4403027892112732 }, { 'label' : 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor' , 'score' : 0.03433405980467796 }, { 'label' : 'snow leopard, ounce, Panthera uncia' , 'score' : 0.032148055732250214 }, { 'label' : 'Egyptian cat' , 'score' : 0.02353910356760025 }, { 'label' : 'tiger cat' , 'score' : 0.023034192621707916 }] Let's try another image, and let's try predict two image in one batch: 1 2 3 4 image = \"https://cdn.pixabay.com/photo/2018/08/12/16/59/parrot-3601194_1280.jpg\" vision_classifier ( images = [ image , image ] ) Result: [[{ 'score' : 0.9489120244979858 , 'label' : 'macaw' }, { 'score' : 0.014800671488046646 , 'label' : 'broom' }, { 'score' : 0.009150494821369648 , 'label' : 'swab, swob, mop' }, { 'score' : 0.0018255198374390602 , 'label' : \"plunger, plumber's helper\" }, { 'score' : 0.0017631321679800749 , 'label' : 'African grey, African gray, Psittacus erithacus' }], [{ 'score' : 0.9489120244979858 , 'label' : 'macaw' }, { 'score' : 0.014800671488046646 , 'label' : 'broom' }, { 'score' : 0.009150494821369648 , 'label' : 'swab, swob, mop' }, { 'score' : 0.0018255198374390602 , 'label' : \"plunger, plumber's helper\" }, { 'score' : 0.0017631321679800749 , 'label' : 'African grey, African gray, Psittacus erithacus' }]] Amazingly easy! Now let's try:","title":"Download the model and\u00a0predict"},{"location":"ml/huggingface/pipeline/vision/#deploy-the-model","text":"Without deployment, how could a machine learning tutorial be complete? First, let's install Pinferencia . pip install \"pinferencia[uvicorn]\" Now let's create an app.py file with the codes: app.py 1 2 3 4 5 6 7 8 9 10 11 from transformers import pipeline from pinferencia import Server vision_classifier = pipeline ( task = \"image-classification\" ) def classify ( data ): return vision_classifier ( images = data ) service = Server () service . register ( model_name = \"vision\" , model = classify ) Easy, right?","title":"Deploy the\u00a0model"},{"location":"ml/huggingface/pipeline/vision/#predict","text":"Curl Python requests curl --location --request POST 'http://127.0.0.1:8000/v1/models/vision/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"https://cdn.pixabay.com/photo/2018/08/12/16/59/parrot-3601194_1280.jpg\" }' Result: Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/vision/predict\" , json = { \"data\" : \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\" # noqa }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) Run python test.py and result: Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] Even cooler, go to http://127.0.0.1:8000 , and you will have an interactive ui. You can send predict request just there!","title":"Predict"},{"location":"ml/huggingface/pipeline/vision/#improve-it","text":"However, using the url of the image to predict sometimes is not appropriate. Let's modify the app.py a little bit to accept Base64 Encoded String as the input. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import base64 from io import BytesIO from PIL import Image from transformers import pipeline from pinferencia import Server vision_classifier = pipeline ( task = \"image-classification\" ) def classify ( image_base64_str ): image = Image . open ( BytesIO ( base64 . b64decode ( image_base64_str ))) return vision_classifier ( images = image ) service = Server () service . register ( model_name = \"vision\" , model = classify )","title":"Improve it"},{"location":"ml/huggingface/pipeline/vision/#predict-again","text":"Curl Python requests curl --location --request POST 'http://127.0.0.1:8000/v1/models/vision/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"...\" }' Result: Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/vision/predict\" , json = { \"data\" : \"...\" # noqa }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) Run python test.py and result: Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ]","title":"Predict Again"},{"location":"models/home/","text":"Model? \u00b6 What is a Model ? Generally, it is a way to calculate something that is more complicated than a equation. Should it be a file? Perhaps. Could it be a python object? Of course. In Pinferencia , a model is just a piece of codes that can get called. A function, or an instance of a class just like those pytorch models.","title":"About Models"},{"location":"models/home/#model","text":"What is a Model ? Generally, it is a way to calculate something that is more complicated than a equation. Should it be a file? Perhaps. Could it be a python object? Of course. In Pinferencia , a model is just a piece of codes that can get called. A function, or an instance of a class just like those pytorch models.","title":"Model?"},{"location":"models/machine-learning/","text":"Machine Learning Frameworks \u00b6 Here is how to load models from different framework: Scikit-Learn PyTorch Tensorflow Any Model Any Function app.py import joblib from pinferencia import Server # train your model model = \"...\" # or load your model model = joblib . load ( \"/path/to/model.joblib\" ) # (1) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , # (2) ) For more details, please visit https://scikit-learn.org/stable/modules/model_persistence.html entrypoint is the function name of the model to perform predictions. Here the data will be sent to the predict function: model.predict(data) . app.py import torch from pinferencia import Server # train your models model = \"...\" # or load your models (1) # from state_dict model = TheModelClass ( * args , ** kwargs ) model . load_state_dict ( torch . load ( PATH )) # entire model model = torch . load ( PATH ) # torchscript model = torch . jit . load ( 'model_scripted.pt' ) model . eval () service = Server () service . register ( model_name = \"mymodel\" , model = model , ) For more details, please visit https://pytorch.org/tutorials/beginner/saving_loading_models.html app.py import tensorflow as tf from pinferencia import Server # train your models model = \"...\" # or load your models (1) # saved_model model = tf . keras . models . load_model ( 'saved_model/model' ) # HDF5 model = tf . keras . models . load_model ( 'model.h5' ) # from weights model = create_model () model . load_weights ( './checkpoints/my_checkpoint' ) loss , acc = model . evaluate ( test_images , test_labels , verbose = 2 ) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) For more details, please visit https://www.tensorflow.org/tutorials/keras/save_and_load app.py from pinferencia import Server class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) app.py from pinferencia import Server def model ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = model , )","title":"Other Machine Learning Frameworks"},{"location":"models/machine-learning/#machine-learning-frameworks","text":"Here is how to load models from different framework: Scikit-Learn PyTorch Tensorflow Any Model Any Function app.py import joblib from pinferencia import Server # train your model model = \"...\" # or load your model model = joblib . load ( \"/path/to/model.joblib\" ) # (1) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , # (2) ) For more details, please visit https://scikit-learn.org/stable/modules/model_persistence.html entrypoint is the function name of the model to perform predictions. Here the data will be sent to the predict function: model.predict(data) . app.py import torch from pinferencia import Server # train your models model = \"...\" # or load your models (1) # from state_dict model = TheModelClass ( * args , ** kwargs ) model . load_state_dict ( torch . load ( PATH )) # entire model model = torch . load ( PATH ) # torchscript model = torch . jit . load ( 'model_scripted.pt' ) model . eval () service = Server () service . register ( model_name = \"mymodel\" , model = model , ) For more details, please visit https://pytorch.org/tutorials/beginner/saving_loading_models.html app.py import tensorflow as tf from pinferencia import Server # train your models model = \"...\" # or load your models (1) # saved_model model = tf . keras . models . load_model ( 'saved_model/model' ) # HDF5 model = tf . keras . models . load_model ( 'model.h5' ) # from weights model = create_model () model . load_weights ( './checkpoints/my_checkpoint' ) loss , acc = model . evaluate ( test_images , test_labels , verbose = 2 ) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) For more details, please visit https://www.tensorflow.org/tutorials/keras/save_and_load app.py from pinferencia import Server class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) app.py from pinferencia import Server def model ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = model , )","title":"Machine Learning Frameworks"},{"location":"models/register/","text":"Register \u00b6 Registering a model is as easy as: 1 2 3 4 5 service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) Register Multiple Model and Multiple Versions? You can register multiple models with multiple versions: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 service . register ( model_name = \"my-model\" , model = my_model , entrypoint = \"predict\" , ) service . register ( model_name = \"my-model\" , model = my_model_v1 , entrypoint = \"predict\" , version_name = \"v1, ) service . register ( model_name = \"your-model\" , model = your_model , entrypoint = \"predict\" , ) service . register ( model_name = \"your-model\" , model = your_model_v1 , entrypoint = \"predict\" , version_name = \"v1, ) service . register ( model_name = \"your-model\" , model = your_model_v2 , entrypoint = \"predict\" , version_name = \"v2, ) Parameters \u00b6 Parameter Type Default Details model_name str Name of the model model object Model object or path version_name str None Name of the version entrypoint str None Name of the function to use metadata dict None Metadata of the model handler object None A class to handler model loading and predicting load_now bool True Whether loading the model on registration Examples \u00b6 Model Name \u00b6 1 2 3 4 5 6 7 8 9 10 11 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , ) Model \u00b6 Model Object Function 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from pinferencia import Server class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict ) 1 2 3 4 5 6 7 8 9 10 11 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , ) Version Name \u00b6 Model without version name will be registered as default version. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pinferencia import Server def add ( data ): return data [ 0 ] + data [ 1 ] def substract ( data ): return data [ 0 ] + data [ 1 ] service = Server () service . register ( model_name = \"mymodel\" , model = add , version_name = \"add\" , # (1) ) service . register ( model_name = \"mymodel\" , model = substract , version_name = \"substract\" , # (2) ) The predicting endpoint will be http://127.0.0.1/v1/models/mymodel/versions/add/predict The predicting endpoint will be http://127.0.0.1/v1/models/mymodel/versions/substract/predict Entrypoint \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pinferencia import Server class MyModel : def add ( self , data ): return data [ 0 ] + data [ 1 ] def substract ( self , data ): return data [ 0 ] - data [ 1 ] model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , version_name = \"add\" , # (1) entrypoint = \"add\" , # (3) ) service . register ( model_name = \"mymodel\" , model = model , version_name = \"substract\" , # (2) entrypoint = \"substract\" , # (4) ) The predicting endpoint will be http://127.0.0.1/v1/models/mymodel/versions/add/predict The predicting endpoint will be http://127.0.0.1/v1/models/mymodel/versions/substract/predict add function of the model will be used to predict. substract function of the model will be used to predict. Metadata \u00b6 Default API \u00b6 Pinferencia default metadata schema supports platform and device These are information for display purpose only. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , metadata = { \"platform\" : \"Linux\" , \"device\" : \"CPU+GPU\" , } ) Kserve API \u00b6 Pinferencia also supports Kserve API. For Kserve V2, the metadata supports: - platform - inputs - outputs The inputs and outputs metadata will determine the data and datatype model received and returned. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server ( api = \"kserve\" ) # (1) service . register ( model_name = \"mymodel\" , model = predict , metadata = { \"platform\" : \"mac os\" , \"inputs\" : [ { \"name\" : \"integers\" , # (2) \"datatype\" : \"int64\" , \"shape\" : [ 1 ], \"data\" : [ 1 , 2 , 3 ], } ], \"outputs\" : [ { \"name\" : \"sum\" , \"datatype\" : \"int64\" , \"shape\" : - 1 , \"data\" : 6 }, # (3) { \"name\" : \"product\" , \"datatype\" : \"int64\" , \"shape\" : - 1 , \"data\" : 6 }, ], } ) If you want to use kserve API, you need to set api=\"kserve\" when initializing the service. In the request, if there are multiple inputs, only input with name intergers will be passed to the model. Output data will be converted into int64 . The datatype field only supports numpy data type. If the data cannot be converted, there will be an extra error field in the output, indicating the reason of the failure. Handler \u00b6 Details of handlers can be found at Handlers . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pinferencia import Server from pinferencia.handlers import PickleHandler class MyPrintHandler ( PickleHandler ): def predict ( self , data ): print ( data ) return self . model . predict ( data ) def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , handler = MyPrintHandler ) Load Now \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import joblib from pinferencia import Server class JoblibHandler ( BaseHandler ): def load_model ( self ): return joblib . load ( self . model_path ) service = Server ( model_dir = \"/opt/models\" ) service . register ( model_name = \"mymodel\" , model = \"/path/to/model.joblib\" , entrypoint = \"predict\" , handler = JoblibHandler , load_now = True , )","title":"Register Models"},{"location":"models/register/#register","text":"Registering a model is as easy as: 1 2 3 4 5 service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) Register Multiple Model and Multiple Versions? You can register multiple models with multiple versions: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 service . register ( model_name = \"my-model\" , model = my_model , entrypoint = \"predict\" , ) service . register ( model_name = \"my-model\" , model = my_model_v1 , entrypoint = \"predict\" , version_name = \"v1, ) service . register ( model_name = \"your-model\" , model = your_model , entrypoint = \"predict\" , ) service . register ( model_name = \"your-model\" , model = your_model_v1 , entrypoint = \"predict\" , version_name = \"v1, ) service . register ( model_name = \"your-model\" , model = your_model_v2 , entrypoint = \"predict\" , version_name = \"v2, )","title":"Register"},{"location":"models/register/#parameters","text":"Parameter Type Default Details model_name str Name of the model model object Model object or path version_name str None Name of the version entrypoint str None Name of the function to use metadata dict None Metadata of the model handler object None A class to handler model loading and predicting load_now bool True Whether loading the model on registration","title":"Parameters"},{"location":"models/register/#examples","text":"","title":"Examples"},{"location":"models/register/#model-name","text":"1 2 3 4 5 6 7 8 9 10 11 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , )","title":"Model Name"},{"location":"models/register/#model","text":"Model Object Function 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from pinferencia import Server class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict ) 1 2 3 4 5 6 7 8 9 10 11 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , )","title":"Model"},{"location":"models/register/#version-name","text":"Model without version name will be registered as default version. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pinferencia import Server def add ( data ): return data [ 0 ] + data [ 1 ] def substract ( data ): return data [ 0 ] + data [ 1 ] service = Server () service . register ( model_name = \"mymodel\" , model = add , version_name = \"add\" , # (1) ) service . register ( model_name = \"mymodel\" , model = substract , version_name = \"substract\" , # (2) ) The predicting endpoint will be http://127.0.0.1/v1/models/mymodel/versions/add/predict The predicting endpoint will be http://127.0.0.1/v1/models/mymodel/versions/substract/predict","title":"Version Name"},{"location":"models/register/#entrypoint","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pinferencia import Server class MyModel : def add ( self , data ): return data [ 0 ] + data [ 1 ] def substract ( self , data ): return data [ 0 ] - data [ 1 ] model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , version_name = \"add\" , # (1) entrypoint = \"add\" , # (3) ) service . register ( model_name = \"mymodel\" , model = model , version_name = \"substract\" , # (2) entrypoint = \"substract\" , # (4) ) The predicting endpoint will be http://127.0.0.1/v1/models/mymodel/versions/add/predict The predicting endpoint will be http://127.0.0.1/v1/models/mymodel/versions/substract/predict add function of the model will be used to predict. substract function of the model will be used to predict.","title":"Entrypoint"},{"location":"models/register/#metadata","text":"","title":"Metadata"},{"location":"models/register/#default-api","text":"Pinferencia default metadata schema supports platform and device These are information for display purpose only. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , metadata = { \"platform\" : \"Linux\" , \"device\" : \"CPU+GPU\" , } )","title":"Default API"},{"location":"models/register/#kserve-api","text":"Pinferencia also supports Kserve API. For Kserve V2, the metadata supports: - platform - inputs - outputs The inputs and outputs metadata will determine the data and datatype model received and returned. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server ( api = \"kserve\" ) # (1) service . register ( model_name = \"mymodel\" , model = predict , metadata = { \"platform\" : \"mac os\" , \"inputs\" : [ { \"name\" : \"integers\" , # (2) \"datatype\" : \"int64\" , \"shape\" : [ 1 ], \"data\" : [ 1 , 2 , 3 ], } ], \"outputs\" : [ { \"name\" : \"sum\" , \"datatype\" : \"int64\" , \"shape\" : - 1 , \"data\" : 6 }, # (3) { \"name\" : \"product\" , \"datatype\" : \"int64\" , \"shape\" : - 1 , \"data\" : 6 }, ], } ) If you want to use kserve API, you need to set api=\"kserve\" when initializing the service. In the request, if there are multiple inputs, only input with name intergers will be passed to the model. Output data will be converted into int64 . The datatype field only supports numpy data type. If the data cannot be converted, there will be an extra error field in the output, indicating the reason of the failure.","title":"Kserve API"},{"location":"models/register/#handler","text":"Details of handlers can be found at Handlers . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pinferencia import Server from pinferencia.handlers import PickleHandler class MyPrintHandler ( PickleHandler ): def predict ( self , data ): print ( data ) return self . model . predict ( data ) def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , handler = MyPrintHandler )","title":"Handler"},{"location":"models/register/#load-now","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import joblib from pinferencia import Server class JoblibHandler ( BaseHandler ): def load_model ( self ): return joblib . load ( self . model_path ) service = Server ( model_dir = \"/opt/models\" ) service . register ( model_name = \"mymodel\" , model = \"/path/to/model.joblib\" , entrypoint = \"predict\" , handler = JoblibHandler , load_now = True , )","title":"Load Now"},{"location":"overview/","text":"Welcome to Pinferencia \u00b6 What is Pinferencia? \u00b6 Straight forward. Simple. Powerful. Three extra lines and your model goes online . Pinferencia ( python + inference ) aims to provide the simplest way to serve any of your machine learning models with a fully functioning Rest API. Features \u00b6 Pinferencia features include: Fast to code, fast to go alive . Minimal codes to write, minimum codes modifications needed. Just based on what you have. 100% Test Coverage : Both statement and branch coverages, no kidding. Easy to use, easy to understand . Automatic API documentation page . All API explained in details with online try-out feature. Thanks to FastAPI and Starlette . Serve any model , even a single function can be served. Try it now! \u00b6 Install \u00b6 $ pip install \"pinferencia[uvicorn]\" ---> 100% Create the App \u00b6 Any Model Any Function Scikit-Learn PyTorch Tensorflow HuggingFace Transformer app.py from pinferencia import Server class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) app.py from pinferencia import Server def model ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = model , ) app.py import joblib import uvicorn from pinferencia import Server # train your model model = \"...\" # or load your model model = joblib . load ( \"/path/to/model.joblib\" ) # (1) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , # (2) ) For more details, please visit https://scikit-learn.org/stable/modules/model_persistence.html entrypoint is the function name of the model to perform predictions. Here the data will be sent to the predict function: model.predict(data) . app.py import torch from pinferencia import Server # train your models model = \"...\" # or load your models (1) # from state_dict model = TheModelClass ( * args , ** kwargs ) model . load_state_dict ( torch . load ( PATH )) # entire model model = torch . load ( PATH ) # torchscript model = torch . jit . load ( 'model_scripted.pt' ) model . eval () service = Server () service . register ( model_name = \"mymodel\" , model = model , ) For more details, please visit https://pytorch.org/tutorials/beginner/saving_loading_models.html app.py import tensorflow as tf from pinferencia import Server # train your models model = \"...\" # or load your models (1) # saved_model model = tf . keras . models . load_model ( 'saved_model/model' ) # HDF5 model = tf . keras . models . load_model ( 'model.h5' ) # from weights model = create_model () model . load_weights ( './checkpoints/my_checkpoint' ) loss , acc = model . evaluate ( test_images , test_labels , verbose = 2 ) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) For more details, please visit https://www.tensorflow.org/tutorials/keras/save_and_load app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from transformers import pipeline from pinferencia import Server vision_classifier = pipeline ( task = \"image-classification\" ) def predict ( data ): return vision_classifier ( images = data ) service = Server () service . register ( model_name = \"vision\" , model = predict ) Run! \u00b6 $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) Hooray , your service is alive. Go to http://127.0.0.1:8000/ and have fun. Remember to come back to our Get Started class!","title":"Overview"},{"location":"overview/#welcome-to-pinferencia","text":"","title":"Welcome to Pinferencia"},{"location":"overview/#what-is-pinferencia","text":"Straight forward. Simple. Powerful. Three extra lines and your model goes online . Pinferencia ( python + inference ) aims to provide the simplest way to serve any of your machine learning models with a fully functioning Rest API.","title":"What is Pinferencia?"},{"location":"overview/#features","text":"Pinferencia features include: Fast to code, fast to go alive . Minimal codes to write, minimum codes modifications needed. Just based on what you have. 100% Test Coverage : Both statement and branch coverages, no kidding. Easy to use, easy to understand . Automatic API documentation page . All API explained in details with online try-out feature. Thanks to FastAPI and Starlette . Serve any model , even a single function can be served.","title":"Features"},{"location":"overview/#try-it-now","text":"","title":"Try it now!"},{"location":"overview/#install","text":"$ pip install \"pinferencia[uvicorn]\" ---> 100%","title":"Install"},{"location":"overview/#create-the-app","text":"Any Model Any Function Scikit-Learn PyTorch Tensorflow HuggingFace Transformer app.py from pinferencia import Server class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) app.py from pinferencia import Server def model ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = model , ) app.py import joblib import uvicorn from pinferencia import Server # train your model model = \"...\" # or load your model model = joblib . load ( \"/path/to/model.joblib\" ) # (1) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , # (2) ) For more details, please visit https://scikit-learn.org/stable/modules/model_persistence.html entrypoint is the function name of the model to perform predictions. Here the data will be sent to the predict function: model.predict(data) . app.py import torch from pinferencia import Server # train your models model = \"...\" # or load your models (1) # from state_dict model = TheModelClass ( * args , ** kwargs ) model . load_state_dict ( torch . load ( PATH )) # entire model model = torch . load ( PATH ) # torchscript model = torch . jit . load ( 'model_scripted.pt' ) model . eval () service = Server () service . register ( model_name = \"mymodel\" , model = model , ) For more details, please visit https://pytorch.org/tutorials/beginner/saving_loading_models.html app.py import tensorflow as tf from pinferencia import Server # train your models model = \"...\" # or load your models (1) # saved_model model = tf . keras . models . load_model ( 'saved_model/model' ) # HDF5 model = tf . keras . models . load_model ( 'model.h5' ) # from weights model = create_model () model . load_weights ( './checkpoints/my_checkpoint' ) loss , acc = model . evaluate ( test_images , test_labels , verbose = 2 ) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) For more details, please visit https://www.tensorflow.org/tutorials/keras/save_and_load app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from transformers import pipeline from pinferencia import Server vision_classifier = pipeline ( task = \"image-classification\" ) def predict ( data ): return vision_classifier ( images = data ) service = Server () service . register ( model_name = \"vision\" , model = predict )","title":"Create the App"},{"location":"overview/#run","text":"$ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) Hooray , your service is alive. Go to http://127.0.0.1:8000/ and have fun. Remember to come back to our Get Started class!","title":"Run!"},{"location":"pinferencia-is-different/","text":"Why is Pinferencia different? \u00b6 Different? \u00b6 Actually, it is not something different. It is something more intuitive, more straight forward or just more simple. How do you serve a model yesterday? Write some script, save a model file, or do something else according to the tools' requirements . And you spend a lot of time to understand those requirements. And a lot time to get it right. Once finished, you're so relieved. However, after almost half a year, you've got new and more complicated models and serve them again using your previous tool. What's in your mind now? No way!!!!!!!!!!!! You have your model, you train it in python , and you predict in python . You even write complicated python codes to perform more difficult tasks . How many changes you need to make and how many extra codes you need to write to get your model served using those tools or platforms? The answer is A lot . With Pinferencia \u00b6 You don't need to do any of these. You just use the model in your own python code. It doesn't matter whether the model is - a PyTorch model or - a Tensorflow model or - any machine learning model or - simply your own codes or - just your own functions . Register the model/function, and Pinferencia will use it to predict, in the way just as expected . Simple, and Powerful \u00b6 Pinferencia aims to be the simplest AI model inference server! Serving a model has never been so easy. If you want to find a simple but robust way to serve your model write minimal codes while maintain controls over you service avoid those heavy tools or platforms You're at the right place.","title":"Pinferencia is different?"},{"location":"pinferencia-is-different/#why-is-pinferencia-different","text":"","title":"Why is Pinferencia different?"},{"location":"pinferencia-is-different/#different","text":"Actually, it is not something different. It is something more intuitive, more straight forward or just more simple. How do you serve a model yesterday? Write some script, save a model file, or do something else according to the tools' requirements . And you spend a lot of time to understand those requirements. And a lot time to get it right. Once finished, you're so relieved. However, after almost half a year, you've got new and more complicated models and serve them again using your previous tool. What's in your mind now?","title":"Different?"},{"location":"pinferencia-is-different/#with-pinferencia","text":"You don't need to do any of these. You just use the model in your own python code. It doesn't matter whether the model is - a PyTorch model or - a Tensorflow model or - any machine learning model or - simply your own codes or - just your own functions . Register the model/function, and Pinferencia will use it to predict, in the way just as expected .","title":"With Pinferencia"},{"location":"pinferencia-is-different/#simple-and-powerful","text":"Pinferencia aims to be the simplest AI model inference server! Serving a model has never been so easy. If you want to find a simple but robust way to serve your model write minimal codes while maintain controls over you service avoid those heavy tools or platforms You're at the right place.","title":"Simple, and Powerful"},{"location":"restapi/","text":"REST API \u00b6 Overview \u00b6 Pinferencia has two built-in API sets: Default API Kserve API from pinferencia import Server service = Server () # or service = Server ( api = \"default\" ) from pinferencia import Server service = Server ( api = \"kserve\" ) Are you using other serving tools now? If you also use other model serving tools, here are the Kserve API versions the tools support: Name API Pinferencia Kserve V1 & V2 TF Serving Kserve V1 TorchServe Kserve V1 or V2 Triton Kserve V2 KServe Kserve V1 No Pain, Just Gain \u00b6 As you can see You can switch between Pinferencia and other tools with almost no code changes in client. If you want to use Pinferencia for prototyping and client building, then use other tools in production, you got it supported out of the box. You can use Pinferencia in production with other tools with the same API set. If you're switching from Kserve V1 to Kserve V2 and you need a server supporting both during the transition, you got Pinferencia . So, no pain, just gain. Default API \u00b6 Path Method Summary /v1/healthz GET Healthz /v1/models GET List Models /v1/models/{model_name} GET List Model Versions /v1/models/{model_name}/ready GET Model Is Ready /v1/models/{model_name}/versions/{version_name}/ready GET Model Version Is Ready /v1/models/{model_name}/load POST Load Model /v1/models/{model_name}/versions/{version_name}/load POST Load Version /v1/models/{model_name}/unload POST Unload Model /v1/models/{model_name}/versions/{version_name}/unload POST Unload Version /v1/models/{model_name}/predict POST Model Predict /v1/models/{model_name}/versions/{version_name}/predict POST Model Version Predict Kserve API \u00b6 Path Method Summary /v1/healthz GET Healthz /v1/models GET List Models /v1/models/{model_name} GET List Model Versions /v1/models/{model_name}/ready GET Model Is Ready /v1/models/{model_name}/versions/{version_name}/ready GET Model Version Is Ready /v1/models/{model_name}/load POST Load Model /v1/models/{model_name}/versions/{version_name}/load POST Load Version /v1/models/{model_name}/unload POST Unload Model /v1/models/{model_name}/versions/{version_name}/unload POST Unload Version /v1/models/{model_name}/infer POST Model Predict /v1/models/{model_name}/versions/{version_name}/infer POST Model Version Predict /v2/healthz GET Healthz /v2/models GET List Models /v2/models/{model_name} GET List Model Versions /v2/models/{model_name}/ready GET Model Is Ready /v2/models/{model_name}/versions/{version_name}/ready GET Model Version Is Ready /v2/models/{model_name}/load POST Load Model /v2/models/{model_name}/versions/{version_name}/load POST Load Version /v2/models/{model_name}/unload POST Unload Model /v2/models/{model_name}/versions/{version_name}/unload POST Unload Version /v2/models/{model_name}/infer POST Model Predict /v2/models/{model_name}/versions/{version_name}/infer POST Model Version Predict","title":"REST API"},{"location":"restapi/#rest-api","text":"","title":"REST API"},{"location":"restapi/#overview","text":"Pinferencia has two built-in API sets: Default API Kserve API from pinferencia import Server service = Server () # or service = Server ( api = \"default\" ) from pinferencia import Server service = Server ( api = \"kserve\" ) Are you using other serving tools now? If you also use other model serving tools, here are the Kserve API versions the tools support: Name API Pinferencia Kserve V1 & V2 TF Serving Kserve V1 TorchServe Kserve V1 or V2 Triton Kserve V2 KServe Kserve V1","title":"Overview"},{"location":"restapi/#no-pain-just-gain","text":"As you can see You can switch between Pinferencia and other tools with almost no code changes in client. If you want to use Pinferencia for prototyping and client building, then use other tools in production, you got it supported out of the box. You can use Pinferencia in production with other tools with the same API set. If you're switching from Kserve V1 to Kserve V2 and you need a server supporting both during the transition, you got Pinferencia . So, no pain, just gain.","title":"No Pain, Just Gain"},{"location":"restapi/#default-api","text":"Path Method Summary /v1/healthz GET Healthz /v1/models GET List Models /v1/models/{model_name} GET List Model Versions /v1/models/{model_name}/ready GET Model Is Ready /v1/models/{model_name}/versions/{version_name}/ready GET Model Version Is Ready /v1/models/{model_name}/load POST Load Model /v1/models/{model_name}/versions/{version_name}/load POST Load Version /v1/models/{model_name}/unload POST Unload Model /v1/models/{model_name}/versions/{version_name}/unload POST Unload Version /v1/models/{model_name}/predict POST Model Predict /v1/models/{model_name}/versions/{version_name}/predict POST Model Version Predict","title":"Default API"},{"location":"restapi/#kserve-api","text":"Path Method Summary /v1/healthz GET Healthz /v1/models GET List Models /v1/models/{model_name} GET List Model Versions /v1/models/{model_name}/ready GET Model Is Ready /v1/models/{model_name}/versions/{version_name}/ready GET Model Version Is Ready /v1/models/{model_name}/load POST Load Model /v1/models/{model_name}/versions/{version_name}/load POST Load Version /v1/models/{model_name}/unload POST Unload Model /v1/models/{model_name}/versions/{version_name}/unload POST Unload Version /v1/models/{model_name}/infer POST Model Predict /v1/models/{model_name}/versions/{version_name}/infer POST Model Version Predict /v2/healthz GET Healthz /v2/models GET List Models /v2/models/{model_name} GET List Model Versions /v2/models/{model_name}/ready GET Model Is Ready /v2/models/{model_name}/versions/{version_name}/ready GET Model Version Is Ready /v2/models/{model_name}/load POST Load Model /v2/models/{model_name}/versions/{version_name}/load POST Load Version /v2/models/{model_name}/unload POST Unload Model /v2/models/{model_name}/versions/{version_name}/unload POST Unload Version /v2/models/{model_name}/infer POST Model Predict /v2/models/{model_name}/versions/{version_name}/infer POST Model Version Predict","title":"Kserve API"},{"location":"en/restapi/","text":"REST API \u00b6 Overview \u00b6 Pinferencia has two built-in API sets: Default API Kserve API from pinferencia import Server service = Server () # or service = Server ( api = \"default\" ) from pinferencia import Server service = Server ( api = \"kserve\" ) Are you using other serving tools now? If you also use other model serving tools, here are the Kserve API versions the tools support: Name API Pinferencia Kserve V1 & V2 TF Serving Kserve V1 TorchServe Kserve V1 or V2 Triton Kserve V2 KServe Kserve V1 No Pain, Just Gain \u00b6 As you can see You can switch between Pinferencia and other tools with almost no code changes in client. If you want to use Pinferencia for prototyping and client building, then use other tools in production, you got it supported out of the box. You can use Pinferencia in production with other tools with the same API set. If you're switching from Kserve V1 to Kserve V2 and you need a server supporting both during the transition, you got Pinferencia . So, no pain, just gain. Default API \u00b6 Path Method Summary /v1/healthz GET Healthz /v1/models GET List Models /v1/models/{model_name} GET List Model Versions /v1/models/{model_name}/ready GET Model Is Ready /v1/models/{model_name}/versions/{version_name}/ready GET Model Version Is Ready /v1/models/{model_name}/load POST Load Model /v1/models/{model_name}/versions/{version_name}/load POST Load Version /v1/models/{model_name}/unload POST Unload Model /v1/models/{model_name}/versions/{version_name}/unload POST Unload Version /v1/models/{model_name}/predict POST Model Predict /v1/models/{model_name}/versions/{version_name}/predict POST Model Version Predict Kserve API \u00b6 Path Method Summary /v1/healthz GET Healthz /v1/models GET List Models /v1/models/{model_name} GET List Model Versions /v1/models/{model_name}/ready GET Model Is Ready /v1/models/{model_name}/versions/{version_name}/ready GET Model Version Is Ready /v1/models/{model_name}/load POST Load Model /v1/models/{model_name}/versions/{version_name}/load POST Load Version /v1/models/{model_name}/unload POST Unload Model /v1/models/{model_name}/versions/{version_name}/unload POST Unload Version /v1/models/{model_name}/infer POST Model Predict /v1/models/{model_name}/versions/{version_name}/infer POST Model Version Predict /v2/healthz GET Healthz /v2/models GET List Models /v2/models/{model_name} GET List Model Versions /v2/models/{model_name}/ready GET Model Is Ready /v2/models/{model_name}/versions/{version_name}/ready GET Model Version Is Ready /v2/models/{model_name}/load POST Load Model /v2/models/{model_name}/versions/{version_name}/load POST Load Version /v2/models/{model_name}/unload POST Unload Model /v2/models/{model_name}/versions/{version_name}/unload POST Unload Version /v2/models/{model_name}/infer POST Model Predict /v2/models/{model_name}/versions/{version_name}/infer POST Model Version Predict","title":"REST API"},{"location":"en/restapi/#rest-api","text":"","title":"REST API"},{"location":"en/restapi/#overview","text":"Pinferencia has two built-in API sets: Default API Kserve API from pinferencia import Server service = Server () # or service = Server ( api = \"default\" ) from pinferencia import Server service = Server ( api = \"kserve\" ) Are you using other serving tools now? If you also use other model serving tools, here are the Kserve API versions the tools support: Name API Pinferencia Kserve V1 & V2 TF Serving Kserve V1 TorchServe Kserve V1 or V2 Triton Kserve V2 KServe Kserve V1","title":"Overview"},{"location":"en/restapi/#no-pain-just-gain","text":"As you can see You can switch between Pinferencia and other tools with almost no code changes in client. If you want to use Pinferencia for prototyping and client building, then use other tools in production, you got it supported out of the box. You can use Pinferencia in production with other tools with the same API set. If you're switching from Kserve V1 to Kserve V2 and you need a server supporting both during the transition, you got Pinferencia . So, no pain, just gain.","title":"No Pain, Just Gain"},{"location":"en/restapi/#default-api","text":"Path Method Summary /v1/healthz GET Healthz /v1/models GET List Models /v1/models/{model_name} GET List Model Versions /v1/models/{model_name}/ready GET Model Is Ready /v1/models/{model_name}/versions/{version_name}/ready GET Model Version Is Ready /v1/models/{model_name}/load POST Load Model /v1/models/{model_name}/versions/{version_name}/load POST Load Version /v1/models/{model_name}/unload POST Unload Model /v1/models/{model_name}/versions/{version_name}/unload POST Unload Version /v1/models/{model_name}/predict POST Model Predict /v1/models/{model_name}/versions/{version_name}/predict POST Model Version Predict","title":"Default API"},{"location":"en/restapi/#kserve-api","text":"Path Method Summary /v1/healthz GET Healthz /v1/models GET List Models /v1/models/{model_name} GET List Model Versions /v1/models/{model_name}/ready GET Model Is Ready /v1/models/{model_name}/versions/{version_name}/ready GET Model Version Is Ready /v1/models/{model_name}/load POST Load Model /v1/models/{model_name}/versions/{version_name}/load POST Load Version /v1/models/{model_name}/unload POST Unload Model /v1/models/{model_name}/versions/{version_name}/unload POST Unload Version /v1/models/{model_name}/infer POST Model Predict /v1/models/{model_name}/versions/{version_name}/infer POST Model Version Predict /v2/healthz GET Healthz /v2/models GET List Models /v2/models/{model_name} GET List Model Versions /v2/models/{model_name}/ready GET Model Is Ready /v2/models/{model_name}/versions/{version_name}/ready GET Model Version Is Ready /v2/models/{model_name}/load POST Load Model /v2/models/{model_name}/versions/{version_name}/load POST Load Version /v2/models/{model_name}/unload POST Unload Model /v2/models/{model_name}/versions/{version_name}/unload POST Unload Version /v2/models/{model_name}/infer POST Model Predict /v2/models/{model_name}/versions/{version_name}/infer POST Model Version Predict","title":"Kserve API"},{"location":"re/","text":"","title":"Home"},{"location":"re/frontend/requirements/","text":"Requirements \u00b6 To use Pinferencia's frontend with your model, there are some requirements of your model's predict function. Templates \u00b6 Currently, there are mainly two major catogory of the template's inputs and outputs. More (audio and video) will be supported in the future. Base Templates \u00b6 Template Input Output Text to Text Text Text Text to Image Text Image Image to Text Image Text Camera to Text Image Text Image to Image Image Image Camera to Image Image Image Derived Templates \u00b6 Template Input Output Translation Text Text Image Classification Image Text Image Style Transfer Image Image Input \u00b6 The predict function must be able to accept a list of data as inputs. For text input, the input will be a list of strings. For image input, the input will be a list of strings representing the base64 encoded images. Output \u00b6 The predict function must produce a list of data as outputs. For text output, the output must be a list. For image output, the output must be a list of strings representing the base64 encoded images. Text Output The frontend will try to parse the outputs into table, json or pure text. Table Text JSON If the output is similar to below: [ [ { \"a\" : 1 , \"b\" : 2 }, { \"a\" : 3 , \"b\" : 4 }, { \"a\" : 5 , \"b\" : 6 } ] ] It will be displayed as a table. If the output is similar to below: [ \"Text output.\" ] It will be displayed as a text. All other format of outputs will be displayed as a JSON.","title":"Frontend Requirements"},{"location":"re/frontend/requirements/#requirements","text":"To use Pinferencia's frontend with your model, there are some requirements of your model's predict function.","title":"Requirements"},{"location":"re/frontend/requirements/#templates","text":"Currently, there are mainly two major catogory of the template's inputs and outputs. More (audio and video) will be supported in the future.","title":"Templates"},{"location":"re/frontend/requirements/#base-templates","text":"Template Input Output Text to Text Text Text Text to Image Text Image Image to Text Image Text Camera to Text Image Text Image to Image Image Image Camera to Image Image Image","title":"Base Templates"},{"location":"re/frontend/requirements/#derived-templates","text":"Template Input Output Translation Text Text Image Classification Image Text Image Style Transfer Image Image","title":"Derived Templates"},{"location":"re/frontend/requirements/#input","text":"The predict function must be able to accept a list of data as inputs. For text input, the input will be a list of strings. For image input, the input will be a list of strings representing the base64 encoded images.","title":"Input"},{"location":"re/frontend/requirements/#output","text":"The predict function must produce a list of data as outputs. For text output, the output must be a list. For image output, the output must be a list of strings representing the base64 encoded images. Text Output The frontend will try to parse the outputs into table, json or pure text. Table Text JSON If the output is similar to below: [ [ { \"a\" : 1 , \"b\" : 2 }, { \"a\" : 3 , \"b\" : 4 }, { \"a\" : 5 , \"b\" : 6 } ] ] It will be displayed as a table. If the output is similar to below: [ \"Text output.\" ] It will be displayed as a text. All other format of outputs will be displayed as a JSON.","title":"Output"},{"location":"re/get-started/home/","text":"Solve the Riddle \u00b6 In this world there are so many tools, however you never learn them at schools. Want to has some API for you model? It's not easy, like a myth, like a riddle. All I want are some predictions, and the way led by pinferencia, there'll be no more frictions. In this series of tutorials, you will master how to serve: a custom model : a simple JSON model a custom function PyTorch MNIST model with two methods and have some fun with the MNIST model","title":"Introduction"},{"location":"re/get-started/home/#solve-the-riddle","text":"In this world there are so many tools, however you never learn them at schools. Want to has some API for you model? It's not easy, like a myth, like a riddle. All I want are some predictions, and the way led by pinferencia, there'll be no more frictions. In this series of tutorials, you will master how to serve: a custom model : a simple JSON model a custom function PyTorch MNIST model with two methods and have some fun with the MNIST model","title":"Solve the Riddle"},{"location":"re/get-started/other-models/","text":"What's Next \u00b6 Well, I bet you had some fun with the PyTorch MNIST model in the previous tutorial. I think you are familiar with Pinferencia now. Pinferencia can serve any callable object in a very straight-forward way. No complications. And it's easy to integrate with your existing codes. That is what Pinferencia is designed for. Minimum codes modifications . Now you can serve models from any framework , and you can even mix up them together . You can have an API using different models from different frameworks at the same time ! Enjoy yourself! If you like Pinferencia , don't forget to go to Github and give a star. Thank you. If you want to explore more examples on different machine learning models, you can find the inside the Example section from the navigation panel on your left.","title":"What's Next"},{"location":"re/get-started/other-models/#whats-next","text":"Well, I bet you had some fun with the PyTorch MNIST model in the previous tutorial. I think you are familiar with Pinferencia now. Pinferencia can serve any callable object in a very straight-forward way. No complications. And it's easy to integrate with your existing codes. That is what Pinferencia is designed for. Minimum codes modifications . Now you can serve models from any framework , and you can even mix up them together . You can have an API using different models from different frameworks at the same time ! Enjoy yourself! If you like Pinferencia , don't forget to go to Github and give a star. Thank you. If you want to explore more examples on different machine learning models, you can find the inside the Example section from the navigation panel on your left.","title":"What's Next"},{"location":"re/get-started/pytorch-mnist/","text":"Serve PyTorch MNIST Model \u00b6 In this tutorial, we will serve a PyTorch MNIST model. It receives a Base64 encoded image as request data, and return the prediction in the response. Prerequisite \u00b6 Visit PyTorch Examples - MNIST , download the files. Run below commands to install and train the model: pip install -r requirements.txt python main.py --save-model After the training is finished, you will have a folder structure as below. A mnist_cnn.pt file is created . \u251c\u2500\u2500 README.md \u251c\u2500\u2500 main.py \u251c\u2500\u2500 mnist_cnn.pt \u2514\u2500\u2500 requirements.txt Deploy Methods \u00b6 There are two methods you can deploy the model. Directly register a function. Only register a model path, with an additioanl handler. We will cover both methods step by step in this tutorial. Directly Register a Function \u00b6 Create the App \u00b6 Let's create a file func_app.py in the same folder. func_app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 import base64 from io import BytesIO import torch from main import Net # (1) from PIL import Image from torchvision import transforms from pinferencia import Server use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) # (2) model = Net () . to ( device ) # (3) model . load_state_dict ( torch . load ( \"mnist_cnn.pt\" )) model . eval () def preprocessing ( img_str ): image = Image . open ( BytesIO ( base64 . b64decode ( img_str ))) tensor = transform ( image ) return torch . stack ([ tensor ]) . to ( device ) def predict ( data ): return model ( preprocessing ( data )) . argmax ( 1 ) . tolist ()[ 0 ] service = Server () # (4) service . register ( model_name = \"mnist\" , model = predict ) Make suer you can import the Net Model. Preprocessing transformation codes. The example script only save the state_dict . Here we need to initialize the model and load the state_dict . Get ready, 3, 2, 1. GO! Start the Service \u00b6 $ uvicorn func_app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. Test the Service \u00b6 Test Data? Because our input is a base64 encoded MNIST image, where can we get these data? You can make use of PyTorch's datasets. Create a file with in the same folder named get-base64-img . get-base64-img.py import base64 import random from io import BytesIO from PIL import Image from torchvision import datasets dataset = datasets . MNIST ( # (1) \"./data\" , train = True , download = True , transform = None , ) index = random . randint ( 0 , len ( dataset . data )) # (2) img = dataset . data [ index ] img = Image . fromarray ( img . numpy (), mode = \"L\" ) buffered = BytesIO () img . save ( buffered , format = \"JPEG\" ) base64_img_str = base64 . b64encode ( buffered . getvalue ()) . decode () print ( \"Base64 String:\" , base64_img_str ) # (3) print ( \"target:\" , dataset . targets [ index ] . tolist ()) This is the MNIST dataset used during training. Let's use a random image. The string and the target are printed to stdout. Run the script and copy the string. python get-base64-img.py Output: Base64 String: /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+uhfwXqy2Ph25VYnPiB3SzhUkPlXCfNkAAEsCCCeOeKx9RsLjStUu9Ou1C3NpM8Eqg5AdSVIz35FVqK9xl0HXhb/C20sdMubjTLMQXs11AhkRXmmDsCwzgAYPpz+XI/GrSLrTfiVqNzPapbw3xE8AWQNvUAKXOOmWVjg+teeUV2fgXxd4hsPE2hWEGuX8Vh9uhja3Fw3lbGcBhtzjGCad8XI7iL4p68twHDGcMm45+QqCuPbBFcVRRU97fXepXb3d9dT3VzJjfNPIXdsAAZY8nAAH4VBX/9k = target: 4 Let's create a file test.py test.py 1 2 3 4 5 6 7 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/mnist/predict\" , json = { \"data\" : \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+uhfwXqy2Ph25VYnPiB3SzhUkPlXCfNkAAEsCCCeOeKx9RsLjStUu9Ou1C3NpM8Eqg5AdSVIz35FVqK9xl0HXhb/C20sdMubjTLMQXs11AhkRXmmDsCwzgAYPpz+XI/GrSLrTfiVqNzPapbw3xE8AWQNvUAKXOOmWVjg+teeUV2fgXxd4hsPE2hWEGuX8Vh9uhja3Fw3lbGcBhtzjGCad8XI7iL4p68twHDGcMm45+QqCuPbBFcVRRU97fXepXb3d9dT3VzJjfNPIXdsAAZY8nAAH4VBX/9k=\" }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) Run the test: $ python test.py Prediction: 4 You can try out the API with more images, or even using the interactive API documentation page http://127.0.0.1 Register a Model Path, with a Handler \u00b6 Handler If you prefer the old classical way of serving a model with a file, using a handler is your choice. For details of handlers, please visit Handlers Create the App \u00b6 Let's create a file func_app.py in the same folder. The codes below are refactored into a handle class. It looks cleaner! path_app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 import base64 import pathlib from io import BytesIO import torch from main import Net from PIL import Image from torchvision import transforms from pinferencia import Server from pinferencia.handlers import BaseHandler class MNISTHandler ( BaseHandler ): transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) def load_model ( self ): # (1) model = Net () . to ( self . device ) model . load_state_dict ( torch . load ( self . model_path )) model . eval () return model def predict ( self , data ): # (2) image = Image . open ( BytesIO ( base64 . b64decode ( data ))) tensor = self . transform ( image ) input_data = torch . stack ([ tensor ]) . to ( self . device ) return self . model ( input_data ) . argmax ( 1 ) . tolist ()[ 0 ] service = Server ( model_dir = pathlib . Path ( __file__ ) . parent . resolve ()) # (3) service . register ( model_name = \"mnist\" , model = \"mnist_cnn.pt\" , handler = MNISTHandler , load_now = True , # (4) ) We move the codes of loading the model into the load_model function. The model path can be accessed by self.model_path . We move the codes of predicting into the predict function. The model can be accessed by self.model . model_dir is where Pinferencia will look for your model files. Set the model_dir to the folder having the mnist_cnn.pt and this script. load_now determine if the model will be get loaded immediately during registration. The default is True . If set to False , you need to call the load API to load the model before prediction. Start the Service \u00b6 $ uvicorn path_app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. Test the Service \u00b6 Run the test: $ python test.py Prediction: 4 No suprise, the same result. Finally \u00b6 Using Pinferencia , you can serve any model. You can load the models by yourself, just what you have done to do a offline prediction. The codes are already there. Then, just register the model using Pinferencia , and your model is alive. Alternatively, you can choose to refactor your codes into a Handler Class . The old classic way also works with Pinferencia . Both worlds work for your model, classic music and rock'n'roll . Isn't it great! Now you have mastered how to use Pinferencia to: Register any model, any function and serve them. Use your custom handler to serve your machine learning model. If you still have time, let's try something fun. Extra: Sum Up the MNIST Images \u00b6 Let's create a sum_mnist.py . It accepts an array of images, predicts their digits and sum up them. sum_mnist.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 import base64 import pathlib from io import BytesIO import torch from main import Net from PIL import Image from torchvision import transforms from pinferencia import Server from pinferencia.handlers import BaseHandler class MNISTHandler ( BaseHandler ): transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) def load_model ( self ): model = Net () . to ( self . device ) model . load_state_dict ( torch . load ( self . model_path )) model . eval () return model def predict ( self , data ): tensors = [] # (1) for img in data : image = Image . open ( BytesIO ( base64 . b64decode ( img ))) tensors . append ( self . transform ( image )) input_data = torch . stack ( tensors ) . to ( self . device ) return sum ( self . model ( input_data ) . argmax ( 1 ) . tolist ()) service = Server ( model_dir = pathlib . Path ( __file__ ) . parent . resolve ()) service . register ( model_name = \"mnist\" , model = \"mnist_cnn.pt\" , handler = MNISTHandler , load_now = True , ) Here we pre-process each image, predict its digit and sum up. Have fun with Pinferencia !","title":"Serve PyTorch MNIST Model"},{"location":"re/get-started/pytorch-mnist/#serve-pytorch-mnist-model","text":"In this tutorial, we will serve a PyTorch MNIST model. It receives a Base64 encoded image as request data, and return the prediction in the response.","title":"Serve PyTorch MNIST Model"},{"location":"re/get-started/pytorch-mnist/#prerequisite","text":"Visit PyTorch Examples - MNIST , download the files. Run below commands to install and train the model: pip install -r requirements.txt python main.py --save-model After the training is finished, you will have a folder structure as below. A mnist_cnn.pt file is created . \u251c\u2500\u2500 README.md \u251c\u2500\u2500 main.py \u251c\u2500\u2500 mnist_cnn.pt \u2514\u2500\u2500 requirements.txt","title":"Prerequisite"},{"location":"re/get-started/pytorch-mnist/#deploy-methods","text":"There are two methods you can deploy the model. Directly register a function. Only register a model path, with an additioanl handler. We will cover both methods step by step in this tutorial.","title":"Deploy Methods"},{"location":"re/get-started/pytorch-mnist/#directly-register-a-function","text":"","title":"Directly Register a Function"},{"location":"re/get-started/pytorch-mnist/#create-the-app","text":"Let's create a file func_app.py in the same folder. func_app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 import base64 from io import BytesIO import torch from main import Net # (1) from PIL import Image from torchvision import transforms from pinferencia import Server use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) # (2) model = Net () . to ( device ) # (3) model . load_state_dict ( torch . load ( \"mnist_cnn.pt\" )) model . eval () def preprocessing ( img_str ): image = Image . open ( BytesIO ( base64 . b64decode ( img_str ))) tensor = transform ( image ) return torch . stack ([ tensor ]) . to ( device ) def predict ( data ): return model ( preprocessing ( data )) . argmax ( 1 ) . tolist ()[ 0 ] service = Server () # (4) service . register ( model_name = \"mnist\" , model = predict ) Make suer you can import the Net Model. Preprocessing transformation codes. The example script only save the state_dict . Here we need to initialize the model and load the state_dict . Get ready, 3, 2, 1. GO!","title":"Create the App"},{"location":"re/get-started/pytorch-mnist/#start-the-service","text":"$ uvicorn func_app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete.","title":"Start the Service"},{"location":"re/get-started/pytorch-mnist/#test-the-service","text":"Test Data? Because our input is a base64 encoded MNIST image, where can we get these data? You can make use of PyTorch's datasets. Create a file with in the same folder named get-base64-img . get-base64-img.py import base64 import random from io import BytesIO from PIL import Image from torchvision import datasets dataset = datasets . MNIST ( # (1) \"./data\" , train = True , download = True , transform = None , ) index = random . randint ( 0 , len ( dataset . data )) # (2) img = dataset . data [ index ] img = Image . fromarray ( img . numpy (), mode = \"L\" ) buffered = BytesIO () img . save ( buffered , format = \"JPEG\" ) base64_img_str = base64 . b64encode ( buffered . getvalue ()) . decode () print ( \"Base64 String:\" , base64_img_str ) # (3) print ( \"target:\" , dataset . targets [ index ] . tolist ()) This is the MNIST dataset used during training. Let's use a random image. The string and the target are printed to stdout. Run the script and copy the string. python get-base64-img.py Output: Base64 String: /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+uhfwXqy2Ph25VYnPiB3SzhUkPlXCfNkAAEsCCCeOeKx9RsLjStUu9Ou1C3NpM8Eqg5AdSVIz35FVqK9xl0HXhb/C20sdMubjTLMQXs11AhkRXmmDsCwzgAYPpz+XI/GrSLrTfiVqNzPapbw3xE8AWQNvUAKXOOmWVjg+teeUV2fgXxd4hsPE2hWEGuX8Vh9uhja3Fw3lbGcBhtzjGCad8XI7iL4p68twHDGcMm45+QqCuPbBFcVRRU97fXepXb3d9dT3VzJjfNPIXdsAAZY8nAAH4VBX/9k = target: 4 Let's create a file test.py test.py 1 2 3 4 5 6 7 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/mnist/predict\" , json = { \"data\" : \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+uhfwXqy2Ph25VYnPiB3SzhUkPlXCfNkAAEsCCCeOeKx9RsLjStUu9Ou1C3NpM8Eqg5AdSVIz35FVqK9xl0HXhb/C20sdMubjTLMQXs11AhkRXmmDsCwzgAYPpz+XI/GrSLrTfiVqNzPapbw3xE8AWQNvUAKXOOmWVjg+teeUV2fgXxd4hsPE2hWEGuX8Vh9uhja3Fw3lbGcBhtzjGCad8XI7iL4p68twHDGcMm45+QqCuPbBFcVRRU97fXepXb3d9dT3VzJjfNPIXdsAAZY8nAAH4VBX/9k=\" }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) Run the test: $ python test.py Prediction: 4 You can try out the API with more images, or even using the interactive API documentation page http://127.0.0.1","title":"Test the Service"},{"location":"re/get-started/pytorch-mnist/#register-a-model-path-with-a-handler","text":"Handler If you prefer the old classical way of serving a model with a file, using a handler is your choice. For details of handlers, please visit Handlers","title":"Register a Model Path, with a Handler"},{"location":"re/get-started/pytorch-mnist/#create-the-app_1","text":"Let's create a file func_app.py in the same folder. The codes below are refactored into a handle class. It looks cleaner! path_app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 import base64 import pathlib from io import BytesIO import torch from main import Net from PIL import Image from torchvision import transforms from pinferencia import Server from pinferencia.handlers import BaseHandler class MNISTHandler ( BaseHandler ): transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) def load_model ( self ): # (1) model = Net () . to ( self . device ) model . load_state_dict ( torch . load ( self . model_path )) model . eval () return model def predict ( self , data ): # (2) image = Image . open ( BytesIO ( base64 . b64decode ( data ))) tensor = self . transform ( image ) input_data = torch . stack ([ tensor ]) . to ( self . device ) return self . model ( input_data ) . argmax ( 1 ) . tolist ()[ 0 ] service = Server ( model_dir = pathlib . Path ( __file__ ) . parent . resolve ()) # (3) service . register ( model_name = \"mnist\" , model = \"mnist_cnn.pt\" , handler = MNISTHandler , load_now = True , # (4) ) We move the codes of loading the model into the load_model function. The model path can be accessed by self.model_path . We move the codes of predicting into the predict function. The model can be accessed by self.model . model_dir is where Pinferencia will look for your model files. Set the model_dir to the folder having the mnist_cnn.pt and this script. load_now determine if the model will be get loaded immediately during registration. The default is True . If set to False , you need to call the load API to load the model before prediction.","title":"Create the App"},{"location":"re/get-started/pytorch-mnist/#start-the-service_1","text":"$ uvicorn path_app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete.","title":"Start the Service"},{"location":"re/get-started/pytorch-mnist/#test-the-service_1","text":"Run the test: $ python test.py Prediction: 4 No suprise, the same result.","title":"Test the Service"},{"location":"re/get-started/pytorch-mnist/#finally","text":"Using Pinferencia , you can serve any model. You can load the models by yourself, just what you have done to do a offline prediction. The codes are already there. Then, just register the model using Pinferencia , and your model is alive. Alternatively, you can choose to refactor your codes into a Handler Class . The old classic way also works with Pinferencia . Both worlds work for your model, classic music and rock'n'roll . Isn't it great! Now you have mastered how to use Pinferencia to: Register any model, any function and serve them. Use your custom handler to serve your machine learning model. If you still have time, let's try something fun.","title":"Finally"},{"location":"re/get-started/pytorch-mnist/#extra-sum-up-the-mnist-images","text":"Let's create a sum_mnist.py . It accepts an array of images, predicts their digits and sum up them. sum_mnist.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 import base64 import pathlib from io import BytesIO import torch from main import Net from PIL import Image from torchvision import transforms from pinferencia import Server from pinferencia.handlers import BaseHandler class MNISTHandler ( BaseHandler ): transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) def load_model ( self ): model = Net () . to ( self . device ) model . load_state_dict ( torch . load ( self . model_path )) model . eval () return model def predict ( self , data ): tensors = [] # (1) for img in data : image = Image . open ( BytesIO ( base64 . b64decode ( img ))) tensors . append ( self . transform ( image )) input_data = torch . stack ( tensors ) . to ( self . device ) return sum ( self . model ( input_data ) . argmax ( 1 ) . tolist ()) service = Server ( model_dir = pathlib . Path ( __file__ ) . parent . resolve ()) service . register ( model_name = \"mnist\" , model = \"mnist_cnn.pt\" , handler = MNISTHandler , load_now = True , ) Here we pre-process each image, predict its digit and sum up. Have fun with Pinferencia !","title":"Extra: Sum Up the MNIST Images"},{"location":"re/get-started/serve-a-function/","text":"Serve a Function \u00b6 Well, serving a function? Is it useful? Of course it is. If you have a whole workflow of inferences , it consists of many steps. Most of the time, you will implement a function to do this job. Now you can register the function immediately. If you want to share some pre-processing or post-processing functions, now you've got your Robin, Batman ! Or a function is just enough for your job. Mission \u00b6 We're given a list of mountains' heights. We need to find out the highest, the loweset, and the difference between the highest and the lowest. It's a simple problem, let's solve it in a function to get you familiar with the concept: graph LR heights(Mountains' Heights) --> max(Find Out the Highest) heights --> min(Find Out the Lowest) min --> diff(Calculate the Difference) max --> diff diff --> output(Output) subgraph Workflow max min diff end Create the Service and Register the Model \u00b6 Save the following codes in app.py . app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from typing import List from pinferencia import Server def calc ( data : List [ int ]) -> int : highest = max ( data ) lowest = min ( data ) return highest - lowest service = Server () service . register ( model_name = \"mountain\" , model = calc ) Start the Server \u00b6 $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) Test the API \u00b6 Create a test.py with the codes below. Tips You need to have requests installed. pip install requests test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/mountain/predict\" , json = { \"data\" : [ 1000 , 2000 , 3000 ]}, ) difference = response . json ()[ \"data\" ] print ( f \"Difference between the highest and lowest is { difference } m.\" ) Run the script and check the result. $ python test.py Difference between the highest and lowest is 2000m. Further more \u00b6 So now you have learned how to serve a model define as a Class or a Function . If you have just a single model to serve, it's easy-peasy. But in real world, you have custom codes like pre-processing and post-processing. And some tasks need multiple models to work together. For example, if you want to predict an animal's breed, you may need the below workflow: graph LR pic(Picture) --> species(Species Classification) species --> cat(Cat) --> cat_breed(Cat Breed Classification) --> Persian(Persian) species --> dog(Dog) --> dog_breed(Dog Breed Classification) --> Labrador(Labrador) species --> monkey(Monkey) --> monkey_breed(Monkey Breed Classification) --> spider(Spider Monkeys) Deploying this on many platform or tools aren't that easy. However, now you have Pinferencia , you have a choice!","title":"Serve a Function"},{"location":"re/get-started/serve-a-function/#serve-a-function","text":"Well, serving a function? Is it useful? Of course it is. If you have a whole workflow of inferences , it consists of many steps. Most of the time, you will implement a function to do this job. Now you can register the function immediately. If you want to share some pre-processing or post-processing functions, now you've got your Robin, Batman ! Or a function is just enough for your job.","title":"Serve a Function"},{"location":"re/get-started/serve-a-function/#mission","text":"We're given a list of mountains' heights. We need to find out the highest, the loweset, and the difference between the highest and the lowest. It's a simple problem, let's solve it in a function to get you familiar with the concept: graph LR heights(Mountains' Heights) --> max(Find Out the Highest) heights --> min(Find Out the Lowest) min --> diff(Calculate the Difference) max --> diff diff --> output(Output) subgraph Workflow max min diff end","title":"Mission"},{"location":"re/get-started/serve-a-function/#create-the-service-and-register-the-model","text":"Save the following codes in app.py . app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from typing import List from pinferencia import Server def calc ( data : List [ int ]) -> int : highest = max ( data ) lowest = min ( data ) return highest - lowest service = Server () service . register ( model_name = \"mountain\" , model = calc )","title":"Create the Service and Register the Model"},{"location":"re/get-started/serve-a-function/#start-the-server","text":"$ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)","title":"Start the Server"},{"location":"re/get-started/serve-a-function/#test-the-api","text":"Create a test.py with the codes below. Tips You need to have requests installed. pip install requests test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/mountain/predict\" , json = { \"data\" : [ 1000 , 2000 , 3000 ]}, ) difference = response . json ()[ \"data\" ] print ( f \"Difference between the highest and lowest is { difference } m.\" ) Run the script and check the result. $ python test.py Difference between the highest and lowest is 2000m.","title":"Test the API"},{"location":"re/get-started/serve-a-function/#further-more","text":"So now you have learned how to serve a model define as a Class or a Function . If you have just a single model to serve, it's easy-peasy. But in real world, you have custom codes like pre-processing and post-processing. And some tasks need multiple models to work together. For example, if you want to predict an animal's breed, you may need the below workflow: graph LR pic(Picture) --> species(Species Classification) species --> cat(Cat) --> cat_breed(Cat Breed Classification) --> Persian(Persian) species --> dog(Dog) --> dog_breed(Dog Breed Classification) --> Labrador(Labrador) species --> monkey(Monkey) --> monkey_breed(Monkey Breed Classification) --> spider(Spider Monkeys) Deploying this on many platform or tools aren't that easy. However, now you have Pinferencia , you have a choice!","title":"Further more"},{"location":"re/get-started/serve-a-json-model/","text":"Run a JSON Model \u00b6 Now let's first try something easy to get you familiar with Pinferecia . TL;DR It's important for you to understand how to register and serve a model in Pinferencia . However, if you want to try machine learning model now, you can jump to Serve Pytorch MNIST Model Define the JSON Model \u00b6 Let's create a file named app.py . Below is a JSON Model. It simply return 1 for input a , 2 for input b , and 0 for other inputs. app.py 1 2 3 4 class JSONModel : def predict ( self , data ): knowledge = { \"a\" : 1 , \"b\" : 2 } return knowledge . get ( data , 0 ) Create the Service and Register the Model \u00b6 First we import Server from pinferencia , then create an instance and register a instance of our JSON Model . app.py 1 2 3 4 5 6 7 8 9 10 11 12 from pinferencia import Server class JSONModel : def predict ( self , data : str ) -> int : knowledge = { \"a\" : 1 , \"b\" : 2 } return knowledge . get ( data , 0 ) model = JSONModel () service = Server () service . register ( model_name = \"json\" , model = model , entrypoint = \"predict\" ) What are the model_name and entrypoint here? model_name is the name you give to the model for later API access. Here we give the model a name json , and the url for this model is http://127.0.0.1:8000/v1/models/json . If you have any confusion about the APIs, you can always visit the documentation page mentioned in the next part. The entrypoint predict means we will use the predict function of JSON Model to predict the data. Start the Server \u00b6 $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) Open your browser and visit http://127.0.0.1:8000 , and now you have an automatically generated API Documentation page! FastAPI and Starlette Pinferencia builds on FastAPI which is built on Starlette . Thanks to them, you will have an API with OpenAPI Specification. It means you will have an automatic documentation webpage and client codes can also be generated automatically. Tips There are two API documentation endpoints: http://127.0.0.1:8000 or http://127.0.0.1:8000/docs http://127.0.0.1:8000/redoc You can view the API specifiacations and even try out the API by yourself! Test the API \u00b6 Create a test.py with the codes below. Tips You need to have requests installed. pip install requests test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/json/predict\" , json = { \"data\" : \"a\" }, ) print ( response . json ()) Run the script and check the result. $ python test.py {'model_name': 'json', 'data': 1} Now let's add two more inputs and make the print pretty. test.py 1 2 3 4 5 6 7 8 9 10 11 import requests print ( \"| {:^10} | {:^15} |\" . format ( \"Input\" , \"Prediction\" )) print ( \"| {:^10} | {:^15} |\" . format ( \"-\" * 10 , \"-\" * 15 )) for character in [ \"a\" , \"b\" , \"c\" ]: response = requests . post ( url = \"http://localhost:8000/v1/models/json/predict\" , json = { \"data\" : character }, ) print ( f \"| { character : ^10 } | { str ( response . json ()[ 'data' ]) : ^15 } |\" ) Run the script again and check the result. $ python test.py | Input | Prediction | |----------|---------------| | a | 1 | | b | 2 | | c | 0 |","title":"Serve a Simple JSON Model"},{"location":"re/get-started/serve-a-json-model/#run-a-json-model","text":"Now let's first try something easy to get you familiar with Pinferecia . TL;DR It's important for you to understand how to register and serve a model in Pinferencia . However, if you want to try machine learning model now, you can jump to Serve Pytorch MNIST Model","title":"Run a JSON Model"},{"location":"re/get-started/serve-a-json-model/#define-the-json-model","text":"Let's create a file named app.py . Below is a JSON Model. It simply return 1 for input a , 2 for input b , and 0 for other inputs. app.py 1 2 3 4 class JSONModel : def predict ( self , data ): knowledge = { \"a\" : 1 , \"b\" : 2 } return knowledge . get ( data , 0 )","title":"Define the JSON Model"},{"location":"re/get-started/serve-a-json-model/#create-the-service-and-register-the-model","text":"First we import Server from pinferencia , then create an instance and register a instance of our JSON Model . app.py 1 2 3 4 5 6 7 8 9 10 11 12 from pinferencia import Server class JSONModel : def predict ( self , data : str ) -> int : knowledge = { \"a\" : 1 , \"b\" : 2 } return knowledge . get ( data , 0 ) model = JSONModel () service = Server () service . register ( model_name = \"json\" , model = model , entrypoint = \"predict\" ) What are the model_name and entrypoint here? model_name is the name you give to the model for later API access. Here we give the model a name json , and the url for this model is http://127.0.0.1:8000/v1/models/json . If you have any confusion about the APIs, you can always visit the documentation page mentioned in the next part. The entrypoint predict means we will use the predict function of JSON Model to predict the data.","title":"Create the Service and Register the Model"},{"location":"re/get-started/serve-a-json-model/#start-the-server","text":"$ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) Open your browser and visit http://127.0.0.1:8000 , and now you have an automatically generated API Documentation page! FastAPI and Starlette Pinferencia builds on FastAPI which is built on Starlette . Thanks to them, you will have an API with OpenAPI Specification. It means you will have an automatic documentation webpage and client codes can also be generated automatically. Tips There are two API documentation endpoints: http://127.0.0.1:8000 or http://127.0.0.1:8000/docs http://127.0.0.1:8000/redoc You can view the API specifiacations and even try out the API by yourself!","title":"Start the Server"},{"location":"re/get-started/serve-a-json-model/#test-the-api","text":"Create a test.py with the codes below. Tips You need to have requests installed. pip install requests test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/json/predict\" , json = { \"data\" : \"a\" }, ) print ( response . json ()) Run the script and check the result. $ python test.py {'model_name': 'json', 'data': 1} Now let's add two more inputs and make the print pretty. test.py 1 2 3 4 5 6 7 8 9 10 11 import requests print ( \"| {:^10} | {:^15} |\" . format ( \"Input\" , \"Prediction\" )) print ( \"| {:^10} | {:^15} |\" . format ( \"-\" * 10 , \"-\" * 15 )) for character in [ \"a\" , \"b\" , \"c\" ]: response = requests . post ( url = \"http://localhost:8000/v1/models/json/predict\" , json = { \"data\" : character }, ) print ( f \"| { character : ^10 } | { str ( response . json ()[ 'data' ]) : ^15 } |\" ) Run the script again and check the result. $ python test.py | Input | Prediction | |----------|---------------| | a | 1 | | b | 2 | | c | 0 |","title":"Test the API"},{"location":"re/handlers/","text":"Handlers \u00b6 BaseHandler \u00b6 BaseHandler is only an abstract base class. You can't use it directly. Let's Take a look at some of its functions: BaseHandler 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class BaseHandler ( abc . ABC ): def preprocess ( self , data : object , parameters : object = None ): return data # (1) def postprocess ( self , data : object , parameters : object = None ): return data # (2) def predict ( self , data : object ): if not getattr ( self , \"model\" , None ): raise Exception ( \"Model is not loaded.\" ) predict_func = ( # (3) getattr ( self . model , self . entrypoint ) if self . entrypoint else self . model ) return predict_func ( data ) @abc . abstractmethod def load_model ( self ): return NotImplemented # (4) The default codes do nothing . You can override this function to provide your own pre-processing codes. The default codes do nothing . You can override this function to provide your own post-processing codes. Get predict function from entrypoint name and the model object. Model can be accessed by self.model , the entrypoint registered can be accessed by self.entrypoint . You need to implement this function. Model path can be accessed by self.model_path PickleHandler \u00b6 The default handler is PickleHandler . PickleHandler 1 2 3 4 5 6 7 8 class PickleHandler ( BaseHandler ): \"\"\"Pickle Handler for Models Saved through Pickle\"\"\" def load_model ( self ): if not getattr ( self , \"model_path\" , None ): raise Exception ( \"Model path not provided.\" ) with open ( self . model_path , \"rb\" ) as f : return pickle . load ( f )","title":"Handlers"},{"location":"re/handlers/#handlers","text":"","title":"Handlers"},{"location":"re/handlers/#basehandler","text":"BaseHandler is only an abstract base class. You can't use it directly. Let's Take a look at some of its functions: BaseHandler 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class BaseHandler ( abc . ABC ): def preprocess ( self , data : object , parameters : object = None ): return data # (1) def postprocess ( self , data : object , parameters : object = None ): return data # (2) def predict ( self , data : object ): if not getattr ( self , \"model\" , None ): raise Exception ( \"Model is not loaded.\" ) predict_func = ( # (3) getattr ( self . model , self . entrypoint ) if self . entrypoint else self . model ) return predict_func ( data ) @abc . abstractmethod def load_model ( self ): return NotImplemented # (4) The default codes do nothing . You can override this function to provide your own pre-processing codes. The default codes do nothing . You can override this function to provide your own post-processing codes. Get predict function from entrypoint name and the model object. Model can be accessed by self.model , the entrypoint registered can be accessed by self.entrypoint . You need to implement this function. Model path can be accessed by self.model_path","title":"BaseHandler"},{"location":"re/handlers/#picklehandler","text":"The default handler is PickleHandler . PickleHandler 1 2 3 4 5 6 7 8 class PickleHandler ( BaseHandler ): \"\"\"Pickle Handler for Models Saved through Pickle\"\"\" def load_model ( self ): if not getattr ( self , \"model_path\" , None ): raise Exception ( \"Model path not provided.\" ) with open ( self . model_path , \"rb\" ) as f : return pickle . load ( f )","title":"PickleHandler"},{"location":"re/install/","text":"Install Pinferencia \u00b6 Recommended \u00b6 It's recommended to install Pinferencia with uvicorn . $ pip install \"pinferencia[uvicorn]\" ---> 100% Alternatively \u00b6 You can also choose install Pinferencia without uvicorn , and install other ASGI server you prefer later. $ pip install pinferencia ---> 100%","title":"Install"},{"location":"re/install/#install-pinferencia","text":"","title":"Install Pinferencia"},{"location":"re/install/#recommended","text":"It's recommended to install Pinferencia with uvicorn . $ pip install \"pinferencia[uvicorn]\" ---> 100%","title":"Recommended"},{"location":"re/install/#alternatively","text":"You can also choose install Pinferencia without uvicorn , and install other ASGI server you prefer later. $ pip install pinferencia ---> 100%","title":"Alternatively"},{"location":"re/ml/huggingface/dependencies/","text":"For mac users \u00b6 If you're working on a M1 Mac like me, you need install cmake and rust brew install cmake curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh Install dependencies \u00b6 You can install dependencies using pip. pip install tqdm boto3 requests regex sentencepiece sacremoses or you can use a docker image instead: docker run -it -p 8000 :8000 -v $( pwd ) :/opt/workspace huggingface/transformers-pytorch-cpu:4.18.0 bash","title":"Install Dependencies"},{"location":"re/ml/huggingface/dependencies/#for-mac-users","text":"If you're working on a M1 Mac like me, you need install cmake and rust brew install cmake curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh","title":"For mac users"},{"location":"re/ml/huggingface/dependencies/#install-dependencies","text":"You can install dependencies using pip. pip install tqdm boto3 requests regex sentencepiece sacremoses or you can use a docker image instead: docker run -it -p 8000 :8000 -v $( pwd ) :/opt/workspace huggingface/transformers-pytorch-cpu:4.18.0 bash","title":"Install dependencies"},{"location":"re/ml/huggingface/pipeline/nlp/bert/","text":"Many of you must have heard of Bert , or transformers . And you may also know huggingface. In this tutorial, let's play with its pytorch transformer model and serve it through REST API How does the model work? \u00b6 With an input of an incomplete sentence, the model will give its prediction: Input Output Paris is the [MASK] of France. Paris is the capital of France. Let's try it out now Prerequisite \u00b6 Please visit Dependencies Serve the Model \u00b6 Install Pinferencia \u00b6 First, let's install Pinferencia . pip install \"pinferencia[uvicorn]\" Create app.py \u00b6 Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 from transformers import pipeline from pinferencia import Server bert = pipeline ( \"fill-mask\" , model = \"bert-base-uncased\" ) service = Server () service . register ( model_name = \"bert\" , model = lambda text : bert ( text )) Run the service, and wait for it to load the model and start the server: $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) Test the service \u00b6 curl Python Requests Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/bert/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"Paris is the [MASK] of France.\" }' Response { \"model_name\":\"bert\", \"data\":\"Paris is the capital of France.\" } Create the test.py . test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/bert/predict\" , json = { \"data\" : \"Paris is the [MASK] of France.\" }, ) print ( response . json ()) Run the script and check the result. $ python test.py {'model_name': 'bert', 'data': 'Paris is the capital of France.'}","title":"Bert"},{"location":"re/ml/huggingface/pipeline/nlp/bert/#how-does-the-model-work","text":"With an input of an incomplete sentence, the model will give its prediction: Input Output Paris is the [MASK] of France. Paris is the capital of France. Let's try it out now","title":"How does the model work?"},{"location":"re/ml/huggingface/pipeline/nlp/bert/#prerequisite","text":"Please visit Dependencies","title":"Prerequisite"},{"location":"re/ml/huggingface/pipeline/nlp/bert/#serve-the-model","text":"","title":"Serve the Model"},{"location":"re/ml/huggingface/pipeline/nlp/bert/#install-pinferencia","text":"First, let's install Pinferencia . pip install \"pinferencia[uvicorn]\"","title":"Install Pinferencia"},{"location":"re/ml/huggingface/pipeline/nlp/bert/#create-apppy","text":"Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 from transformers import pipeline from pinferencia import Server bert = pipeline ( \"fill-mask\" , model = \"bert-base-uncased\" ) service = Server () service . register ( model_name = \"bert\" , model = lambda text : bert ( text )) Run the service, and wait for it to load the model and start the server: $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)","title":"Create app.py"},{"location":"re/ml/huggingface/pipeline/nlp/bert/#test-the-service","text":"curl Python Requests Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/bert/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"Paris is the [MASK] of France.\" }' Response { \"model_name\":\"bert\", \"data\":\"Paris is the capital of France.\" } Create the test.py . test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/bert/predict\" , json = { \"data\" : \"Paris is the [MASK] of France.\" }, ) print ( response . json ()) Run the script and check the result. $ python test.py {'model_name': 'bert', 'data': 'Paris is the capital of France.'}","title":"Test the service"},{"location":"re/ml/huggingface/pipeline/nlp/text-generation/","text":"GPT2\u200a-\u200aText Generation Transformer: How to Use & How to Serve \u00b6 What is text generation? Input some texts, and the model will predict what the following texts will be. Sounds interesting. How can it be interesting without trying out the model by ourself? How to Use \u00b6 The model will be downloaded automatically from transformers import pipeline , set_seed generator = pipeline ( \"text-generation\" , model = \"gpt2\" ) set_seed ( 42 ) def predict ( text ): return generator ( text , max_length = 50 , num_return_sequences = 3 ) That's it! Let's try it out a little bit: predict ( \"You look amazing today,\" ) And the result: [{'generated_text': 'You look amazing today, guys. If you\\'re still in school and you still have a job where you work in the field\u2026 you\\'re going to look ridiculous by now, you\\'re going to look really ridiculous.\"\\n\\nHe turned to his friends'}, {'generated_text': 'You look amazing today, aren\\'t you?\"\\n\\nHe turned and looked at me. He had an expression that was full of worry as he looked at me. Even before he told me I\\'d have sex, he gave up after I told him'}, {'generated_text': 'You look amazing today, and look amazing in the sunset.\"\\n\\nGarry, then 33, won the London Marathon at age 15, and the World Triathlon in 2007, the two youngest Olympians to ride 100-meters. He also'}] Let's have a look at the first result. You look amazing today, guys. If you're still in school and you still have a job where you work in the field\u2026 you're going to look ridiculous by now, you're going to look really ridiculous.\" He turned to his friends \ud83e\udd23 That's the thing we're looking for! If you run the prediction again, it'll give different results every time. How to Deploy \u00b6 Install Pinferencia \u00b6 $ pip install \"pinferencia[uvicorn]\" ---> 100% Create the Service \u00b6 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from transformers import pipeline , set_seed from pinferencia import Server generator = pipeline ( \"text-generation\" , model = \"gpt2\" ) set_seed ( 42 ) def predict ( text ): return generator ( text , max_length = 50 , num_return_sequences = 3 ) service = Server () service . register ( model_name = \"gpt2\" , model = predict ) Start the Server \u00b6 $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) Test the Service \u00b6 Curl Python requests curl -X 'POST' \\ 'http://127.0.0.1:8000/v1/models/gpt2/predict' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"id\": \"string\", \"parameters\": {}, \"data\": \"You look amazing today,\" }' Result: { \"id\" : \"string\" , \"model_name\" : \"gpt2\" , \"data\" : [ { \"generated_text\" : \"You look amazing today, I was in front of my friends. I wanted everyone to see me. But that's all. No one really cares about me in the eyes of the whole world unless I love them.\\\"\\n\\nIn a second Facebook post\" }, { \"generated_text\" : \"You look amazing today, and I know I am going to get the job done! So thank you all for all those donations, money, help, and hugs. I hope to see you again soon.\" }, { \"generated_text\" : \"You look amazing today, but I will have to wait until early June for what will go down as the first NBA championship (a thing I had been expecting). If it's not the biggest, it is also not great. Now let's look at\" } ] } test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/gpt2/predict\" , json = { \"data\" : \"You look amazing today,\" }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) Run python test.py and print the result: Prediction: [ { \"generated_text\": \"You look amazing today, I was in front of my friends. I wanted everyone to see me. But that's all. No one really cares about me in the eyes of the whole world unless I love them.\\\"\\n\\nIn a second Facebook post\" }, { \"generated_text\": \"You look amazing today, and I know I am going to get the job done! So thank you all for all those donations, money, help, and hugs. I hope to see you again soon.\" }, { \"generated_text\": \"You look amazing today, but I will have to wait until early June for what will go down as the first NBA championship (a thing I had been expecting). If it's not the biggest, it is also not great. Now let's look at\" } ] Even cooler, go to http://127.0.0.1:8000 , and you will have an interactive ui. You can send predict requests just there!","title":"Text Generation - GPT2"},{"location":"re/ml/huggingface/pipeline/nlp/text-generation/#gpt2-text-generation-transformer-how-to-use-how-to-serve","text":"What is text generation? Input some texts, and the model will predict what the following texts will be. Sounds interesting. How can it be interesting without trying out the model by ourself?","title":"GPT2\u200a-\u200aText Generation Transformer: How to Use &amp; How to\u00a0Serve"},{"location":"re/ml/huggingface/pipeline/nlp/text-generation/#how-to-use","text":"The model will be downloaded automatically from transformers import pipeline , set_seed generator = pipeline ( \"text-generation\" , model = \"gpt2\" ) set_seed ( 42 ) def predict ( text ): return generator ( text , max_length = 50 , num_return_sequences = 3 ) That's it! Let's try it out a little bit: predict ( \"You look amazing today,\" ) And the result: [{'generated_text': 'You look amazing today, guys. If you\\'re still in school and you still have a job where you work in the field\u2026 you\\'re going to look ridiculous by now, you\\'re going to look really ridiculous.\"\\n\\nHe turned to his friends'}, {'generated_text': 'You look amazing today, aren\\'t you?\"\\n\\nHe turned and looked at me. He had an expression that was full of worry as he looked at me. Even before he told me I\\'d have sex, he gave up after I told him'}, {'generated_text': 'You look amazing today, and look amazing in the sunset.\"\\n\\nGarry, then 33, won the London Marathon at age 15, and the World Triathlon in 2007, the two youngest Olympians to ride 100-meters. He also'}] Let's have a look at the first result. You look amazing today, guys. If you're still in school and you still have a job where you work in the field\u2026 you're going to look ridiculous by now, you're going to look really ridiculous.\" He turned to his friends \ud83e\udd23 That's the thing we're looking for! If you run the prediction again, it'll give different results every time.","title":"How to\u00a0Use"},{"location":"re/ml/huggingface/pipeline/nlp/text-generation/#how-to-deploy","text":"","title":"How to Deploy"},{"location":"re/ml/huggingface/pipeline/nlp/text-generation/#install-pinferencia","text":"$ pip install \"pinferencia[uvicorn]\" ---> 100%","title":"Install Pinferencia"},{"location":"re/ml/huggingface/pipeline/nlp/text-generation/#create-the-service","text":"app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from transformers import pipeline , set_seed from pinferencia import Server generator = pipeline ( \"text-generation\" , model = \"gpt2\" ) set_seed ( 42 ) def predict ( text ): return generator ( text , max_length = 50 , num_return_sequences = 3 ) service = Server () service . register ( model_name = \"gpt2\" , model = predict )","title":"Create the Service"},{"location":"re/ml/huggingface/pipeline/nlp/text-generation/#start-the-server","text":"$ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)","title":"Start the Server"},{"location":"re/ml/huggingface/pipeline/nlp/text-generation/#test-the-service","text":"Curl Python requests curl -X 'POST' \\ 'http://127.0.0.1:8000/v1/models/gpt2/predict' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"id\": \"string\", \"parameters\": {}, \"data\": \"You look amazing today,\" }' Result: { \"id\" : \"string\" , \"model_name\" : \"gpt2\" , \"data\" : [ { \"generated_text\" : \"You look amazing today, I was in front of my friends. I wanted everyone to see me. But that's all. No one really cares about me in the eyes of the whole world unless I love them.\\\"\\n\\nIn a second Facebook post\" }, { \"generated_text\" : \"You look amazing today, and I know I am going to get the job done! So thank you all for all those donations, money, help, and hugs. I hope to see you again soon.\" }, { \"generated_text\" : \"You look amazing today, but I will have to wait until early June for what will go down as the first NBA championship (a thing I had been expecting). If it's not the biggest, it is also not great. Now let's look at\" } ] } test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/gpt2/predict\" , json = { \"data\" : \"You look amazing today,\" }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) Run python test.py and print the result: Prediction: [ { \"generated_text\": \"You look amazing today, I was in front of my friends. I wanted everyone to see me. But that's all. No one really cares about me in the eyes of the whole world unless I love them.\\\"\\n\\nIn a second Facebook post\" }, { \"generated_text\": \"You look amazing today, and I know I am going to get the job done! So thank you all for all those donations, money, help, and hugs. I hope to see you again soon.\" }, { \"generated_text\": \"You look amazing today, but I will have to wait until early June for what will go down as the first NBA championship (a thing I had been expecting). If it's not the biggest, it is also not great. Now let's look at\" } ] Even cooler, go to http://127.0.0.1:8000 , and you will have an interactive ui. You can send predict requests just there!","title":"Test the Service"},{"location":"re/ml/huggingface/pipeline/nlp/translation/","text":"Google T5 Translation as a Service with Just 7 lines of Codes \u00b6 What is T5? Text-To-Text Transfer Transformer (T5) from Google gives the power of translation. In the article, we will deploy Google T5 model as a REST API service. Difficult? What about I\u2019ll tell you: you just need to write 7 lines of codes? Install Dependencies \u00b6 HuggingFace \u00b6 pip install \"transformers[pytorch]\" If it doesn\u2019t work, please visit Installation and check their official documentations. Pinferencia \u00b6 pip install \"pinferencia[uvicorn]\" Define the Service \u00b6 First let\u2019s create the app.py to define the service: app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from transformers import pipeline from pinferencia import Server t5 = pipeline ( model = \"t5-base\" , tokenizer = \"t5-base\" ) def translate ( text ): return t5 ( text ) service = Server () service . register ( model_name = \"t5\" , model = translate ) Start the Service \u00b6 $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) Test the Service \u00b6 Curl Python requests curl -X 'POST' \\ 'http://localhost:8000/v1/models/t5/predict' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"parameters\": {}, \"data\": \"translate English to German: Good morning, my love.\" }' Result: { \"model_name\" : \"t5\" , \"data\" : [ { \"translation_text\" : \"Guten Morgen, liebe Liebe.\" } ] } test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/gpt2/predict\" , json = { \"data\" : \"translate English to German: Good morning, my love.\" }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) Run python test.py and print the result: Prediction: { \"translation_text\": \"Guten Morgen, liebe Liebe.\" } Even cooler, go to http://127.0.0.1:8000 , and you will have an interactive ui. You can send predict requests just there!","title":"Translation - Google T5"},{"location":"re/ml/huggingface/pipeline/nlp/translation/#google-t5-translation-as-a-service-with-just-7-lines-of-codes","text":"What is T5? Text-To-Text Transfer Transformer (T5) from Google gives the power of translation. In the article, we will deploy Google T5 model as a REST API service. Difficult? What about I\u2019ll tell you: you just need to write 7 lines of codes?","title":"Google T5 Translation as a Service with Just 7 lines of Codes"},{"location":"re/ml/huggingface/pipeline/nlp/translation/#install-dependencies","text":"","title":"Install Dependencies"},{"location":"re/ml/huggingface/pipeline/nlp/translation/#huggingface","text":"pip install \"transformers[pytorch]\" If it doesn\u2019t work, please visit Installation and check their official documentations.","title":"HuggingFace"},{"location":"re/ml/huggingface/pipeline/nlp/translation/#pinferencia","text":"pip install \"pinferencia[uvicorn]\"","title":"Pinferencia"},{"location":"re/ml/huggingface/pipeline/nlp/translation/#define-the-service","text":"First let\u2019s create the app.py to define the service: app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from transformers import pipeline from pinferencia import Server t5 = pipeline ( model = \"t5-base\" , tokenizer = \"t5-base\" ) def translate ( text ): return t5 ( text ) service = Server () service . register ( model_name = \"t5\" , model = translate )","title":"Define the Service"},{"location":"re/ml/huggingface/pipeline/nlp/translation/#start-the-service","text":"$ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)","title":"Start the Service"},{"location":"re/ml/huggingface/pipeline/nlp/translation/#test-the-service","text":"Curl Python requests curl -X 'POST' \\ 'http://localhost:8000/v1/models/t5/predict' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"parameters\": {}, \"data\": \"translate English to German: Good morning, my love.\" }' Result: { \"model_name\" : \"t5\" , \"data\" : [ { \"translation_text\" : \"Guten Morgen, liebe Liebe.\" } ] } test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/gpt2/predict\" , json = { \"data\" : \"translate English to German: Good morning, my love.\" }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) Run python test.py and print the result: Prediction: { \"translation_text\": \"Guten Morgen, liebe Liebe.\" } Even cooler, go to http://127.0.0.1:8000 , and you will have an interactive ui. You can send predict requests just there!","title":"Test the Service"},{"location":"re/ml/huggingface/pipeline/vision/","text":"In this tutorial, we will explore how to use Hugging Face pipeline, and how to deploy it with Pinferencia as REST API. Prerequisite \u00b6 Please visit Dependencies Download the model and predict \u00b6 The model will be automatically downloaded. 1 2 3 4 5 6 from transformers import pipeline vision_classifier = pipeline ( task = \"image-classification\" ) vision_classifier ( images = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\" ) Result: [{ 'label' : 'lynx, catamount' , 'score' : 0.4403027892112732 }, { 'label' : 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor' , 'score' : 0.03433405980467796 }, { 'label' : 'snow leopard, ounce, Panthera uncia' , 'score' : 0.032148055732250214 }, { 'label' : 'Egyptian cat' , 'score' : 0.02353910356760025 }, { 'label' : 'tiger cat' , 'score' : 0.023034192621707916 }] Let's try another image, and let's try predict two image in one batch: 1 2 3 4 image = \"https://cdn.pixabay.com/photo/2018/08/12/16/59/parrot-3601194_1280.jpg\" vision_classifier ( images = [ image , image ] ) Result: [[{ 'score' : 0.9489120244979858 , 'label' : 'macaw' }, { 'score' : 0.014800671488046646 , 'label' : 'broom' }, { 'score' : 0.009150494821369648 , 'label' : 'swab, swob, mop' }, { 'score' : 0.0018255198374390602 , 'label' : \"plunger, plumber's helper\" }, { 'score' : 0.0017631321679800749 , 'label' : 'African grey, African gray, Psittacus erithacus' }], [{ 'score' : 0.9489120244979858 , 'label' : 'macaw' }, { 'score' : 0.014800671488046646 , 'label' : 'broom' }, { 'score' : 0.009150494821369648 , 'label' : 'swab, swob, mop' }, { 'score' : 0.0018255198374390602 , 'label' : \"plunger, plumber's helper\" }, { 'score' : 0.0017631321679800749 , 'label' : 'African grey, African gray, Psittacus erithacus' }]] Amazingly easy! Now let's try: Deploy the model \u00b6 Without deployment, how could a machine learning tutorial be complete? First, let's install Pinferencia . pip install \"pinferencia[uvicorn]\" Now let's create an app.py file with the codes: app.py 1 2 3 4 5 6 7 8 9 10 11 from transformers import pipeline from pinferencia import Server vision_classifier = pipeline ( task = \"image-classification\" ) def classify ( data ): return vision_classifier ( images = data ) service = Server () service . register ( model_name = \"vision\" , model = classify ) Easy, right? Predict \u00b6 Curl Python requests curl --location --request POST 'http://127.0.0.1:8000/v1/models/vision/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"https://cdn.pixabay.com/photo/2018/08/12/16/59/parrot-3601194_1280.jpg\" }' Result: Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/vision/predict\" , json = { \"data\" : \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\" # noqa }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) Run python test.py and result: Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] Even cooler, go to http://127.0.0.1:8000 , and you will have an interactive ui. You can send predict request just there! Improve it \u00b6 However, using the url of the image to predict sometimes is not appropriate. Let's modify the app.py a little bit to accept Base64 Encoded String as the input. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import base64 from io import BytesIO from PIL import Image from transformers import pipeline from pinferencia import Server vision_classifier = pipeline ( task = \"image-classification\" ) def classify ( image_base64_str ): image = Image . open ( BytesIO ( base64 . b64decode ( image_base64_str ))) return vision_classifier ( images = image ) service = Server () service . register ( model_name = \"vision\" , model = classify ) Predict Again \u00b6 Curl Python requests curl --location --request POST 'http://127.0.0.1:8000/v1/models/vision/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"...\" }' Result: Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/vision/predict\" , json = { \"data\" : \"...\" # noqa }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) Run python test.py and result: Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ]","title":"Image Classification"},{"location":"re/ml/huggingface/pipeline/vision/#prerequisite","text":"Please visit Dependencies","title":"Prerequisite"},{"location":"re/ml/huggingface/pipeline/vision/#download-the-model-and-predict","text":"The model will be automatically downloaded. 1 2 3 4 5 6 from transformers import pipeline vision_classifier = pipeline ( task = \"image-classification\" ) vision_classifier ( images = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\" ) Result: [{ 'label' : 'lynx, catamount' , 'score' : 0.4403027892112732 }, { 'label' : 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor' , 'score' : 0.03433405980467796 }, { 'label' : 'snow leopard, ounce, Panthera uncia' , 'score' : 0.032148055732250214 }, { 'label' : 'Egyptian cat' , 'score' : 0.02353910356760025 }, { 'label' : 'tiger cat' , 'score' : 0.023034192621707916 }] Let's try another image, and let's try predict two image in one batch: 1 2 3 4 image = \"https://cdn.pixabay.com/photo/2018/08/12/16/59/parrot-3601194_1280.jpg\" vision_classifier ( images = [ image , image ] ) Result: [[{ 'score' : 0.9489120244979858 , 'label' : 'macaw' }, { 'score' : 0.014800671488046646 , 'label' : 'broom' }, { 'score' : 0.009150494821369648 , 'label' : 'swab, swob, mop' }, { 'score' : 0.0018255198374390602 , 'label' : \"plunger, plumber's helper\" }, { 'score' : 0.0017631321679800749 , 'label' : 'African grey, African gray, Psittacus erithacus' }], [{ 'score' : 0.9489120244979858 , 'label' : 'macaw' }, { 'score' : 0.014800671488046646 , 'label' : 'broom' }, { 'score' : 0.009150494821369648 , 'label' : 'swab, swob, mop' }, { 'score' : 0.0018255198374390602 , 'label' : \"plunger, plumber's helper\" }, { 'score' : 0.0017631321679800749 , 'label' : 'African grey, African gray, Psittacus erithacus' }]] Amazingly easy! Now let's try:","title":"Download the model and\u00a0predict"},{"location":"re/ml/huggingface/pipeline/vision/#deploy-the-model","text":"Without deployment, how could a machine learning tutorial be complete? First, let's install Pinferencia . pip install \"pinferencia[uvicorn]\" Now let's create an app.py file with the codes: app.py 1 2 3 4 5 6 7 8 9 10 11 from transformers import pipeline from pinferencia import Server vision_classifier = pipeline ( task = \"image-classification\" ) def classify ( data ): return vision_classifier ( images = data ) service = Server () service . register ( model_name = \"vision\" , model = classify ) Easy, right?","title":"Deploy the\u00a0model"},{"location":"re/ml/huggingface/pipeline/vision/#predict","text":"Curl Python requests curl --location --request POST 'http://127.0.0.1:8000/v1/models/vision/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"https://cdn.pixabay.com/photo/2018/08/12/16/59/parrot-3601194_1280.jpg\" }' Result: Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/vision/predict\" , json = { \"data\" : \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\" # noqa }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) Run python test.py and result: Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] Even cooler, go to http://127.0.0.1:8000 , and you will have an interactive ui. You can send predict request just there!","title":"Predict"},{"location":"re/ml/huggingface/pipeline/vision/#improve-it","text":"However, using the url of the image to predict sometimes is not appropriate. Let's modify the app.py a little bit to accept Base64 Encoded String as the input. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import base64 from io import BytesIO from PIL import Image from transformers import pipeline from pinferencia import Server vision_classifier = pipeline ( task = \"image-classification\" ) def classify ( image_base64_str ): image = Image . open ( BytesIO ( base64 . b64decode ( image_base64_str ))) return vision_classifier ( images = image ) service = Server () service . register ( model_name = \"vision\" , model = classify )","title":"Improve it"},{"location":"re/ml/huggingface/pipeline/vision/#predict-again","text":"Curl Python requests curl --location --request POST 'http://127.0.0.1:8000/v1/models/vision/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"...\" }' Result: Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/vision/predict\" , json = { \"data\" : \"...\" # noqa }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) Run python test.py and result: Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ]","title":"Predict Again"},{"location":"re/models/home/","text":"Model? \u00b6 What is a Model ? Generally, it is a way to calculate something that is more complicated than a equation. Should it be a file? Perhaps. Could it be a python object? Of course. In Pinferencia , a model is just a piece of codes that can get called. A function, or an instance of a class just like those pytorch models.","title":"About Models"},{"location":"re/models/home/#model","text":"What is a Model ? Generally, it is a way to calculate something that is more complicated than a equation. Should it be a file? Perhaps. Could it be a python object? Of course. In Pinferencia , a model is just a piece of codes that can get called. A function, or an instance of a class just like those pytorch models.","title":"Model?"},{"location":"re/models/machine-learning/","text":"Machine Learning Frameworks \u00b6 Here is how to load models from different framework: Scikit-Learn PyTorch Tensorflow Any Model Any Function app.py import joblib from pinferencia import Server # train your model model = \"...\" # or load your model model = joblib . load ( \"/path/to/model.joblib\" ) # (1) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , # (2) ) For more details, please visit https://scikit-learn.org/stable/modules/model_persistence.html entrypoint is the function name of the model to perform predictions. Here the data will be sent to the predict function: model.predict(data) . app.py import torch from pinferencia import Server # train your models model = \"...\" # or load your models (1) # from state_dict model = TheModelClass ( * args , ** kwargs ) model . load_state_dict ( torch . load ( PATH )) # entire model model = torch . load ( PATH ) # torchscript model = torch . jit . load ( 'model_scripted.pt' ) model . eval () service = Server () service . register ( model_name = \"mymodel\" , model = model , ) For more details, please visit https://pytorch.org/tutorials/beginner/saving_loading_models.html app.py import tensorflow as tf from pinferencia import Server # train your models model = \"...\" # or load your models (1) # saved_model model = tf . keras . models . load_model ( 'saved_model/model' ) # HDF5 model = tf . keras . models . load_model ( 'model.h5' ) # from weights model = create_model () model . load_weights ( './checkpoints/my_checkpoint' ) loss , acc = model . evaluate ( test_images , test_labels , verbose = 2 ) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) For more details, please visit https://www.tensorflow.org/tutorials/keras/save_and_load app.py from pinferencia import Server class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) app.py from pinferencia import Server def model ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = model , )","title":"Other Machine Learning Frameworks"},{"location":"re/models/machine-learning/#machine-learning-frameworks","text":"Here is how to load models from different framework: Scikit-Learn PyTorch Tensorflow Any Model Any Function app.py import joblib from pinferencia import Server # train your model model = \"...\" # or load your model model = joblib . load ( \"/path/to/model.joblib\" ) # (1) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , # (2) ) For more details, please visit https://scikit-learn.org/stable/modules/model_persistence.html entrypoint is the function name of the model to perform predictions. Here the data will be sent to the predict function: model.predict(data) . app.py import torch from pinferencia import Server # train your models model = \"...\" # or load your models (1) # from state_dict model = TheModelClass ( * args , ** kwargs ) model . load_state_dict ( torch . load ( PATH )) # entire model model = torch . load ( PATH ) # torchscript model = torch . jit . load ( 'model_scripted.pt' ) model . eval () service = Server () service . register ( model_name = \"mymodel\" , model = model , ) For more details, please visit https://pytorch.org/tutorials/beginner/saving_loading_models.html app.py import tensorflow as tf from pinferencia import Server # train your models model = \"...\" # or load your models (1) # saved_model model = tf . keras . models . load_model ( 'saved_model/model' ) # HDF5 model = tf . keras . models . load_model ( 'model.h5' ) # from weights model = create_model () model . load_weights ( './checkpoints/my_checkpoint' ) loss , acc = model . evaluate ( test_images , test_labels , verbose = 2 ) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) For more details, please visit https://www.tensorflow.org/tutorials/keras/save_and_load app.py from pinferencia import Server class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) app.py from pinferencia import Server def model ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = model , )","title":"Machine Learning Frameworks"},{"location":"re/models/register/","text":"Register \u00b6 Registering a model is as easy as: 1 2 3 4 5 service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) Register Multiple Model and Multiple Versions? You can register multiple models with multiple versions: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 service . register ( model_name = \"my-model\" , model = my_model , entrypoint = \"predict\" , ) service . register ( model_name = \"my-model\" , model = my_model_v1 , entrypoint = \"predict\" , version_name = \"v1, ) service . register ( model_name = \"your-model\" , model = your_model , entrypoint = \"predict\" , ) service . register ( model_name = \"your-model\" , model = your_model_v1 , entrypoint = \"predict\" , version_name = \"v1, ) service . register ( model_name = \"your-model\" , model = your_model_v2 , entrypoint = \"predict\" , version_name = \"v2, ) Parameters \u00b6 Parameter Type Default Details model_name str Name of the model model object Model object or path version_name str None Name of the version entrypoint str None Name of the function to use metadata dict None Metadata of the model handler object None A class to handler model loading and predicting load_now bool True Whether loading the model on registration Examples \u00b6 Model Name \u00b6 1 2 3 4 5 6 7 8 9 10 11 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , ) Model \u00b6 Model Object Function 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from pinferencia import Server class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict ) 1 2 3 4 5 6 7 8 9 10 11 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , ) Version Name \u00b6 Model without version name will be registered as default version. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pinferencia import Server def add ( data ): return data [ 0 ] + data [ 1 ] def substract ( data ): return data [ 0 ] + data [ 1 ] service = Server () service . register ( model_name = \"mymodel\" , model = add , version_name = \"add\" , # (1) ) service . register ( model_name = \"mymodel\" , model = substract , version_name = \"substract\" , # (2) ) The predicting endpoint will be http://127.0.0.1/v1/models/mymodel/versions/add/predict The predicting endpoint will be http://127.0.0.1/v1/models/mymodel/versions/substract/predict Entrypoint \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pinferencia import Server class MyModel : def add ( self , data ): return data [ 0 ] + data [ 1 ] def substract ( self , data ): return data [ 0 ] - data [ 1 ] model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , version_name = \"add\" , # (1) entrypoint = \"add\" , # (3) ) service . register ( model_name = \"mymodel\" , model = model , version_name = \"substract\" , # (2) entrypoint = \"substract\" , # (4) ) The predicting endpoint will be http://127.0.0.1/v1/models/mymodel/versions/add/predict The predicting endpoint will be http://127.0.0.1/v1/models/mymodel/versions/substract/predict add function of the model will be used to predict. substract function of the model will be used to predict. Metadata \u00b6 Default API \u00b6 Pinferencia default metadata schema supports platform and device These are information for display purpose only. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , metadata = { \"platform\" : \"Linux\" , \"device\" : \"CPU+GPU\" , } ) Kserve API \u00b6 Pinferencia also supports Kserve API. For Kserve V2, the metadata supports: - platform - inputs - outputs The inputs and outputs metadata will determine the data and datatype model received and returned. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server ( api = \"kserve\" ) # (1) service . register ( model_name = \"mymodel\" , model = predict , metadata = { \"platform\" : \"mac os\" , \"inputs\" : [ { \"name\" : \"integers\" , # (2) \"datatype\" : \"int64\" , \"shape\" : [ 1 ], \"data\" : [ 1 , 2 , 3 ], } ], \"outputs\" : [ { \"name\" : \"sum\" , \"datatype\" : \"int64\" , \"shape\" : - 1 , \"data\" : 6 }, # (3) { \"name\" : \"product\" , \"datatype\" : \"int64\" , \"shape\" : - 1 , \"data\" : 6 }, ], } ) If you want to use kserve API, you need to set api=\"kserve\" when initializing the service. In the request, if there are multiple inputs, only input with name intergers will be passed to the model. Output data will be converted into int64 . The datatype field only supports numpy data type. If the data cannot be converted, there will be an extra error field in the output, indicating the reason of the failure. Handler \u00b6 Details of handlers can be found at Handlers . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pinferencia import Server from pinferencia.handlers import PickleHandler class MyPrintHandler ( PickleHandler ): def predict ( self , data ): print ( data ) return self . model . predict ( data ) def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , handler = MyPrintHandler ) Load Now \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import joblib from pinferencia import Server class JoblibHandler ( BaseHandler ): def load_model ( self ): return joblib . load ( self . model_path ) service = Server ( model_dir = \"/opt/models\" ) service . register ( model_name = \"mymodel\" , model = \"/path/to/model.joblib\" , entrypoint = \"predict\" , handler = JoblibHandler , load_now = True , )","title":"Register Models"},{"location":"re/models/register/#register","text":"Registering a model is as easy as: 1 2 3 4 5 service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) Register Multiple Model and Multiple Versions? You can register multiple models with multiple versions: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 service . register ( model_name = \"my-model\" , model = my_model , entrypoint = \"predict\" , ) service . register ( model_name = \"my-model\" , model = my_model_v1 , entrypoint = \"predict\" , version_name = \"v1, ) service . register ( model_name = \"your-model\" , model = your_model , entrypoint = \"predict\" , ) service . register ( model_name = \"your-model\" , model = your_model_v1 , entrypoint = \"predict\" , version_name = \"v1, ) service . register ( model_name = \"your-model\" , model = your_model_v2 , entrypoint = \"predict\" , version_name = \"v2, )","title":"Register"},{"location":"re/models/register/#parameters","text":"Parameter Type Default Details model_name str Name of the model model object Model object or path version_name str None Name of the version entrypoint str None Name of the function to use metadata dict None Metadata of the model handler object None A class to handler model loading and predicting load_now bool True Whether loading the model on registration","title":"Parameters"},{"location":"re/models/register/#examples","text":"","title":"Examples"},{"location":"re/models/register/#model-name","text":"1 2 3 4 5 6 7 8 9 10 11 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , )","title":"Model Name"},{"location":"re/models/register/#model","text":"Model Object Function 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from pinferencia import Server class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict ) 1 2 3 4 5 6 7 8 9 10 11 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , )","title":"Model"},{"location":"re/models/register/#version-name","text":"Model without version name will be registered as default version. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pinferencia import Server def add ( data ): return data [ 0 ] + data [ 1 ] def substract ( data ): return data [ 0 ] + data [ 1 ] service = Server () service . register ( model_name = \"mymodel\" , model = add , version_name = \"add\" , # (1) ) service . register ( model_name = \"mymodel\" , model = substract , version_name = \"substract\" , # (2) ) The predicting endpoint will be http://127.0.0.1/v1/models/mymodel/versions/add/predict The predicting endpoint will be http://127.0.0.1/v1/models/mymodel/versions/substract/predict","title":"Version Name"},{"location":"re/models/register/#entrypoint","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pinferencia import Server class MyModel : def add ( self , data ): return data [ 0 ] + data [ 1 ] def substract ( self , data ): return data [ 0 ] - data [ 1 ] model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , version_name = \"add\" , # (1) entrypoint = \"add\" , # (3) ) service . register ( model_name = \"mymodel\" , model = model , version_name = \"substract\" , # (2) entrypoint = \"substract\" , # (4) ) The predicting endpoint will be http://127.0.0.1/v1/models/mymodel/versions/add/predict The predicting endpoint will be http://127.0.0.1/v1/models/mymodel/versions/substract/predict add function of the model will be used to predict. substract function of the model will be used to predict.","title":"Entrypoint"},{"location":"re/models/register/#metadata","text":"","title":"Metadata"},{"location":"re/models/register/#default-api","text":"Pinferencia default metadata schema supports platform and device These are information for display purpose only. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , metadata = { \"platform\" : \"Linux\" , \"device\" : \"CPU+GPU\" , } )","title":"Default API"},{"location":"re/models/register/#kserve-api","text":"Pinferencia also supports Kserve API. For Kserve V2, the metadata supports: - platform - inputs - outputs The inputs and outputs metadata will determine the data and datatype model received and returned. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server ( api = \"kserve\" ) # (1) service . register ( model_name = \"mymodel\" , model = predict , metadata = { \"platform\" : \"mac os\" , \"inputs\" : [ { \"name\" : \"integers\" , # (2) \"datatype\" : \"int64\" , \"shape\" : [ 1 ], \"data\" : [ 1 , 2 , 3 ], } ], \"outputs\" : [ { \"name\" : \"sum\" , \"datatype\" : \"int64\" , \"shape\" : - 1 , \"data\" : 6 }, # (3) { \"name\" : \"product\" , \"datatype\" : \"int64\" , \"shape\" : - 1 , \"data\" : 6 }, ], } ) If you want to use kserve API, you need to set api=\"kserve\" when initializing the service. In the request, if there are multiple inputs, only input with name intergers will be passed to the model. Output data will be converted into int64 . The datatype field only supports numpy data type. If the data cannot be converted, there will be an extra error field in the output, indicating the reason of the failure.","title":"Kserve API"},{"location":"re/models/register/#handler","text":"Details of handlers can be found at Handlers . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pinferencia import Server from pinferencia.handlers import PickleHandler class MyPrintHandler ( PickleHandler ): def predict ( self , data ): print ( data ) return self . model . predict ( data ) def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , handler = MyPrintHandler )","title":"Handler"},{"location":"re/models/register/#load-now","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import joblib from pinferencia import Server class JoblibHandler ( BaseHandler ): def load_model ( self ): return joblib . load ( self . model_path ) service = Server ( model_dir = \"/opt/models\" ) service . register ( model_name = \"mymodel\" , model = \"/path/to/model.joblib\" , entrypoint = \"predict\" , handler = JoblibHandler , load_now = True , )","title":"Load Now"},{"location":"re/overview/","text":"Welcome to Pinferencia \u00b6 What is Pinferencia? \u00b6 Never heard of Pinferencia ? No one's gonna blame ya. No coins in my pocket, can't put this in the rocket. Lots of models you have got, serve them online, not a easy job. Now you've got Pinferencia, all you need is to say \"abracadabra\". Pinferencia ( python + inference ) aims to provide the simplest way to serve any of your deep learning models with a fully functioning Rest API. Straight forward. Simple. Powerful. $ pip install \"pinferencia[uvicorn]\" ---> 100% Features \u00b6 Pinferencia features include: Fast to code, fast to go alive . Minimal codes to write, minimum codes modifications needed. Just based on what you have. 100% Test Coverage : Both statement and branch coverages, no kidding. Easy to use, easy to understand . Automatic API documentation page . All API explained in details with online try-out feature. Thanks to FastAPI and Starlette . Serve any model , even a single function can be served. Try it now! \u00b6 Create the App \u00b6 Scikit-Learn PyTorch Tensorflow HuggingFace Transformer Any Model Any Function app.py import joblib import uvicorn from pinferencia import Server # train your model model = \"...\" # or load your model model = joblib . load ( \"/path/to/model.joblib\" ) # (1) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , # (2) ) For more details, please visit https://scikit-learn.org/stable/modules/model_persistence.html entrypoint is the function name of the model to perform predictions. Here the data will be sent to the predict function: model.predict(data) . app.py import torch from pinferencia import Server # train your models model = \"...\" # or load your models (1) # from state_dict model = TheModelClass ( * args , ** kwargs ) model . load_state_dict ( torch . load ( PATH )) # entire model model = torch . load ( PATH ) # torchscript model = torch . jit . load ( 'model_scripted.pt' ) model . eval () service = Server () service . register ( model_name = \"mymodel\" , model = model , ) For more details, please visit https://pytorch.org/tutorials/beginner/saving_loading_models.html app.py import tensorflow as tf from pinferencia import Server # train your models model = \"...\" # or load your models (1) # saved_model model = tf . keras . models . load_model ( 'saved_model/model' ) # HDF5 model = tf . keras . models . load_model ( 'model.h5' ) # from weights model = create_model () model . load_weights ( './checkpoints/my_checkpoint' ) loss , acc = model . evaluate ( test_images , test_labels , verbose = 2 ) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) For more details, please visit https://www.tensorflow.org/tutorials/keras/save_and_load app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from transformers import pipeline from pinferencia import Server vision_classifier = pipeline ( task = \"image-classification\" ) def predict ( data ): return vision_classifier ( images = data ) service = Server () service . register ( model_name = \"vision\" , model = predict ) app.py from pinferencia import Server class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) app.py from pinferencia import Server def model ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = model , ) Run! \u00b6 $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) Hooray , your service is alive. Go to http://127.0.0.1/ and have fun.","title":"Overview"},{"location":"re/overview/#welcome-to-pinferencia","text":"","title":"Welcome to Pinferencia"},{"location":"re/overview/#what-is-pinferencia","text":"Never heard of Pinferencia ? No one's gonna blame ya. No coins in my pocket, can't put this in the rocket. Lots of models you have got, serve them online, not a easy job. Now you've got Pinferencia, all you need is to say \"abracadabra\". Pinferencia ( python + inference ) aims to provide the simplest way to serve any of your deep learning models with a fully functioning Rest API. Straight forward. Simple. Powerful. $ pip install \"pinferencia[uvicorn]\" ---> 100%","title":"What is Pinferencia?"},{"location":"re/overview/#features","text":"Pinferencia features include: Fast to code, fast to go alive . Minimal codes to write, minimum codes modifications needed. Just based on what you have. 100% Test Coverage : Both statement and branch coverages, no kidding. Easy to use, easy to understand . Automatic API documentation page . All API explained in details with online try-out feature. Thanks to FastAPI and Starlette . Serve any model , even a single function can be served.","title":"Features"},{"location":"re/overview/#try-it-now","text":"","title":"Try it now!"},{"location":"re/overview/#create-the-app","text":"Scikit-Learn PyTorch Tensorflow HuggingFace Transformer Any Model Any Function app.py import joblib import uvicorn from pinferencia import Server # train your model model = \"...\" # or load your model model = joblib . load ( \"/path/to/model.joblib\" ) # (1) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , # (2) ) For more details, please visit https://scikit-learn.org/stable/modules/model_persistence.html entrypoint is the function name of the model to perform predictions. Here the data will be sent to the predict function: model.predict(data) . app.py import torch from pinferencia import Server # train your models model = \"...\" # or load your models (1) # from state_dict model = TheModelClass ( * args , ** kwargs ) model . load_state_dict ( torch . load ( PATH )) # entire model model = torch . load ( PATH ) # torchscript model = torch . jit . load ( 'model_scripted.pt' ) model . eval () service = Server () service . register ( model_name = \"mymodel\" , model = model , ) For more details, please visit https://pytorch.org/tutorials/beginner/saving_loading_models.html app.py import tensorflow as tf from pinferencia import Server # train your models model = \"...\" # or load your models (1) # saved_model model = tf . keras . models . load_model ( 'saved_model/model' ) # HDF5 model = tf . keras . models . load_model ( 'model.h5' ) # from weights model = create_model () model . load_weights ( './checkpoints/my_checkpoint' ) loss , acc = model . evaluate ( test_images , test_labels , verbose = 2 ) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) For more details, please visit https://www.tensorflow.org/tutorials/keras/save_and_load app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from transformers import pipeline from pinferencia import Server vision_classifier = pipeline ( task = \"image-classification\" ) def predict ( data ): return vision_classifier ( images = data ) service = Server () service . register ( model_name = \"vision\" , model = predict ) app.py from pinferencia import Server class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) app.py from pinferencia import Server def model ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = model , )","title":"Create the App"},{"location":"re/overview/#run","text":"$ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) Hooray , your service is alive. Go to http://127.0.0.1/ and have fun.","title":"Run!"},{"location":"re/pinferencia-is-different/","text":"Why is Pinferencia different? \u00b6 Different? \u00b6 Actually, it is not something different. It is something more intuitive, more straight forward or just more simple. How do you serve a model yesterday? Write some script, save a model file, or do something else according to the tools' requirements . And you spend a lot of time to understand those requirements. And a lot time to get it right. Once finished, you're so relieved. However, after almost half a year, you've got new and more complicated models and serve them again using your previous tool. What's in your mind now? No way!!!!!!!!!!!! You have your model, you train it in python , and you predict in python . You even write complicated python codes to perform more difficult tasks . How many changes you need to make and how many extra codes you need to write to get your model served using those tools or platforms? The answer is A lot . With Pinferencia \u00b6 You don't need to do any of these. You just use the model in your own python code. It doesn't matter whether the model is - a PyTorch model or - a Tensorflow model or - any machine learning model or - simply your own codes or - just your own functions . Register the model/function, and Pinferencia will use it to predict, in the way just as expected . Simple, and Powerful \u00b6 Pinferencia aims to be the simplest AI model inference server! Serving a model has never been so easy. If you want to find a simple but robust way to serve your model write minimal codes while maintain controls over you service avoid those heavy tools or platforms You're at the right place.","title":"Pinferencia is different?"},{"location":"re/pinferencia-is-different/#why-is-pinferencia-different","text":"","title":"Why is Pinferencia different?"},{"location":"re/pinferencia-is-different/#different","text":"Actually, it is not something different. It is something more intuitive, more straight forward or just more simple. How do you serve a model yesterday? Write some script, save a model file, or do something else according to the tools' requirements . And you spend a lot of time to understand those requirements. And a lot time to get it right. Once finished, you're so relieved. However, after almost half a year, you've got new and more complicated models and serve them again using your previous tool. What's in your mind now?","title":"Different?"},{"location":"re/pinferencia-is-different/#with-pinferencia","text":"You don't need to do any of these. You just use the model in your own python code. It doesn't matter whether the model is - a PyTorch model or - a Tensorflow model or - any machine learning model or - simply your own codes or - just your own functions . Register the model/function, and Pinferencia will use it to predict, in the way just as expected .","title":"With Pinferencia"},{"location":"re/pinferencia-is-different/#simple-and-powerful","text":"Pinferencia aims to be the simplest AI model inference server! Serving a model has never been so easy. If you want to find a simple but robust way to serve your model write minimal codes while maintain controls over you service avoid those heavy tools or platforms You're at the right place.","title":"Simple, and Powerful"},{"location":"re/restapi/","text":"REST API \u00b6 Overview \u00b6 Pinferencia has two built-in API sets: Default API Kserve API from pinferencia import Server service = Server () # or service = Server ( api = \"default\" ) from pinferencia import Server service = Server ( api = \"kserve\" ) Are you using other serving tools now? If you also use other model serving tools, here are the Kserve API versions the tools support: Name API Pinferencia Kserve V1 & V2 TF Serving Kserve V1 TorchServe Kserve V1 or V2 Triton Kserve V2 KServe Kserve V1 No Pain, Just Gain \u00b6 As you can see You can switch between Pinferencia and other tools with almost no code changes in client. If you want to use Pinferencia for prototyping and client building, then use other tools in production, you got it supported out of the box. You can use Pinferencia in production with other tools with the same API set. If you're switching from Kserve V1 to Kserve V2 and you need a server supporting both during the transition, you got Pinferencia . So, no pain, just gain. Default API \u00b6 Path Method Summary /v1/healthz GET Healthz /v1/models GET List Models /v1/models/{model_name} GET List Model Versions /v1/models/{model_name}/ready GET Model Is Ready /v1/models/{model_name}/versions/{version_name}/ready GET Model Version Is Ready /v1/models/{model_name}/load POST Load Model /v1/models/{model_name}/versions/{version_name}/load POST Load Version /v1/models/{model_name}/unload POST Unload Model /v1/models/{model_name}/versions/{version_name}/unload POST Unload Version /v1/models/{model_name}/predict POST Model Predict /v1/models/{model_name}/versions/{version_name}/predict POST Model Version Predict Kserve API \u00b6 Path Method Summary /v1/healthz GET Healthz /v1/models GET List Models /v1/models/{model_name} GET List Model Versions /v1/models/{model_name}/ready GET Model Is Ready /v1/models/{model_name}/versions/{version_name}/ready GET Model Version Is Ready /v1/models/{model_name}/load POST Load Model /v1/models/{model_name}/versions/{version_name}/load POST Load Version /v1/models/{model_name}/unload POST Unload Model /v1/models/{model_name}/versions/{version_name}/unload POST Unload Version /v1/models/{model_name}/infer POST Model Predict /v1/models/{model_name}/versions/{version_name}/infer POST Model Version Predict /v2/healthz GET Healthz /v2/models GET List Models /v2/models/{model_name} GET List Model Versions /v2/models/{model_name}/ready GET Model Is Ready /v2/models/{model_name}/versions/{version_name}/ready GET Model Version Is Ready /v2/models/{model_name}/load POST Load Model /v2/models/{model_name}/versions/{version_name}/load POST Load Version /v2/models/{model_name}/unload POST Unload Model /v2/models/{model_name}/versions/{version_name}/unload POST Unload Version /v2/models/{model_name}/infer POST Model Predict /v2/models/{model_name}/versions/{version_name}/infer POST Model Version Predict","title":"REST API"},{"location":"re/restapi/#rest-api","text":"","title":"REST API"},{"location":"re/restapi/#overview","text":"Pinferencia has two built-in API sets: Default API Kserve API from pinferencia import Server service = Server () # or service = Server ( api = \"default\" ) from pinferencia import Server service = Server ( api = \"kserve\" ) Are you using other serving tools now? If you also use other model serving tools, here are the Kserve API versions the tools support: Name API Pinferencia Kserve V1 & V2 TF Serving Kserve V1 TorchServe Kserve V1 or V2 Triton Kserve V2 KServe Kserve V1","title":"Overview"},{"location":"re/restapi/#no-pain-just-gain","text":"As you can see You can switch between Pinferencia and other tools with almost no code changes in client. If you want to use Pinferencia for prototyping and client building, then use other tools in production, you got it supported out of the box. You can use Pinferencia in production with other tools with the same API set. If you're switching from Kserve V1 to Kserve V2 and you need a server supporting both during the transition, you got Pinferencia . So, no pain, just gain.","title":"No Pain, Just Gain"},{"location":"re/restapi/#default-api","text":"Path Method Summary /v1/healthz GET Healthz /v1/models GET List Models /v1/models/{model_name} GET List Model Versions /v1/models/{model_name}/ready GET Model Is Ready /v1/models/{model_name}/versions/{version_name}/ready GET Model Version Is Ready /v1/models/{model_name}/load POST Load Model /v1/models/{model_name}/versions/{version_name}/load POST Load Version /v1/models/{model_name}/unload POST Unload Model /v1/models/{model_name}/versions/{version_name}/unload POST Unload Version /v1/models/{model_name}/predict POST Model Predict /v1/models/{model_name}/versions/{version_name}/predict POST Model Version Predict","title":"Default API"},{"location":"re/restapi/#kserve-api","text":"Path Method Summary /v1/healthz GET Healthz /v1/models GET List Models /v1/models/{model_name} GET List Model Versions /v1/models/{model_name}/ready GET Model Is Ready /v1/models/{model_name}/versions/{version_name}/ready GET Model Version Is Ready /v1/models/{model_name}/load POST Load Model /v1/models/{model_name}/versions/{version_name}/load POST Load Version /v1/models/{model_name}/unload POST Unload Model /v1/models/{model_name}/versions/{version_name}/unload POST Unload Version /v1/models/{model_name}/infer POST Model Predict /v1/models/{model_name}/versions/{version_name}/infer POST Model Version Predict /v2/healthz GET Healthz /v2/models GET List Models /v2/models/{model_name} GET List Model Versions /v2/models/{model_name}/ready GET Model Is Ready /v2/models/{model_name}/versions/{version_name}/ready GET Model Version Is Ready /v2/models/{model_name}/load POST Load Model /v2/models/{model_name}/versions/{version_name}/load POST Load Version /v2/models/{model_name}/unload POST Unload Model /v2/models/{model_name}/versions/{version_name}/unload POST Unload Version /v2/models/{model_name}/infer POST Model Predict /v2/models/{model_name}/versions/{version_name}/infer POST Model Version Predict","title":"Kserve API"},{"location":"zh/","text":"","title":"\u9996\u9875"},{"location":"zh/frontend/requirements/","text":"Requirements \u00b6 To use Pinferencia's frontend with your model, there are some requirements of your model's predict function. Templates \u00b6 Currently, there are mainly two major catogory of the template's inputs and outputs. More (audio and video) will be supported in the future. Base Templates \u00b6 Template Input Output Text to Text Text Text Text to Image Text Image Image to Text Image Text Camera to Text Image Text Image to Image Image Image Camera to Image Image Image Derived Templates \u00b6 Template Input Output Translation Text Text Image Classification Image Text Image Style Transfer Image Image Input \u00b6 The predict function must be able to accept a list of data as inputs. For text input, the input will be a list of strings. For image input, the input will be a list of strings representing the base64 encoded images. Output \u00b6 The predict function must produce a list of data as outputs. For text output, the output must be a list. For image output, the output must be a list of strings representing the base64 encoded images. Text Output The frontend will try to parse the outputs into table, json or pure text. Table Text JSON If the output is similar to below: [ [ { \"a\" : 1 , \"b\" : 2 }, { \"a\" : 3 , \"b\" : 4 }, { \"a\" : 5 , \"b\" : 6 } ] ] It will be displayed as a table. If the output is similar to below: [ \"Text output.\" ] It will be displayed as a text. All other format of outputs will be displayed as a JSON.","title":"\u524d\u7aef\u4f7f\u7528\u6761\u4ef6"},{"location":"zh/frontend/requirements/#requirements","text":"To use Pinferencia's frontend with your model, there are some requirements of your model's predict function.","title":"Requirements"},{"location":"zh/frontend/requirements/#templates","text":"Currently, there are mainly two major catogory of the template's inputs and outputs. More (audio and video) will be supported in the future.","title":"Templates"},{"location":"zh/frontend/requirements/#base-templates","text":"Template Input Output Text to Text Text Text Text to Image Text Image Image to Text Image Text Camera to Text Image Text Image to Image Image Image Camera to Image Image Image","title":"Base Templates"},{"location":"zh/frontend/requirements/#derived-templates","text":"Template Input Output Translation Text Text Image Classification Image Text Image Style Transfer Image Image","title":"Derived Templates"},{"location":"zh/frontend/requirements/#input","text":"The predict function must be able to accept a list of data as inputs. For text input, the input will be a list of strings. For image input, the input will be a list of strings representing the base64 encoded images.","title":"Input"},{"location":"zh/frontend/requirements/#output","text":"The predict function must produce a list of data as outputs. For text output, the output must be a list. For image output, the output must be a list of strings representing the base64 encoded images. Text Output The frontend will try to parse the outputs into table, json or pure text. Table Text JSON If the output is similar to below: [ [ { \"a\" : 1 , \"b\" : 2 }, { \"a\" : 3 , \"b\" : 4 }, { \"a\" : 5 , \"b\" : 6 } ] ] It will be displayed as a table. If the output is similar to below: [ \"Text output.\" ] It will be displayed as a text. All other format of outputs will be displayed as a JSON.","title":"Output"},{"location":"zh/get-started/home/","text":"\u5feb\u901f\u4e0a\u624b \u00b6 Pinferencia\u7684\u76ee\u6807 Pinferencia, \u81f4\u529b\u4e8e\u63d0\u4f9b\u6700\u7b80\u5355\u76f4\u63a5\u7684\u65b9\u5f0f\u8ba9\u4f60\u7684\u6a21\u578b\u62e5\u6709API\u670d\u52a1\u3002 \u8fd9\u91cc\u4e0d\u662f\u4e3a\u4e86\u90a3\u4e9b\u7cbe\u901a\u7f51\u7edc\u7f16\u7a0b\u7684\u4eba\u51c6\u5907\u7684\uff0c\u66f4\u591a\u662f\u4e3a\u4e86\u5927\u591a\u6570\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u670b\u53cb\u3002 \u4e0d\u7ba1\u4f60\u662f\u60f3\u505ademo\uff0c\u8fd8\u662f\u60f3\u7ed9\u516c\u53f8\u6216\u5b66\u6821\u63d0\u4f9b\u5185\u90e8\u63a5\u53e3\uff0c\u751a\u81f3\u66f4\u8fdb\u4e00\u6b65\u96c6\u6210\u5230CICD\uff0c\u76f4\u63a5\u90e8\u7f72\u4e00\u4e2a\u670d\u52a1\u5230\u4e91\u7aef\uff0c\u4f60\u90fd\u80fd\u5728\u8fd9\u91cc\u627e\u5230\u89e3\u7b54\u3002 \u901a\u8fc7\u8fd9\u4e00\u7cfb\u5217\u6559\u7a0b\uff0c\u4f60\u5c06\u638c\u63e1\u5982\u4f55\uff1a \u542f\u52a8\u4e00\u4e2a JSONModel \u670d\u52a1 \u542f\u52a8\u4e00\u4e2a Function \u670d\u52a1 \u5982\u4f55\u4f7f\u7528\u7528\u4e24\u79cd\u65b9\u6cd5\u542f\u52a8 PyTorch MNIST \u6a21\u578b, \u7528 MNIST \u641e\u70b9\u4e50\u5b50","title":"\u4ecb\u7ecd"},{"location":"zh/get-started/home/#_1","text":"Pinferencia\u7684\u76ee\u6807 Pinferencia, \u81f4\u529b\u4e8e\u63d0\u4f9b\u6700\u7b80\u5355\u76f4\u63a5\u7684\u65b9\u5f0f\u8ba9\u4f60\u7684\u6a21\u578b\u62e5\u6709API\u670d\u52a1\u3002 \u8fd9\u91cc\u4e0d\u662f\u4e3a\u4e86\u90a3\u4e9b\u7cbe\u901a\u7f51\u7edc\u7f16\u7a0b\u7684\u4eba\u51c6\u5907\u7684\uff0c\u66f4\u591a\u662f\u4e3a\u4e86\u5927\u591a\u6570\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u670b\u53cb\u3002 \u4e0d\u7ba1\u4f60\u662f\u60f3\u505ademo\uff0c\u8fd8\u662f\u60f3\u7ed9\u516c\u53f8\u6216\u5b66\u6821\u63d0\u4f9b\u5185\u90e8\u63a5\u53e3\uff0c\u751a\u81f3\u66f4\u8fdb\u4e00\u6b65\u96c6\u6210\u5230CICD\uff0c\u76f4\u63a5\u90e8\u7f72\u4e00\u4e2a\u670d\u52a1\u5230\u4e91\u7aef\uff0c\u4f60\u90fd\u80fd\u5728\u8fd9\u91cc\u627e\u5230\u89e3\u7b54\u3002 \u901a\u8fc7\u8fd9\u4e00\u7cfb\u5217\u6559\u7a0b\uff0c\u4f60\u5c06\u638c\u63e1\u5982\u4f55\uff1a \u542f\u52a8\u4e00\u4e2a JSONModel \u670d\u52a1 \u542f\u52a8\u4e00\u4e2a Function \u670d\u52a1 \u5982\u4f55\u4f7f\u7528\u7528\u4e24\u79cd\u65b9\u6cd5\u542f\u52a8 PyTorch MNIST \u6a21\u578b, \u7528 MNIST \u641e\u70b9\u4e50\u5b50","title":"\u5feb\u901f\u4e0a\u624b"},{"location":"zh/get-started/other-models/","text":"\u4e0b\u4e00\u6b65 \u00b6 \u597d\u5427\uff0c\u6211\u6562\u6253\u8d4c\uff0c\u60a8\u5728\u4e0a\u4e00\u6559\u7a0b\u4e2d\u4f7f\u7528 PyTorch MNIST \u6a21\u578b\u4e00\u5b9a\u4f1a\u5f88\u6709\u8da3\u3002 \u6211\u60f3\u4f60\u73b0\u5728\u5bf9 Pinferencia \u5f88\u719f\u6089\u4e86\u3002 Pinferencia \u53ef\u4ee5\u4ee5\u975e\u5e38\u76f4\u63a5\u7684\u65b9\u5f0f\u4e3a\u4efb\u4f55\u53ef\u8c03\u7528\u5bf9\u8c61\u63d0\u4f9b\u670d\u52a1\u3002\u975e\u5e38\u7b80\u5355\u76f4\u63a5\u3002 \u5e76\u4e14\u8fd9\u5f88\u5bb9\u6613\u4e0e\u60a8\u73b0\u6709\u7684\u4ee3\u7801\u96c6\u6210\u3002 \u8fd9\u5c31\u662f Pinferencia \u7684\u8bbe\u8ba1\u76ee\u7684\u3002 \u6700\u5c11\u7684\u4ee3\u7801\u4fee\u6539 \u3002 \u73b0\u5728\u60a8\u53ef\u4ee5\u4ece \u4efb\u4f55\u6846\u67b6 \u63d0\u4f9b\u6a21\u578b\uff0c\u751a\u81f3\u53ef\u4ee5 \u5c06\u5b83\u4eec\u6df7\u5408\u5728\u4e00\u8d77 \u3002\u60a8\u53ef\u4ee5\u5728\u4e00\u4e2aAPI\u91cc \u540c\u65f6 \u4f7f\u7528\u6765\u81ea \u4e0d\u540c\u6846\u67b6 \u7684 \u4e0d\u540c\u6a21\u578b \uff01 \u73a9\u7684\u5f00\u5fc3\uff01 \u5982\u679c\u4f60\u559c\u6b22 Pinferencia \uff0c\u522b\u5fd8\u4e86\u53bb Github \u5e76\u7ed9\u4e00\u4e2a\u661f\u3002\u8c22\u8c22\u4f60\u3002","title":"\u4e0b\u4e00\u6b65"},{"location":"zh/get-started/other-models/#_1","text":"\u597d\u5427\uff0c\u6211\u6562\u6253\u8d4c\uff0c\u60a8\u5728\u4e0a\u4e00\u6559\u7a0b\u4e2d\u4f7f\u7528 PyTorch MNIST \u6a21\u578b\u4e00\u5b9a\u4f1a\u5f88\u6709\u8da3\u3002 \u6211\u60f3\u4f60\u73b0\u5728\u5bf9 Pinferencia \u5f88\u719f\u6089\u4e86\u3002 Pinferencia \u53ef\u4ee5\u4ee5\u975e\u5e38\u76f4\u63a5\u7684\u65b9\u5f0f\u4e3a\u4efb\u4f55\u53ef\u8c03\u7528\u5bf9\u8c61\u63d0\u4f9b\u670d\u52a1\u3002\u975e\u5e38\u7b80\u5355\u76f4\u63a5\u3002 \u5e76\u4e14\u8fd9\u5f88\u5bb9\u6613\u4e0e\u60a8\u73b0\u6709\u7684\u4ee3\u7801\u96c6\u6210\u3002 \u8fd9\u5c31\u662f Pinferencia \u7684\u8bbe\u8ba1\u76ee\u7684\u3002 \u6700\u5c11\u7684\u4ee3\u7801\u4fee\u6539 \u3002 \u73b0\u5728\u60a8\u53ef\u4ee5\u4ece \u4efb\u4f55\u6846\u67b6 \u63d0\u4f9b\u6a21\u578b\uff0c\u751a\u81f3\u53ef\u4ee5 \u5c06\u5b83\u4eec\u6df7\u5408\u5728\u4e00\u8d77 \u3002\u60a8\u53ef\u4ee5\u5728\u4e00\u4e2aAPI\u91cc \u540c\u65f6 \u4f7f\u7528\u6765\u81ea \u4e0d\u540c\u6846\u67b6 \u7684 \u4e0d\u540c\u6a21\u578b \uff01 \u73a9\u7684\u5f00\u5fc3\uff01 \u5982\u679c\u4f60\u559c\u6b22 Pinferencia \uff0c\u522b\u5fd8\u4e86\u53bb Github \u5e76\u7ed9\u4e00\u4e2a\u661f\u3002\u8c22\u8c22\u4f60\u3002","title":"\u4e0b\u4e00\u6b65"},{"location":"zh/get-started/pytorch-mnist/","text":"\u4e0a\u7ebf PyTorch MNIST \u6a21\u578b \u00b6 \u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u63d0\u4f9b PyTorch MNIST \u6a21\u578b\u3002 \u5b83\u63a5\u6536 Base64 \u7f16\u7801\u7684\u56fe\u50cf\u4f5c\u4e3a\u8bf7\u6c42\u6570\u636e\uff0c\u5e76\u5728\u54cd\u5e94\u4e2d\u8fd4\u56de\u9884\u6d4b\u3002 \u51c6\u5907\u5de5\u4f5c \u00b6 \u8bbf\u95ee PyTorch \u793a\u4f8b - MNIST \uff0c\u4e0b\u8f7d\u6587\u4ef6\u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6765\u5b89\u88c5\u548c\u8bad\u7ec3\u6a21\u578b\uff1a pip install -r requirements.txt python main.py --save-model \u8bad\u7ec3\u5b8c\u6210\u540e\uff0c\u60a8\u5c06\u62e5\u6709\u5982\u4e0b\u6587\u4ef6\u5939\u7ed3\u6784\u3002\u521b\u5efa\u4e86\u4e00\u4e2a mnist_cnn.pt \u6587\u4ef6 . \u251c\u2500\u2500 README.md \u251c\u2500\u2500 main.py \u251c\u2500\u2500 mnist_cnn.pt \u2514\u2500\u2500 requirements.txt \u90e8\u7f72\u65b9\u6cd5 \u00b6 \u6709\u4e24\u79cd\u65b9\u6cd5\u53ef\u4ee5\u90e8\u7f72\u6a21\u578b\u3002 \u76f4\u63a5\u6ce8\u518c\u4e00\u4e2a\u51fd\u6570\u3002 \u4ec5\u4f7f\u7528\u9644\u52a0\u5904\u7406\u7a0b\u5e8f Handler \u6ce8\u518c\u6a21\u578b\u8def\u5f84\u3002 \u6211\u4eec\u5c06\u5728\u672c\u6559\u7a0b\u4e2d\u9010\u6b65\u4ecb\u7ecd\u8fd9\u4e24\u79cd\u65b9\u6cd5\u3002 \u76f4\u63a5\u6ce8\u518c\u4e00\u4e2a\u51fd\u6570 \u00b6 \u521b\u5efa\u5e94\u7528\u7a0b\u5e8f \u00b6 \u8ba9\u6211\u4eec\u5728\u540c\u4e00\u4e2a\u6587\u4ef6\u5939\u4e2d\u521b\u5efa\u4e00\u4e2a\u6587\u4ef6 func_app.py \u3002 func_app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 import base64 from io import BytesIO import torch from main import Net # (1) from PIL import Image from torchvision import transforms from pinferencia import Server use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) # (2) model = Net () . to ( device ) # (3) model . load_state_dict ( torch . load ( \"mnist_cnn.pt\" )) model . eval () def preprocessing ( img_str ): image = Image . open ( BytesIO ( base64 . b64decode ( img_str ))) tensor = transform ( image ) return torch . stack ([ tensor ]) . to ( device ) def predict ( data ): return model ( preprocessing ( data )) . argmax ( 1 ) . tolist ()[ 0 ] service = Server () # (4) service . register ( model_name = \"mnist\" , model = predict ) \u786e\u4fdd\u60a8\u53ef\u4ee5\u5bfc\u5165\u7f51\u7edc\u6a21\u578b\u3002 \u9884\u5904\u7406\u8f6c\u6362\u4ee3\u7801\u3002 \u793a\u4f8b\u811a\u672c\u53ea\u4fdd\u5b58 state_dict \u3002\u8fd9\u91cc\u6211\u4eec\u9700\u8981\u521d\u59cb\u5316\u6a21\u578b\u5e76\u52a0\u8f7d state_dict \u3002 \u51c6\u5907\u597d\uff0c3\u30012\u30011\u3002 GO\uff01 \u542f\u52a8\u670d\u52a1 \u00b6 $ uvicorn func_app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. \u6d4b\u8bd5\u670d\u52a1 \u00b6 \u6d4b\u8bd5\u6570\u636e\u90a3\u91cc\u6765? \u56e0\u4e3a\u6211\u4eec\u7684\u8f93\u5165\u662f base64 \u7f16\u7801\u7684 MNIST \u56fe\u50cf\uff0c\u6211\u4eec\u4ece\u54ea\u91cc\u53ef\u4ee5\u83b7\u5f97\u8fd9\u4e9b\u6570\u636e\uff1f \u60a8\u53ef\u4ee5\u4f7f\u7528 PyTorch \u7684\u6570\u636e\u96c6\u3002\u5728\u540c\u4e00\u6587\u4ef6\u5939\u4e2d\u521b\u5efa\u4e00\u4e2a\u6587\u4ef6\u540d\u4e3a get-base64-img.oy \u3002 get-base64-img.py import base64 import random from io import BytesIO from PIL import Image from torchvision import datasets dataset = datasets . MNIST ( # (1) \"./data\" , train = True , download = True , transform = None , ) index = random . randint ( 0 , len ( dataset . data )) # (2) img = dataset . data [ index ] img = Image . fromarray ( img . numpy (), mode = \"L\" ) buffered = BytesIO () img . save ( buffered , format = \"JPEG\" ) base64_img_str = base64 . b64encode ( buffered . getvalue ()) . decode () print ( \"Base64 String:\" , base64_img_str ) # (3) print ( \"target:\" , dataset . targets [ index ] . tolist ()) \u8fd9\u662f\u8bad\u7ec3\u671f\u95f4\u4f7f\u7528\u7684 MNIST \u6570\u636e\u96c6\u3002 \u8ba9\u6211\u4eec\u4f7f\u7528\u968f\u673a\u56fe\u50cf\u3002 \u5b57\u7b26\u4e32\u548c\u76ee\u6807\u88ab\u6253\u5370\u5230\u6807\u51c6\u8f93\u51fa\u3002 \u8fd0\u884c\u811a\u672c\u5e76\u590d\u5236\u5b57\u7b26\u4e32\u3002 python get-base64-img.py \u8f93\u51fa: Base64 String: /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+uhfwXqy2Ph25VYnPiB3SzhUkPlXCfNkAAEsCCCeOeKx9RsLjStUu9Ou1C3NpM8Eqg5AdSVIz35FVqK9xl0HXhb/C20sdMubjTLMQXs11AhkRXmmDsCwzgAYPpz+XI/GrSLrTfiVqNzPapbw3xE8AWQNvUAKXOOmWVjg+teeUV2fgXxd4hsPE2hWEGuX8Vh9uhja3Fw3lbGcBhtzjGCad8XI7iL4p68twHDGcMm45+QqCuPbBFcVRRU97fXepXb3d9dT3VzJjfNPIXdsAAZY8nAAH4VBX/9k = target: 4 \u8ba9\u6211\u4eec\u521b\u5efa\u4e00\u4e2a\u6587\u4ef6 test.py test.py 1 2 3 4 5 6 7 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/mnist/predict\" , json = { \"data\" : \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+uhfwXqy2Ph25VYnPiB3SzhUkPlXCfNkAAEsCCCeOeKx9RsLjStUu9Ou1C3NpM8Eqg5AdSVIz35FVqK9xl0HXhb/C20sdMubjTLMQXs11AhkRXmmDsCwzgAYPpz+XI/GrSLrTfiVqNzPapbw3xE8AWQNvUAKXOOmWVjg+teeUV2fgXxd4hsPE2hWEGuX8Vh9uhja3Fw3lbGcBhtzjGCad8XI7iL4p68twHDGcMm45+QqCuPbBFcVRRU97fXepXb3d9dT3VzJjfNPIXdsAAZY8nAAH4VBX/9k=\" }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) \u8fd0\u884c\u6d4b\u8bd5: $ python test.py Prediction: 4 \u60a8\u53ef\u4ee5\u5c1d\u8bd5\u4f7f\u7528\u66f4\u591a\u56fe\u50cf\u6765\u6d4b\u8bd5\uff0c\u751a\u81f3\u53ef\u4ee5\u4f7f\u7528\u4ea4\u4e92\u5f0f API \u6587\u6863\u9875\u9762 http://127.0.0.1 \u4f7f\u7528 Handler \u6ce8\u518c\u6a21\u578b\u8def\u5f84 \u00b6 Handler \u5982\u679c\u60a8\u66f4\u559c\u6b22\u4f7f\u7528\u6587\u4ef6\u63d0\u4f9b\u6a21\u578b\u7684\u7ecf\u5178\u65b9\u5f0f\uff0c\u5219\u4f7f\u7528\u201cHandlers\u201d\u662f\u60a8\u7684\u9009\u62e9\u3002 \u5904\u7406\u7a0b\u5e8f\u7684\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u8bbf\u95ee Handlers \u521b\u5efa\u5e94\u7528\u7a0b\u5e8f \u00b6 \u8ba9\u6211\u4eec\u5728\u540c\u4e00\u4e2a\u6587\u4ef6\u5939\u4e2d\u521b\u5efa\u4e00\u4e2a\u6587\u4ef6 func_app.py \u3002 \u4e0b\u9762\u7684\u4ee3\u7801\u88ab\u91cd\u6784\u4e3a MNISTHandler \u3002\u770b\u8d77\u6765\u66f4\u5e72\u51c0\uff01 path_app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 import base64 import pathlib from io import BytesIO import torch from main import Net from PIL import Image from torchvision import transforms from pinferencia import Server from pinferencia.handlers import BaseHandler class MNISTHandler ( BaseHandler ): transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) def load_model ( self ): # (1) model = Net () . to ( self . device ) model . load_state_dict ( torch . load ( self . model_path )) model . eval () return model def predict ( self , data ): # (2) image = Image . open ( BytesIO ( base64 . b64decode ( data ))) tensor = self . transform ( image ) input_data = torch . stack ([ tensor ]) . to ( self . device ) return self . model ( input_data ) . argmax ( 1 ) . tolist ()[ 0 ] service = Server ( model_dir = pathlib . Path ( __file__ ) . parent . resolve ()) # (3) service . register ( model_name = \"mnist\" , model = \"mnist_cnn.pt\" , handler = MNISTHandler , load_now = True , # (4) ) \u6211\u4eec\u5c06\u52a0\u8f7d\u6a21\u578b\u7684\u4ee3\u7801\u79fb\u5230 load_model \u51fd\u6570\u4e2d\u3002\u6a21\u578b\u8def\u5f84\u53ef\u4ee5\u901a\u8fc7 self.model_path \u8bbf\u95ee\u3002 \u6211\u4eec\u5c06\u9884\u6d4b\u4ee3\u7801\u79fb\u5230 predict \u51fd\u6570\u4e2d\u3002\u8be5\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7 self.model \u8bbf\u95ee\u3002 model_dir \u662f Pinferencia \u67e5\u627e\u6a21\u578b\u6587\u4ef6\u7684\u5730\u65b9\u3002\u5c06 model_dir \u8bbe\u7f6e\u4e3a\u5305\u542b mnist_cnn.pt \u548c\u6b64\u811a\u672c\u7684\u6587\u4ef6\u5939\u3002 load_now \u786e\u5b9a\u6a21\u578b\u662f\u5426\u4f1a\u5728\u6ce8\u518c\u671f\u95f4\u7acb\u5373\u52a0\u8f7d\u3002\u9ed8\u8ba4\u503c\u4e3a\u201c\u771f\u201d\u3002\u5982\u679c\u8bbe\u7f6e\u4e3a False \uff0c\u5219\u9700\u8981\u8c03\u7528 load API \u52a0\u8f7d\u6a21\u578b\u624d\u80fd\u8fdb\u884c\u9884\u6d4b\u3002 \u542f\u52a8\u670d\u52a1 \u00b6 $ uvicorn path_app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. \u6d4b\u8bd5\u670d\u52a1 \u00b6 \u8fd0\u884c\u6d4b\u8bd5: $ python test.py Prediction: 4 \u4e0d\u51fa\u610f\u5916\uff0c\u7ed3\u679c\u4e00\u6837\u3002 \u6700\u540e \u00b6 \u4f7f\u7528 Pinferencia \uff0c\u60a8\u53ef\u4ee5\u4e3a\u4efb\u4f55\u6a21\u578b\u63d0\u4f9b\u670d\u52a1\u3002 \u60a8\u53ef\u4ee5\u81ea\u5df1\u52a0\u8f7d\u6a21\u578b\uff0c\u5c31\u50cf\u60a8\u5728\u8fdb\u884c\u79bb\u7ebf\u9884\u6d4b\u65f6\u6240\u505a\u7684\u90a3\u6837\u3002 \u8fd9\u90e8\u5206\u4ee3\u7801\u4f60\u65e9\u5c31\u5df2\u7ecf\u5199\u597d\u4e86\u3002 \u7136\u540e\uff0c\u53ea\u9700\u4f7f\u7528 Pinferencia \u6ce8\u518c\u6a21\u578b\uff0c\u60a8\u7684\u6a21\u578b\u5c31\u4f1a\u751f\u6548\u3002 \u6216\u8005\uff0c\u60a8\u53ef\u4ee5\u9009\u62e9\u5c06\u4ee3\u7801\u91cd\u6784\u4e3a Handler Class \u3002\u65e7\u7684\u7ecf\u5178\u65b9\u5f0f\u4e5f\u9002\u7528\u4e8e Pinferencia \u3002 \u8fd9\u4e24\u4e2a\u4e16\u754c\u90fd\u9002\u7528\u4e8e\u60a8\u7684\u6a21\u578b\uff0c \u7ecf\u5178\u200b\u200b\u97f3\u4e50**\u548c**\u6447\u6eda\u4e50 \u3002 \u662f\u4e0d\u662f\u5f88\u68d2\uff01 \u73b0\u5728\u60a8\u5df2\u7ecf\u638c\u63e1\u4e86\u5982\u4f55\u4f7f\u7528 Pinferencia \u6765\uff1a \u6ce8\u518c\u4efb\u4f55\u6a21\u578b\u3001\u4efb\u4f55\u51fd\u6570\u5e76\u628a\u5b83\u4eec\u4e0a\u7ebf\u3002 \u4f7f\u7528\u60a8\u7684\u81ea\u5b9a\u4e49\u5904\u7406\u7a0b\u5e8f\u4e3a\u60a8\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u63d0\u4f9b\u670d\u52a1\u3002 \u5982\u679c\u4f60\u8fd8\u6709\u65f6\u95f4\uff0c\u8ba9\u6211\u4eec\u5c1d\u8bd5\u4e00\u4e9b\u6709\u8da3\u7684\u4e8b\u60c5\u3002 \u989d\u5916\uff1a MNIST \u56fe\u50cf\u7684\u52a0\u6cd5 \u00b6 \u8ba9\u6211\u4eec\u521b\u5efa\u4e00\u4e2a\u201csum_mnist.py\u201d\u3002\u5b83\u63a5\u53d7\u4e00\u7ec4\u56fe\u50cf\uff0c\u9884\u6d4b\u5b83\u4eec\u7684\u6570\u5b57\u5e76\u628a\u5b83\u4eec\u52a0\u8d77\u6765\u3002 sum_mnist.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 import base64 import pathlib from io import BytesIO import torch from main import Net from PIL import Image from torchvision import transforms from pinferencia import Server from pinferencia.handlers import BaseHandler class MNISTHandler ( BaseHandler ): transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) def load_model ( self ): model = Net () . to ( self . device ) model . load_state_dict ( torch . load ( self . model_path )) model . eval () return model def predict ( self , data ): tensors = [] # (1) for img in data : image = Image . open ( BytesIO ( base64 . b64decode ( img ))) tensors . append ( self . transform ( image )) input_data = torch . stack ( tensors ) . to ( self . device ) return sum ( self . model ( input_data ) . argmax ( 1 ) . tolist ()) service = Server ( model_dir = pathlib . Path ( __file__ ) . parent . resolve ()) service . register ( model_name = \"mnist\" , model = \"mnist_cnn.pt\" , handler = MNISTHandler , load_now = True , ) \u8fd9\u91cc\u6211\u4eec\u5bf9\u6bcf\u5f20\u56fe\u50cf\u8fdb\u884c\u9884\u5904\u7406\uff0c\u9884\u6d4b\u5176\u4f4d\u6570\u5e76\u8fdb\u884c\u603b\u7ed3\u3002 \u5e0c\u671b\u4f60\u5728 Pinferencia \u4e16\u754c\u73a9\u5f97\u5f00\u5fc3\uff01","title":"\u542f\u52a8PyTorch MNIST\u6a21\u578b"},{"location":"zh/get-started/pytorch-mnist/#pytorch-mnist","text":"\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u63d0\u4f9b PyTorch MNIST \u6a21\u578b\u3002 \u5b83\u63a5\u6536 Base64 \u7f16\u7801\u7684\u56fe\u50cf\u4f5c\u4e3a\u8bf7\u6c42\u6570\u636e\uff0c\u5e76\u5728\u54cd\u5e94\u4e2d\u8fd4\u56de\u9884\u6d4b\u3002","title":"\u4e0a\u7ebf PyTorch MNIST \u6a21\u578b"},{"location":"zh/get-started/pytorch-mnist/#_1","text":"\u8bbf\u95ee PyTorch \u793a\u4f8b - MNIST \uff0c\u4e0b\u8f7d\u6587\u4ef6\u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6765\u5b89\u88c5\u548c\u8bad\u7ec3\u6a21\u578b\uff1a pip install -r requirements.txt python main.py --save-model \u8bad\u7ec3\u5b8c\u6210\u540e\uff0c\u60a8\u5c06\u62e5\u6709\u5982\u4e0b\u6587\u4ef6\u5939\u7ed3\u6784\u3002\u521b\u5efa\u4e86\u4e00\u4e2a mnist_cnn.pt \u6587\u4ef6 . \u251c\u2500\u2500 README.md \u251c\u2500\u2500 main.py \u251c\u2500\u2500 mnist_cnn.pt \u2514\u2500\u2500 requirements.txt","title":"\u51c6\u5907\u5de5\u4f5c"},{"location":"zh/get-started/pytorch-mnist/#_2","text":"\u6709\u4e24\u79cd\u65b9\u6cd5\u53ef\u4ee5\u90e8\u7f72\u6a21\u578b\u3002 \u76f4\u63a5\u6ce8\u518c\u4e00\u4e2a\u51fd\u6570\u3002 \u4ec5\u4f7f\u7528\u9644\u52a0\u5904\u7406\u7a0b\u5e8f Handler \u6ce8\u518c\u6a21\u578b\u8def\u5f84\u3002 \u6211\u4eec\u5c06\u5728\u672c\u6559\u7a0b\u4e2d\u9010\u6b65\u4ecb\u7ecd\u8fd9\u4e24\u79cd\u65b9\u6cd5\u3002","title":"\u90e8\u7f72\u65b9\u6cd5"},{"location":"zh/get-started/pytorch-mnist/#_3","text":"","title":"\u76f4\u63a5\u6ce8\u518c\u4e00\u4e2a\u51fd\u6570"},{"location":"zh/get-started/pytorch-mnist/#_4","text":"\u8ba9\u6211\u4eec\u5728\u540c\u4e00\u4e2a\u6587\u4ef6\u5939\u4e2d\u521b\u5efa\u4e00\u4e2a\u6587\u4ef6 func_app.py \u3002 func_app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 import base64 from io import BytesIO import torch from main import Net # (1) from PIL import Image from torchvision import transforms from pinferencia import Server use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) # (2) model = Net () . to ( device ) # (3) model . load_state_dict ( torch . load ( \"mnist_cnn.pt\" )) model . eval () def preprocessing ( img_str ): image = Image . open ( BytesIO ( base64 . b64decode ( img_str ))) tensor = transform ( image ) return torch . stack ([ tensor ]) . to ( device ) def predict ( data ): return model ( preprocessing ( data )) . argmax ( 1 ) . tolist ()[ 0 ] service = Server () # (4) service . register ( model_name = \"mnist\" , model = predict ) \u786e\u4fdd\u60a8\u53ef\u4ee5\u5bfc\u5165\u7f51\u7edc\u6a21\u578b\u3002 \u9884\u5904\u7406\u8f6c\u6362\u4ee3\u7801\u3002 \u793a\u4f8b\u811a\u672c\u53ea\u4fdd\u5b58 state_dict \u3002\u8fd9\u91cc\u6211\u4eec\u9700\u8981\u521d\u59cb\u5316\u6a21\u578b\u5e76\u52a0\u8f7d state_dict \u3002 \u51c6\u5907\u597d\uff0c3\u30012\u30011\u3002 GO\uff01","title":"\u521b\u5efa\u5e94\u7528\u7a0b\u5e8f"},{"location":"zh/get-started/pytorch-mnist/#_5","text":"$ uvicorn func_app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete.","title":"\u542f\u52a8\u670d\u52a1"},{"location":"zh/get-started/pytorch-mnist/#_6","text":"\u6d4b\u8bd5\u6570\u636e\u90a3\u91cc\u6765? \u56e0\u4e3a\u6211\u4eec\u7684\u8f93\u5165\u662f base64 \u7f16\u7801\u7684 MNIST \u56fe\u50cf\uff0c\u6211\u4eec\u4ece\u54ea\u91cc\u53ef\u4ee5\u83b7\u5f97\u8fd9\u4e9b\u6570\u636e\uff1f \u60a8\u53ef\u4ee5\u4f7f\u7528 PyTorch \u7684\u6570\u636e\u96c6\u3002\u5728\u540c\u4e00\u6587\u4ef6\u5939\u4e2d\u521b\u5efa\u4e00\u4e2a\u6587\u4ef6\u540d\u4e3a get-base64-img.oy \u3002 get-base64-img.py import base64 import random from io import BytesIO from PIL import Image from torchvision import datasets dataset = datasets . MNIST ( # (1) \"./data\" , train = True , download = True , transform = None , ) index = random . randint ( 0 , len ( dataset . data )) # (2) img = dataset . data [ index ] img = Image . fromarray ( img . numpy (), mode = \"L\" ) buffered = BytesIO () img . save ( buffered , format = \"JPEG\" ) base64_img_str = base64 . b64encode ( buffered . getvalue ()) . decode () print ( \"Base64 String:\" , base64_img_str ) # (3) print ( \"target:\" , dataset . targets [ index ] . tolist ()) \u8fd9\u662f\u8bad\u7ec3\u671f\u95f4\u4f7f\u7528\u7684 MNIST \u6570\u636e\u96c6\u3002 \u8ba9\u6211\u4eec\u4f7f\u7528\u968f\u673a\u56fe\u50cf\u3002 \u5b57\u7b26\u4e32\u548c\u76ee\u6807\u88ab\u6253\u5370\u5230\u6807\u51c6\u8f93\u51fa\u3002 \u8fd0\u884c\u811a\u672c\u5e76\u590d\u5236\u5b57\u7b26\u4e32\u3002 python get-base64-img.py \u8f93\u51fa: Base64 String: /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+uhfwXqy2Ph25VYnPiB3SzhUkPlXCfNkAAEsCCCeOeKx9RsLjStUu9Ou1C3NpM8Eqg5AdSVIz35FVqK9xl0HXhb/C20sdMubjTLMQXs11AhkRXmmDsCwzgAYPpz+XI/GrSLrTfiVqNzPapbw3xE8AWQNvUAKXOOmWVjg+teeUV2fgXxd4hsPE2hWEGuX8Vh9uhja3Fw3lbGcBhtzjGCad8XI7iL4p68twHDGcMm45+QqCuPbBFcVRRU97fXepXb3d9dT3VzJjfNPIXdsAAZY8nAAH4VBX/9k = target: 4 \u8ba9\u6211\u4eec\u521b\u5efa\u4e00\u4e2a\u6587\u4ef6 test.py test.py 1 2 3 4 5 6 7 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/mnist/predict\" , json = { \"data\" : \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+uhfwXqy2Ph25VYnPiB3SzhUkPlXCfNkAAEsCCCeOeKx9RsLjStUu9Ou1C3NpM8Eqg5AdSVIz35FVqK9xl0HXhb/C20sdMubjTLMQXs11AhkRXmmDsCwzgAYPpz+XI/GrSLrTfiVqNzPapbw3xE8AWQNvUAKXOOmWVjg+teeUV2fgXxd4hsPE2hWEGuX8Vh9uhja3Fw3lbGcBhtzjGCad8XI7iL4p68twHDGcMm45+QqCuPbBFcVRRU97fXepXb3d9dT3VzJjfNPIXdsAAZY8nAAH4VBX/9k=\" }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) \u8fd0\u884c\u6d4b\u8bd5: $ python test.py Prediction: 4 \u60a8\u53ef\u4ee5\u5c1d\u8bd5\u4f7f\u7528\u66f4\u591a\u56fe\u50cf\u6765\u6d4b\u8bd5\uff0c\u751a\u81f3\u53ef\u4ee5\u4f7f\u7528\u4ea4\u4e92\u5f0f API \u6587\u6863\u9875\u9762 http://127.0.0.1","title":"\u6d4b\u8bd5\u670d\u52a1"},{"location":"zh/get-started/pytorch-mnist/#handler","text":"Handler \u5982\u679c\u60a8\u66f4\u559c\u6b22\u4f7f\u7528\u6587\u4ef6\u63d0\u4f9b\u6a21\u578b\u7684\u7ecf\u5178\u65b9\u5f0f\uff0c\u5219\u4f7f\u7528\u201cHandlers\u201d\u662f\u60a8\u7684\u9009\u62e9\u3002 \u5904\u7406\u7a0b\u5e8f\u7684\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u8bbf\u95ee Handlers","title":"\u4f7f\u7528 Handler \u6ce8\u518c\u6a21\u578b\u8def\u5f84"},{"location":"zh/get-started/pytorch-mnist/#_7","text":"\u8ba9\u6211\u4eec\u5728\u540c\u4e00\u4e2a\u6587\u4ef6\u5939\u4e2d\u521b\u5efa\u4e00\u4e2a\u6587\u4ef6 func_app.py \u3002 \u4e0b\u9762\u7684\u4ee3\u7801\u88ab\u91cd\u6784\u4e3a MNISTHandler \u3002\u770b\u8d77\u6765\u66f4\u5e72\u51c0\uff01 path_app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 import base64 import pathlib from io import BytesIO import torch from main import Net from PIL import Image from torchvision import transforms from pinferencia import Server from pinferencia.handlers import BaseHandler class MNISTHandler ( BaseHandler ): transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) def load_model ( self ): # (1) model = Net () . to ( self . device ) model . load_state_dict ( torch . load ( self . model_path )) model . eval () return model def predict ( self , data ): # (2) image = Image . open ( BytesIO ( base64 . b64decode ( data ))) tensor = self . transform ( image ) input_data = torch . stack ([ tensor ]) . to ( self . device ) return self . model ( input_data ) . argmax ( 1 ) . tolist ()[ 0 ] service = Server ( model_dir = pathlib . Path ( __file__ ) . parent . resolve ()) # (3) service . register ( model_name = \"mnist\" , model = \"mnist_cnn.pt\" , handler = MNISTHandler , load_now = True , # (4) ) \u6211\u4eec\u5c06\u52a0\u8f7d\u6a21\u578b\u7684\u4ee3\u7801\u79fb\u5230 load_model \u51fd\u6570\u4e2d\u3002\u6a21\u578b\u8def\u5f84\u53ef\u4ee5\u901a\u8fc7 self.model_path \u8bbf\u95ee\u3002 \u6211\u4eec\u5c06\u9884\u6d4b\u4ee3\u7801\u79fb\u5230 predict \u51fd\u6570\u4e2d\u3002\u8be5\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7 self.model \u8bbf\u95ee\u3002 model_dir \u662f Pinferencia \u67e5\u627e\u6a21\u578b\u6587\u4ef6\u7684\u5730\u65b9\u3002\u5c06 model_dir \u8bbe\u7f6e\u4e3a\u5305\u542b mnist_cnn.pt \u548c\u6b64\u811a\u672c\u7684\u6587\u4ef6\u5939\u3002 load_now \u786e\u5b9a\u6a21\u578b\u662f\u5426\u4f1a\u5728\u6ce8\u518c\u671f\u95f4\u7acb\u5373\u52a0\u8f7d\u3002\u9ed8\u8ba4\u503c\u4e3a\u201c\u771f\u201d\u3002\u5982\u679c\u8bbe\u7f6e\u4e3a False \uff0c\u5219\u9700\u8981\u8c03\u7528 load API \u52a0\u8f7d\u6a21\u578b\u624d\u80fd\u8fdb\u884c\u9884\u6d4b\u3002","title":"\u521b\u5efa\u5e94\u7528\u7a0b\u5e8f"},{"location":"zh/get-started/pytorch-mnist/#_8","text":"$ uvicorn path_app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete.","title":"\u542f\u52a8\u670d\u52a1"},{"location":"zh/get-started/pytorch-mnist/#_9","text":"\u8fd0\u884c\u6d4b\u8bd5: $ python test.py Prediction: 4 \u4e0d\u51fa\u610f\u5916\uff0c\u7ed3\u679c\u4e00\u6837\u3002","title":"\u6d4b\u8bd5\u670d\u52a1"},{"location":"zh/get-started/pytorch-mnist/#_10","text":"\u4f7f\u7528 Pinferencia \uff0c\u60a8\u53ef\u4ee5\u4e3a\u4efb\u4f55\u6a21\u578b\u63d0\u4f9b\u670d\u52a1\u3002 \u60a8\u53ef\u4ee5\u81ea\u5df1\u52a0\u8f7d\u6a21\u578b\uff0c\u5c31\u50cf\u60a8\u5728\u8fdb\u884c\u79bb\u7ebf\u9884\u6d4b\u65f6\u6240\u505a\u7684\u90a3\u6837\u3002 \u8fd9\u90e8\u5206\u4ee3\u7801\u4f60\u65e9\u5c31\u5df2\u7ecf\u5199\u597d\u4e86\u3002 \u7136\u540e\uff0c\u53ea\u9700\u4f7f\u7528 Pinferencia \u6ce8\u518c\u6a21\u578b\uff0c\u60a8\u7684\u6a21\u578b\u5c31\u4f1a\u751f\u6548\u3002 \u6216\u8005\uff0c\u60a8\u53ef\u4ee5\u9009\u62e9\u5c06\u4ee3\u7801\u91cd\u6784\u4e3a Handler Class \u3002\u65e7\u7684\u7ecf\u5178\u65b9\u5f0f\u4e5f\u9002\u7528\u4e8e Pinferencia \u3002 \u8fd9\u4e24\u4e2a\u4e16\u754c\u90fd\u9002\u7528\u4e8e\u60a8\u7684\u6a21\u578b\uff0c \u7ecf\u5178\u200b\u200b\u97f3\u4e50**\u548c**\u6447\u6eda\u4e50 \u3002 \u662f\u4e0d\u662f\u5f88\u68d2\uff01 \u73b0\u5728\u60a8\u5df2\u7ecf\u638c\u63e1\u4e86\u5982\u4f55\u4f7f\u7528 Pinferencia \u6765\uff1a \u6ce8\u518c\u4efb\u4f55\u6a21\u578b\u3001\u4efb\u4f55\u51fd\u6570\u5e76\u628a\u5b83\u4eec\u4e0a\u7ebf\u3002 \u4f7f\u7528\u60a8\u7684\u81ea\u5b9a\u4e49\u5904\u7406\u7a0b\u5e8f\u4e3a\u60a8\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u63d0\u4f9b\u670d\u52a1\u3002 \u5982\u679c\u4f60\u8fd8\u6709\u65f6\u95f4\uff0c\u8ba9\u6211\u4eec\u5c1d\u8bd5\u4e00\u4e9b\u6709\u8da3\u7684\u4e8b\u60c5\u3002","title":"\u6700\u540e"},{"location":"zh/get-started/pytorch-mnist/#mnist","text":"\u8ba9\u6211\u4eec\u521b\u5efa\u4e00\u4e2a\u201csum_mnist.py\u201d\u3002\u5b83\u63a5\u53d7\u4e00\u7ec4\u56fe\u50cf\uff0c\u9884\u6d4b\u5b83\u4eec\u7684\u6570\u5b57\u5e76\u628a\u5b83\u4eec\u52a0\u8d77\u6765\u3002 sum_mnist.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 import base64 import pathlib from io import BytesIO import torch from main import Net from PIL import Image from torchvision import transforms from pinferencia import Server from pinferencia.handlers import BaseHandler class MNISTHandler ( BaseHandler ): transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) def load_model ( self ): model = Net () . to ( self . device ) model . load_state_dict ( torch . load ( self . model_path )) model . eval () return model def predict ( self , data ): tensors = [] # (1) for img in data : image = Image . open ( BytesIO ( base64 . b64decode ( img ))) tensors . append ( self . transform ( image )) input_data = torch . stack ( tensors ) . to ( self . device ) return sum ( self . model ( input_data ) . argmax ( 1 ) . tolist ()) service = Server ( model_dir = pathlib . Path ( __file__ ) . parent . resolve ()) service . register ( model_name = \"mnist\" , model = \"mnist_cnn.pt\" , handler = MNISTHandler , load_now = True , ) \u8fd9\u91cc\u6211\u4eec\u5bf9\u6bcf\u5f20\u56fe\u50cf\u8fdb\u884c\u9884\u5904\u7406\uff0c\u9884\u6d4b\u5176\u4f4d\u6570\u5e76\u8fdb\u884c\u603b\u7ed3\u3002 \u5e0c\u671b\u4f60\u5728 Pinferencia \u4e16\u754c\u73a9\u5f97\u5f00\u5fc3\uff01","title":"\u989d\u5916\uff1a MNIST \u56fe\u50cf\u7684\u52a0\u6cd5"},{"location":"zh/get-started/serve-a-function/","text":"\u542f\u52a8\u4e00\u4e2a\u51fd\u6570 \u00b6 \u597d\u5427\uff0c\u670d\u52a1\u4e00\u4e2a\u51fd\u6570\uff1f\u6709\u7528\u5417\uff1f \u5f53\u7136\u662f\u7684\u3002 \u5982\u679c\u60a8\u6709 \u4e00\u4e2a\u5b8c\u6574\u7684\u63a8\u7406\u5de5\u4f5c\u6d41\u7a0b \uff0c\u5b83\u5305\u542b\u8bb8\u591a\u6b65\u9aa4\u3002\u5927\u591a\u6570\u65f6\u5019\uff0c\u60a8\u5c06\u5b9e\u73b0\u4e00\u4e2a\u51fd\u6570\u6765\u5b8c\u6210\u8fd9\u9879\u5de5\u4f5c\u3002\u73b0\u5728\u60a8\u53ef\u4ee5\u7acb\u5373\u6ce8\u518c\u8be5\u51fd\u6570\u3002 \u5982\u679c\u4f60\u60f3\u5206\u4eab\u4e00\u4e9b\u9884\u5904\u7406\u6216\u540e\u5904\u7406\u529f\u80fd\uff0c\u73b0\u5728\u4f60\u6709\u4f60\u7684\u7f57\u5bbe\u4e86\uff0c \u8759\u8760\u4fa0 \uff01 \u6216\u8005\u4e00\u4e2a\u51fd\u6570\u5bf9\u4f60\u7684\u5de5\u4f5c\u6765\u8bf4\u5c31\u8db3\u591f\u4e86\u3002 \u4efb\u52a1 \u00b6 \u6211\u4eec\u5f97\u5230\u4e86\u4e00\u4efd\u5c71\u8109\u9ad8\u5ea6\u7684\u5217\u8868\u3002\u6211\u4eec\u9700\u8981\u627e\u51fa\u6700\u9ad8\u3001\u6700\u4f4e\u4ee5\u53ca\u6700\u9ad8\u548c\u6700\u4f4e\u4e4b\u95f4\u7684\u5dee\u5f02\u3002 \u8fd9\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u95ee\u9898\uff0c\u8ba9\u6211\u4eec\u5728\u4e00\u4e2a\u51fd\u6570\u4e2d\u89e3\u51b3\u5b83\uff0c\u8ba9\u60a8\u66f4\u719f\u6089\u8fd9\u4e2a\u6982\u5ff5\u3002 graph LR heights(\u5c71\u7684\u9ad8\u5ea6) --> max(\u627e\u51fa\u6700\u9ad8\u7684&nbsp&nbsp) heights --> min(\u627e\u51fa\u6700\u4f4e\u7684&nbsp&nbsp) min --> diff(\u8ba1\u7b97\u5dee\u5f02) max --> diff diff --> output(\u8f93\u51fa) subgraph Workflow max min diff end \u521b\u5efa\u670d\u52a1\u5e76\u6ce8\u518c\u6a21\u578b \u00b6 \u5c06\u4ee5\u4e0b\u4ee3\u7801\u4fdd\u5b58\u5728 app.py \u4e2d\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from typing import List from pinferencia import Server def calc ( data : List [ int ]) -> int : highest = max ( data ) lowest = min ( data ) return highest - lowest service = Server () service . register ( model_name = \"mountain\" , model = calc ) \u542f\u52a8\u670d\u52a1\u5668 \u00b6 $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) \u6d4b\u8bd5 API \u00b6 \u4f7f\u7528\u4e0b\u9762\u7684\u4ee3\u7801\u521b\u5efa\u4e00\u4e2a test.py \u3002 \u63d0\u793a You need to have requests installed. pip install requests test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/mountain/predict\" , json = { \"data\" : [ 1000 , 2000 , 3000 ]}, ) difference = response . json ()[ \"data\" ] print ( f \"Difference between the highest and lowest is { difference } m.\" ) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py Difference between the highest and lowest is 2000m. \u6b64\u5916 \u00b6 \u73b0\u5728\u4f60\u5df2\u7ecf\u5b66\u4f1a\u4e86\u5982\u4f55\u5c06\u5b9a\u4e49\u4e3a\u201c\u7c7b\u201d\u6216\u201c\u51fd\u6570\u201d\u6a21\u578b\u4e0a\u7ebf\u3002 \u5982\u679c\u60a8\u53ea\u6709\u4e00\u4e2a\u6a21\u578b\u8981\u670d\u52a1\uff0c\u90a3\u5f88\u5bb9\u6613\u3002 \u4f46\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\uff0c\u60a8\u6709\u81ea\u5b9a\u4e49\u4ee3\u7801\uff0c\u4f8b\u5982\u9884\u5904\u7406\u548c\u540e\u5904\u7406\u3002\u6709\u4e9b\u4efb\u52a1\u9700\u8981\u591a\u4e2a\u6a21\u578b\u534f\u540c\u5de5\u4f5c\u3002 \u4f8b\u5982\uff0c\u5982\u679c\u60a8\u60f3\u9884\u6d4b\u52a8\u7269\u7684\u54c1\u79cd\uff0c\u60a8\u53ef\u80fd\u9700\u8981\u4ee5\u4e0b\u5de5\u4f5c\u6d41\u7a0b\uff1a graph LR pic(\u56fe\u7247) --> species(\u7269\u79cd\u5206\u7c7b) species --> cat(Cat) --> cat_breed(\u732b\u54c1\u79cd\u5206\u7c7b&nbsp&nbsp) --> Persian(\u6ce2\u65af\u732b) species-->\u72d7(\u72d7)--> dog_breed(\u72d7\u54c1\u79cd\u5206\u7c7b&nbsp&nbsp)-->\u62c9\u5e03\u62c9\u591a(\u62c9\u5e03\u62c9\u591a) species-->\u7334\u5b50(\u7334\u5b50)-->\u7334\u5b50\u54c1\u79cd(\u7334\u5b50\u54c1\u79cd\u5206\u7c7b&nbsp&nbsp)-->\u8718\u86db(\u8718\u86db\u7334) \u5728\u8bb8\u591a\u5e73\u53f0\u6216\u5de5\u5177\u4e0a\u90e8\u7f72\u5b83\u5e76\u4e0d\u5bb9\u6613\u3002 \u4f46\u662f\uff0c\u73b0\u5728\u60a8\u62e5\u6709 Pinferencia \uff0c\u60a8\u591a\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u9009\u62e9\uff01","title":"\u542f\u52a8\u4e00\u4e2a\u51fd\u6570"},{"location":"zh/get-started/serve-a-function/#_1","text":"\u597d\u5427\uff0c\u670d\u52a1\u4e00\u4e2a\u51fd\u6570\uff1f\u6709\u7528\u5417\uff1f \u5f53\u7136\u662f\u7684\u3002 \u5982\u679c\u60a8\u6709 \u4e00\u4e2a\u5b8c\u6574\u7684\u63a8\u7406\u5de5\u4f5c\u6d41\u7a0b \uff0c\u5b83\u5305\u542b\u8bb8\u591a\u6b65\u9aa4\u3002\u5927\u591a\u6570\u65f6\u5019\uff0c\u60a8\u5c06\u5b9e\u73b0\u4e00\u4e2a\u51fd\u6570\u6765\u5b8c\u6210\u8fd9\u9879\u5de5\u4f5c\u3002\u73b0\u5728\u60a8\u53ef\u4ee5\u7acb\u5373\u6ce8\u518c\u8be5\u51fd\u6570\u3002 \u5982\u679c\u4f60\u60f3\u5206\u4eab\u4e00\u4e9b\u9884\u5904\u7406\u6216\u540e\u5904\u7406\u529f\u80fd\uff0c\u73b0\u5728\u4f60\u6709\u4f60\u7684\u7f57\u5bbe\u4e86\uff0c \u8759\u8760\u4fa0 \uff01 \u6216\u8005\u4e00\u4e2a\u51fd\u6570\u5bf9\u4f60\u7684\u5de5\u4f5c\u6765\u8bf4\u5c31\u8db3\u591f\u4e86\u3002","title":"\u542f\u52a8\u4e00\u4e2a\u51fd\u6570"},{"location":"zh/get-started/serve-a-function/#_2","text":"\u6211\u4eec\u5f97\u5230\u4e86\u4e00\u4efd\u5c71\u8109\u9ad8\u5ea6\u7684\u5217\u8868\u3002\u6211\u4eec\u9700\u8981\u627e\u51fa\u6700\u9ad8\u3001\u6700\u4f4e\u4ee5\u53ca\u6700\u9ad8\u548c\u6700\u4f4e\u4e4b\u95f4\u7684\u5dee\u5f02\u3002 \u8fd9\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u95ee\u9898\uff0c\u8ba9\u6211\u4eec\u5728\u4e00\u4e2a\u51fd\u6570\u4e2d\u89e3\u51b3\u5b83\uff0c\u8ba9\u60a8\u66f4\u719f\u6089\u8fd9\u4e2a\u6982\u5ff5\u3002 graph LR heights(\u5c71\u7684\u9ad8\u5ea6) --> max(\u627e\u51fa\u6700\u9ad8\u7684&nbsp&nbsp) heights --> min(\u627e\u51fa\u6700\u4f4e\u7684&nbsp&nbsp) min --> diff(\u8ba1\u7b97\u5dee\u5f02) max --> diff diff --> output(\u8f93\u51fa) subgraph Workflow max min diff end","title":"\u4efb\u52a1"},{"location":"zh/get-started/serve-a-function/#_3","text":"\u5c06\u4ee5\u4e0b\u4ee3\u7801\u4fdd\u5b58\u5728 app.py \u4e2d\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from typing import List from pinferencia import Server def calc ( data : List [ int ]) -> int : highest = max ( data ) lowest = min ( data ) return highest - lowest service = Server () service . register ( model_name = \"mountain\" , model = calc )","title":"\u521b\u5efa\u670d\u52a1\u5e76\u6ce8\u518c\u6a21\u578b"},{"location":"zh/get-started/serve-a-function/#_4","text":"$ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)","title":"\u542f\u52a8\u670d\u52a1\u5668"},{"location":"zh/get-started/serve-a-function/#api","text":"\u4f7f\u7528\u4e0b\u9762\u7684\u4ee3\u7801\u521b\u5efa\u4e00\u4e2a test.py \u3002 \u63d0\u793a You need to have requests installed. pip install requests test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/mountain/predict\" , json = { \"data\" : [ 1000 , 2000 , 3000 ]}, ) difference = response . json ()[ \"data\" ] print ( f \"Difference between the highest and lowest is { difference } m.\" ) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py Difference between the highest and lowest is 2000m.","title":"\u6d4b\u8bd5 API"},{"location":"zh/get-started/serve-a-function/#_5","text":"\u73b0\u5728\u4f60\u5df2\u7ecf\u5b66\u4f1a\u4e86\u5982\u4f55\u5c06\u5b9a\u4e49\u4e3a\u201c\u7c7b\u201d\u6216\u201c\u51fd\u6570\u201d\u6a21\u578b\u4e0a\u7ebf\u3002 \u5982\u679c\u60a8\u53ea\u6709\u4e00\u4e2a\u6a21\u578b\u8981\u670d\u52a1\uff0c\u90a3\u5f88\u5bb9\u6613\u3002 \u4f46\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\uff0c\u60a8\u6709\u81ea\u5b9a\u4e49\u4ee3\u7801\uff0c\u4f8b\u5982\u9884\u5904\u7406\u548c\u540e\u5904\u7406\u3002\u6709\u4e9b\u4efb\u52a1\u9700\u8981\u591a\u4e2a\u6a21\u578b\u534f\u540c\u5de5\u4f5c\u3002 \u4f8b\u5982\uff0c\u5982\u679c\u60a8\u60f3\u9884\u6d4b\u52a8\u7269\u7684\u54c1\u79cd\uff0c\u60a8\u53ef\u80fd\u9700\u8981\u4ee5\u4e0b\u5de5\u4f5c\u6d41\u7a0b\uff1a graph LR pic(\u56fe\u7247) --> species(\u7269\u79cd\u5206\u7c7b) species --> cat(Cat) --> cat_breed(\u732b\u54c1\u79cd\u5206\u7c7b&nbsp&nbsp) --> Persian(\u6ce2\u65af\u732b) species-->\u72d7(\u72d7)--> dog_breed(\u72d7\u54c1\u79cd\u5206\u7c7b&nbsp&nbsp)-->\u62c9\u5e03\u62c9\u591a(\u62c9\u5e03\u62c9\u591a) species-->\u7334\u5b50(\u7334\u5b50)-->\u7334\u5b50\u54c1\u79cd(\u7334\u5b50\u54c1\u79cd\u5206\u7c7b&nbsp&nbsp)-->\u8718\u86db(\u8718\u86db\u7334) \u5728\u8bb8\u591a\u5e73\u53f0\u6216\u5de5\u5177\u4e0a\u90e8\u7f72\u5b83\u5e76\u4e0d\u5bb9\u6613\u3002 \u4f46\u662f\uff0c\u73b0\u5728\u60a8\u62e5\u6709 Pinferencia \uff0c\u60a8\u591a\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u9009\u62e9\uff01","title":"\u6b64\u5916"},{"location":"zh/get-started/serve-a-json-model/","text":"\u542f\u52a8\u4e00\u4e2a JSON \u6a21\u578b \u00b6 \u73b0\u5728\u5148\u8ba9\u6211\u4eec\u5c1d\u8bd5\u4e00\u4e2a\u7b80\u5355\u7684\u4f8b\u5b50\uff0c\u8ba9\u4f60\u6765\u719f\u6089 Pinferecia . \u592a\u957f\u4e0d\u770b \u719f\u6089\u5982\u4f55\u901a\u8fc7 Pinferencia \u6ce8\u518c\u548c\u4e0a\u7ebf\u4e00\u4e2a\u6a21\u578b\u975e\u5e38\u91cd\u8981\u3002 \u4e0d\u8fc7\uff0c\u5982\u679c\u4f60\u60f3\u73b0\u5728\u5c31\u5c1d\u8bd5\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u4f60\u53ef\u4ee5\u79fb\u6b65 \u542f\u52a8 Pytorch MNIST Model \u5b9a\u4e49 JSON \u6a21\u578b \u00b6 \u8ba9\u6211\u4eec\u5148\u521b\u5efa\u4e00\u4e2a\u6587\u4ef6 app.py . \u4e0b\u9762\u5c31\u662f\u8fd9\u4e2a JSON \u6a21\u578b. \u8f93\u5165\u662f a \u8fd4\u56de 1 , \u8f93\u5165 b \u8fd4\u56de 2 , \u5176\u4ed6\u8f93\u5165\u8fd4\u56de 0 \u3002 app.py 1 2 3 4 class JSONModel : def predict ( self , data ): knowledge = { \"a\" : 1 , \"b\" : 2 } return knowledge . get ( data , 0 ) \u521b\u5efa\u670d\u52a1\u5e76\u6ce8\u518c\u6a21\u578b \u00b6 \u9996\u5148\u4ece pinferencia \u5bfc\u5165 Server , \u7136\u540e\u521b\u5efa\u4e00\u4e2aserver\u5b9e\u4f8b\u5e76\u6ce8\u518c JSON \u6a21\u578b . app.py 1 2 3 4 5 6 7 8 9 10 11 12 from pinferencia import Server class JSONModel : def predict ( self , data : str ) -> int : knowledge = { \"a\" : 1 , \"b\" : 2 } return knowledge . get ( data , 0 ) model = JSONModel () service = Server () service . register ( model_name = \"json\" , model = model , entrypoint = \"predict\" ) model_name \u548c entrypoint \u662f\u4ec0\u4e48\u610f\u601d? model_name \u4f60\u7ed9\u8fd9\u4e2a\u6a21\u578b\u53d6\u7684\u540d\u5b57\u3002 \u8fd9\u91cc\u6211\u4eec\u53d6\u540d json , \u5bf9\u5e94\u7684\u8fd9\u4e2a\u6a21\u578b\u7684\u5730\u5740\u5c31\u662f http://127.0.0.1:8000/v1/models/json . \u5982\u679c\u5173\u4e8eAPI\u4f60\u6709\u4ec0\u4e48\u4e0d\u6e05\u695a\u7684\uff0c\u4f60\u53ef\u4ee5\u968f\u65f6\u8bbf\u95ee\u4e0b\u9762\u5c06\u8981\u63d0\u5230\u7684\u5728\u7ebfAPI\u6587\u6863\u9875\u9762\u3002 entrypoint predict \u610f\u5473\u7740\u6211\u4eec\u4f1a\u4f7f\u7528 JSON \u6a21\u578b \u7684 predict \u51fd\u6570\u6765\u9884\u6d4b\u6570\u636e\u3002 \u542f\u52a8\u670d\u52a1 \u00b6 $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) \u6253\u5f00\u6d4f\u89c8\u5668\u8bbf\u95ee http://127.0.0.1:8000 , \u73b0\u5728\u4f60\u62e5\u6709\u4e86\u4e00\u4e2a\u81ea\u52a8\u751f\u6210\u7684 API \u6587\u6863\u9875\u9762! FastAPI \u548c Starlette Pinferencia \u57fa\u4e8e FastAPI \uff0c\u5176\u53c8\u57fa\u4e8e Starlette . \u591a\u4e8f\u4e86\u4ed6\u4eec\uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u5e26\u6709 OpenAPI \u89c4\u8303\u7684 API\u3002\u8fd9\u610f\u5473\u7740\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u81ea\u52a8\u6587\u6863\u7f51\u9875\uff0c\u5e76\u4e14\u5ba2\u6237\u7aef\u4ee3\u7801\u4e5f\u53ef\u4ee5\u81ea\u52a8\u751f\u6210\u3002 \u63d0\u793a Pinferencia \u63d0\u4f9b\u4e86\u4e24\u4e2a API \u6587\u6863\u5730\u5740: http://127.0.0.1:8000 or http://127.0.0.1:8000/docs http://127.0.0.1:8000/redoc \u60a8\u53ef\u4ee5\u67e5\u770b API \u89c4\u8303\uff0c\u751a\u81f3\u53ef\u4ee5\u81ea\u5df1 \u8bd5\u7528 API\uff01 \u6d4b\u8bd5 API \u00b6 \u4f7f\u7528\u4e0b\u9762\u7684\u4ee3\u7801\u521b\u5efa\u4e00\u4e2a test.py \u3002 \u63d0\u793a \u4f60\u9700\u8981\u5b89\u88c5 requests . pip install requests test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/json/predict\" , json = { \"data\" : \"a\" }, ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c. $ python test.py {'model_name': 'json', 'data': 1} \u73b0\u5728\u8ba9\u6211\u4eec\u518d\u6dfb\u52a0\u4e24\u4e2a\u8f93\u5165\uff0c\u5e76\u8ba9\u6253\u5370\u66f4\u6f02\u4eae. test.py 1 2 3 4 5 6 7 8 9 10 11 import requests print ( \"| {:^10} | {:^15} |\" . format ( \"Input\" , \"Prediction\" )) print ( \"| {:^10} | {:^15} |\" . format ( \"-\" * 10 , \"-\" * 15 )) for character in [ \"a\" , \"b\" , \"c\" ]: response = requests . post ( url = \"http://localhost:8000/v1/models/json/predict\" , json = { \"data\" : character }, ) print ( f \"| { character : ^10 } | { str ( response . json ()[ 'data' ]) : ^15 } |\" ) \u518d\u6b21\u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py | Input | Prediction | |----------|---------------| | a | 1 | | b | 2 | | c | 0 |","title":"\u542f\u52a8\u4e00\u4e2a\u7b80\u5355\u7684JSON\u6a21\u578b"},{"location":"zh/get-started/serve-a-json-model/#json","text":"\u73b0\u5728\u5148\u8ba9\u6211\u4eec\u5c1d\u8bd5\u4e00\u4e2a\u7b80\u5355\u7684\u4f8b\u5b50\uff0c\u8ba9\u4f60\u6765\u719f\u6089 Pinferecia . \u592a\u957f\u4e0d\u770b \u719f\u6089\u5982\u4f55\u901a\u8fc7 Pinferencia \u6ce8\u518c\u548c\u4e0a\u7ebf\u4e00\u4e2a\u6a21\u578b\u975e\u5e38\u91cd\u8981\u3002 \u4e0d\u8fc7\uff0c\u5982\u679c\u4f60\u60f3\u73b0\u5728\u5c31\u5c1d\u8bd5\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u4f60\u53ef\u4ee5\u79fb\u6b65 \u542f\u52a8 Pytorch MNIST Model","title":"\u542f\u52a8\u4e00\u4e2a JSON \u6a21\u578b"},{"location":"zh/get-started/serve-a-json-model/#json_1","text":"\u8ba9\u6211\u4eec\u5148\u521b\u5efa\u4e00\u4e2a\u6587\u4ef6 app.py . \u4e0b\u9762\u5c31\u662f\u8fd9\u4e2a JSON \u6a21\u578b. \u8f93\u5165\u662f a \u8fd4\u56de 1 , \u8f93\u5165 b \u8fd4\u56de 2 , \u5176\u4ed6\u8f93\u5165\u8fd4\u56de 0 \u3002 app.py 1 2 3 4 class JSONModel : def predict ( self , data ): knowledge = { \"a\" : 1 , \"b\" : 2 } return knowledge . get ( data , 0 )","title":"\u5b9a\u4e49 JSON \u6a21\u578b"},{"location":"zh/get-started/serve-a-json-model/#_1","text":"\u9996\u5148\u4ece pinferencia \u5bfc\u5165 Server , \u7136\u540e\u521b\u5efa\u4e00\u4e2aserver\u5b9e\u4f8b\u5e76\u6ce8\u518c JSON \u6a21\u578b . app.py 1 2 3 4 5 6 7 8 9 10 11 12 from pinferencia import Server class JSONModel : def predict ( self , data : str ) -> int : knowledge = { \"a\" : 1 , \"b\" : 2 } return knowledge . get ( data , 0 ) model = JSONModel () service = Server () service . register ( model_name = \"json\" , model = model , entrypoint = \"predict\" ) model_name \u548c entrypoint \u662f\u4ec0\u4e48\u610f\u601d? model_name \u4f60\u7ed9\u8fd9\u4e2a\u6a21\u578b\u53d6\u7684\u540d\u5b57\u3002 \u8fd9\u91cc\u6211\u4eec\u53d6\u540d json , \u5bf9\u5e94\u7684\u8fd9\u4e2a\u6a21\u578b\u7684\u5730\u5740\u5c31\u662f http://127.0.0.1:8000/v1/models/json . \u5982\u679c\u5173\u4e8eAPI\u4f60\u6709\u4ec0\u4e48\u4e0d\u6e05\u695a\u7684\uff0c\u4f60\u53ef\u4ee5\u968f\u65f6\u8bbf\u95ee\u4e0b\u9762\u5c06\u8981\u63d0\u5230\u7684\u5728\u7ebfAPI\u6587\u6863\u9875\u9762\u3002 entrypoint predict \u610f\u5473\u7740\u6211\u4eec\u4f1a\u4f7f\u7528 JSON \u6a21\u578b \u7684 predict \u51fd\u6570\u6765\u9884\u6d4b\u6570\u636e\u3002","title":"\u521b\u5efa\u670d\u52a1\u5e76\u6ce8\u518c\u6a21\u578b"},{"location":"zh/get-started/serve-a-json-model/#_2","text":"$ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) \u6253\u5f00\u6d4f\u89c8\u5668\u8bbf\u95ee http://127.0.0.1:8000 , \u73b0\u5728\u4f60\u62e5\u6709\u4e86\u4e00\u4e2a\u81ea\u52a8\u751f\u6210\u7684 API \u6587\u6863\u9875\u9762! FastAPI \u548c Starlette Pinferencia \u57fa\u4e8e FastAPI \uff0c\u5176\u53c8\u57fa\u4e8e Starlette . \u591a\u4e8f\u4e86\u4ed6\u4eec\uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u5e26\u6709 OpenAPI \u89c4\u8303\u7684 API\u3002\u8fd9\u610f\u5473\u7740\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u81ea\u52a8\u6587\u6863\u7f51\u9875\uff0c\u5e76\u4e14\u5ba2\u6237\u7aef\u4ee3\u7801\u4e5f\u53ef\u4ee5\u81ea\u52a8\u751f\u6210\u3002 \u63d0\u793a Pinferencia \u63d0\u4f9b\u4e86\u4e24\u4e2a API \u6587\u6863\u5730\u5740: http://127.0.0.1:8000 or http://127.0.0.1:8000/docs http://127.0.0.1:8000/redoc \u60a8\u53ef\u4ee5\u67e5\u770b API \u89c4\u8303\uff0c\u751a\u81f3\u53ef\u4ee5\u81ea\u5df1 \u8bd5\u7528 API\uff01","title":"\u542f\u52a8\u670d\u52a1"},{"location":"zh/get-started/serve-a-json-model/#api","text":"\u4f7f\u7528\u4e0b\u9762\u7684\u4ee3\u7801\u521b\u5efa\u4e00\u4e2a test.py \u3002 \u63d0\u793a \u4f60\u9700\u8981\u5b89\u88c5 requests . pip install requests test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/json/predict\" , json = { \"data\" : \"a\" }, ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c. $ python test.py {'model_name': 'json', 'data': 1} \u73b0\u5728\u8ba9\u6211\u4eec\u518d\u6dfb\u52a0\u4e24\u4e2a\u8f93\u5165\uff0c\u5e76\u8ba9\u6253\u5370\u66f4\u6f02\u4eae. test.py 1 2 3 4 5 6 7 8 9 10 11 import requests print ( \"| {:^10} | {:^15} |\" . format ( \"Input\" , \"Prediction\" )) print ( \"| {:^10} | {:^15} |\" . format ( \"-\" * 10 , \"-\" * 15 )) for character in [ \"a\" , \"b\" , \"c\" ]: response = requests . post ( url = \"http://localhost:8000/v1/models/json/predict\" , json = { \"data\" : character }, ) print ( f \"| { character : ^10 } | { str ( response . json ()[ 'data' ]) : ^15 } |\" ) \u518d\u6b21\u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py | Input | Prediction | |----------|---------------| | a | 1 | | b | 2 | | c | 0 |","title":"\u6d4b\u8bd5 API"},{"location":"zh/handlers/","text":"Handlers \u00b6 BaseHandler \u00b6 BaseHandler \u662f\u4e00\u4e2a\u62bd\u8c61\u57fa\u7840\u7c7b\uff0c\u4f60\u4e0d\u80fd\u76f4\u63a5\u7528\u5b83\u3002 \u4e0d\u8fc7\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u4e0b\u5b83\u7684\u90e8\u5206\u63a5\u53e3\uff0c\u53ef\u4ee5\u8ba9\u6211\u4eec\u62d3\u5c55\u4f7f\u7528: BaseHandler 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class BaseHandler ( abc . ABC ): def preprocess ( self , data : object , parameters : object = None ): return data # (1) def postprocess ( self , data : object , parameters : object = None ): return data # (2) def predict ( self , data : object ): if not getattr ( self , \"model\" , None ): raise Exception ( \"Model is not loaded.\" ) predict_func = ( # (3) getattr ( self . model , self . entrypoint ) if self . entrypoint else self . model ) return predict_func ( data ) @abc . abstractmethod def load_model ( self ): return NotImplemented # (4) \u9ed8\u8ba4\u4ee3\u7801\u5e76\u6ca1\u6709\u505a\u4efb\u4f55\u5904\u7406\uff0c\u4f60\u53ef\u4ee5\u5b9e\u73b0\u81ea\u5df1\u7684\u903b\u8f91\u6765\u505a pre-processing \u5de5\u4f5c\u3002 \u9ed8\u8ba4\u4ee3\u7801\u5e76\u6ca1\u6709\u505a\u4efb\u4f55\u5904\u7406\uff0c\u4f60\u53ef\u4ee5\u5b9e\u73b0\u81ea\u5df1\u7684\u903b\u8f91\u6765\u505a post-processing \u5de5\u4f5c\u3002 \u6839\u636e entrypoint \u548c model \u5bf9\u8c61\uff0c\u627e\u5230\u9884\u6d4b\u51fd\u6570\u3002\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7 self.model \u83b7\u53d6, entrypoint \u53ef\u4ee5\u901a\u8fc7 self.entrypoint \u83b7\u53d6\u3002 \u4f60\u9700\u8981\u5b9e\u73b0\u8fd9\u4e2a\u65b9\u6cd5\u3002 \u6a21\u578b\u8def\u5f84\u53ef\u4ee5\u901a\u8fc7 self.model_path \u83b7\u53d6\u3002 PickleHandler \u00b6 \u9ed8\u8ba4\u7684 handler \u662f PickleHandler . PickleHandler 1 2 3 4 5 6 7 8 class PickleHandler ( BaseHandler ): \"\"\"Pickle Handler for Models Saved through Pickle\"\"\" def load_model ( self ): if not getattr ( self , \"model_path\" , None ): raise Exception ( \"Model path not provided.\" ) with open ( self . model_path , \"rb\" ) as f : return pickle . load ( f )","title":"Handlers"},{"location":"zh/handlers/#handlers","text":"","title":"Handlers"},{"location":"zh/handlers/#basehandler","text":"BaseHandler \u662f\u4e00\u4e2a\u62bd\u8c61\u57fa\u7840\u7c7b\uff0c\u4f60\u4e0d\u80fd\u76f4\u63a5\u7528\u5b83\u3002 \u4e0d\u8fc7\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u4e0b\u5b83\u7684\u90e8\u5206\u63a5\u53e3\uff0c\u53ef\u4ee5\u8ba9\u6211\u4eec\u62d3\u5c55\u4f7f\u7528: BaseHandler 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class BaseHandler ( abc . ABC ): def preprocess ( self , data : object , parameters : object = None ): return data # (1) def postprocess ( self , data : object , parameters : object = None ): return data # (2) def predict ( self , data : object ): if not getattr ( self , \"model\" , None ): raise Exception ( \"Model is not loaded.\" ) predict_func = ( # (3) getattr ( self . model , self . entrypoint ) if self . entrypoint else self . model ) return predict_func ( data ) @abc . abstractmethod def load_model ( self ): return NotImplemented # (4) \u9ed8\u8ba4\u4ee3\u7801\u5e76\u6ca1\u6709\u505a\u4efb\u4f55\u5904\u7406\uff0c\u4f60\u53ef\u4ee5\u5b9e\u73b0\u81ea\u5df1\u7684\u903b\u8f91\u6765\u505a pre-processing \u5de5\u4f5c\u3002 \u9ed8\u8ba4\u4ee3\u7801\u5e76\u6ca1\u6709\u505a\u4efb\u4f55\u5904\u7406\uff0c\u4f60\u53ef\u4ee5\u5b9e\u73b0\u81ea\u5df1\u7684\u903b\u8f91\u6765\u505a post-processing \u5de5\u4f5c\u3002 \u6839\u636e entrypoint \u548c model \u5bf9\u8c61\uff0c\u627e\u5230\u9884\u6d4b\u51fd\u6570\u3002\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7 self.model \u83b7\u53d6, entrypoint \u53ef\u4ee5\u901a\u8fc7 self.entrypoint \u83b7\u53d6\u3002 \u4f60\u9700\u8981\u5b9e\u73b0\u8fd9\u4e2a\u65b9\u6cd5\u3002 \u6a21\u578b\u8def\u5f84\u53ef\u4ee5\u901a\u8fc7 self.model_path \u83b7\u53d6\u3002","title":"BaseHandler"},{"location":"zh/handlers/#picklehandler","text":"\u9ed8\u8ba4\u7684 handler \u662f PickleHandler . PickleHandler 1 2 3 4 5 6 7 8 class PickleHandler ( BaseHandler ): \"\"\"Pickle Handler for Models Saved through Pickle\"\"\" def load_model ( self ): if not getattr ( self , \"model_path\" , None ): raise Exception ( \"Model path not provided.\" ) with open ( self . model_path , \"rb\" ) as f : return pickle . load ( f )","title":"PickleHandler"},{"location":"zh/install/","text":"\u5b89\u88c5 Pinferencia \u00b6 \u63a8\u8350\u65b9\u5f0f \u00b6 \u63a8\u8350\u5c06 Pinferencia \u4e0e uvicorn \u540c\u65f6\u5b89\u88c5\uff0c\u8fd9\u6837\u53ef\u4ee5\u7acb\u523b\u542f\u52a8\u4f60\u7684\u670d\u52a1. $ pip install \"pinferencia[uvicorn]\" ---> 100% \u6216\u8005... \u00b6 \u4f60\u4e5f\u53ef\u4ee5\u9009\u62e9\u4ec5 Pinferencia \u3002\u7a0d\u540e\u4f60\u53ef\u4ee5\u518d\u9009\u62e9\u5176\u5b83\u7684ASGI Server\u3002 $ pip install pinferencia ---> 100%","title":"\u5b89\u88c5"},{"location":"zh/install/#pinferencia","text":"","title":"\u5b89\u88c5 Pinferencia"},{"location":"zh/install/#_1","text":"\u63a8\u8350\u5c06 Pinferencia \u4e0e uvicorn \u540c\u65f6\u5b89\u88c5\uff0c\u8fd9\u6837\u53ef\u4ee5\u7acb\u523b\u542f\u52a8\u4f60\u7684\u670d\u52a1. $ pip install \"pinferencia[uvicorn]\" ---> 100%","title":"\u63a8\u8350\u65b9\u5f0f"},{"location":"zh/install/#_2","text":"\u4f60\u4e5f\u53ef\u4ee5\u9009\u62e9\u4ec5 Pinferencia \u3002\u7a0d\u540e\u4f60\u53ef\u4ee5\u518d\u9009\u62e9\u5176\u5b83\u7684ASGI Server\u3002 $ pip install pinferencia ---> 100%","title":"\u6216\u8005..."},{"location":"zh/ml/huggingface/dependencies/","text":"\u5bf9\u4e8emac\u7528\u6237 \u00b6 \u5982\u679c\u4f60\u50cf\u6211\u4e00\u6837\u5728 M1 Mac \u4e0a\u5de5\u4f5c\uff0c\u4f60\u9700\u8981\u5b89\u88c5 cmake \u548c rust brew install cmake curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh \u5b89\u88c5\u4f9d\u8d56 \u00b6 \u60a8\u53ef\u4ee5\u4f7f\u7528 pip \u5b89\u88c5\u4f9d\u8d56\u9879\u3002 pip install tqdm boto3 requests regex sentencepiece sacremoses \u6216\u8005\u60a8\u53ef\u4ee5\u6539\u7528 docker \u6620\u50cf\uff1a docker run -it -p 8000 :8000 -v $( pwd ) :/opt/workspace huggingface/transformers-pytorch-cpu:4.18.0 bash","title":"\u5b89\u88c5\u4f9d\u8d56"},{"location":"zh/ml/huggingface/dependencies/#mac","text":"\u5982\u679c\u4f60\u50cf\u6211\u4e00\u6837\u5728 M1 Mac \u4e0a\u5de5\u4f5c\uff0c\u4f60\u9700\u8981\u5b89\u88c5 cmake \u548c rust brew install cmake curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh","title":"\u5bf9\u4e8emac\u7528\u6237"},{"location":"zh/ml/huggingface/dependencies/#_1","text":"\u60a8\u53ef\u4ee5\u4f7f\u7528 pip \u5b89\u88c5\u4f9d\u8d56\u9879\u3002 pip install tqdm boto3 requests regex sentencepiece sacremoses \u6216\u8005\u60a8\u53ef\u4ee5\u6539\u7528 docker \u6620\u50cf\uff1a docker run -it -p 8000 :8000 -v $( pwd ) :/opt/workspace huggingface/transformers-pytorch-cpu:4.18.0 bash","title":"\u5b89\u88c5\u4f9d\u8d56"},{"location":"zh/ml/huggingface/pipeline/nlp/bert/","text":"\u4f60\u4eec\u4e2d\u7684\u8bb8\u591a\u4eba\u4e00\u5b9a\u542c\u8bf4\u8fc7\u201cBert\u201d\u6216\u201ctransformers\u201d\u3002 \u4f60\u53ef\u80fd\u8fd8\u77e5\u9053huggingface\u3002 \u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u8ba9\u6211\u4eec\u4f7f\u7528\u5b83\u7684 pytorch \u8f6c\u6362\u5668\u6a21\u578b\u5e76\u901a\u8fc7 REST API \u4e3a\u5b83\u63d0\u4f9b\u670d\u52a1 \u6a21\u578b\u662f\u5982\u4f55\u5de5\u4f5c\u7684\uff1f \u00b6 \u8f93\u5165\u4e00\u4e2a\u4e0d\u5b8c\u6574\u7684\u53e5\u5b50\uff0c\u6a21\u578b\u5c06\u7ed9\u51fa\u5b83\u7684\u9884\u6d4b\uff1a \u8f93\u5165 \u8f93\u51fa Paris is the [MASK] of France. Paris is the capital of France. \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427 \u5148\u51b3\u6761\u4ef6 \u00b6 \u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879 \u670d\u52a1\u6a21\u578b \u00b6 \u5b89\u88c5 Pinferencia \u00b6 \u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[uvicorn]\" \u521b\u5efaapp.py \u00b6 \u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 from transformers import pipeline from pinferencia import Server bert = pipeline ( \"fill-mask\" , model = \"bert-base-uncased\" ) service = Server () service . register ( model_name = \"bert\" , model = lambda text : bert ( text )) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) \u6d4b\u8bd5\u670d\u52a1 \u00b6 curl Python Requests \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/bert/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"Paris is the [MASK] of France.\" }' \u54cd\u5e94 { \"model_name\":\"bert\", \"data\":\"Paris is the capital of France.\" } \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/bert/predict\" , json = { \"data\" : \"Paris is the [MASK] of France.\" }, ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py {'model_name': 'bert', 'data': 'Paris is the capital of France.'}","title":"Bert"},{"location":"zh/ml/huggingface/pipeline/nlp/bert/#_1","text":"\u8f93\u5165\u4e00\u4e2a\u4e0d\u5b8c\u6574\u7684\u53e5\u5b50\uff0c\u6a21\u578b\u5c06\u7ed9\u51fa\u5b83\u7684\u9884\u6d4b\uff1a \u8f93\u5165 \u8f93\u51fa Paris is the [MASK] of France. Paris is the capital of France. \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427","title":"\u6a21\u578b\u662f\u5982\u4f55\u5de5\u4f5c\u7684\uff1f"},{"location":"zh/ml/huggingface/pipeline/nlp/bert/#_2","text":"\u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"zh/ml/huggingface/pipeline/nlp/bert/#_3","text":"","title":"\u670d\u52a1\u6a21\u578b"},{"location":"zh/ml/huggingface/pipeline/nlp/bert/#pinferencia","text":"\u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[uvicorn]\"","title":"\u5b89\u88c5 Pinferencia"},{"location":"zh/ml/huggingface/pipeline/nlp/bert/#apppy","text":"\u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 from transformers import pipeline from pinferencia import Server bert = pipeline ( \"fill-mask\" , model = \"bert-base-uncased\" ) service = Server () service . register ( model_name = \"bert\" , model = lambda text : bert ( text )) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)","title":"\u521b\u5efaapp.py"},{"location":"zh/ml/huggingface/pipeline/nlp/bert/#_4","text":"curl Python Requests \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/bert/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"Paris is the [MASK] of France.\" }' \u54cd\u5e94 { \"model_name\":\"bert\", \"data\":\"Paris is the capital of France.\" } \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/bert/predict\" , json = { \"data\" : \"Paris is the [MASK] of France.\" }, ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py {'model_name': 'bert', 'data': 'Paris is the capital of France.'}","title":"\u6d4b\u8bd5\u670d\u52a1"},{"location":"zh/ml/huggingface/pipeline/nlp/text-generation/","text":"GPT2\u200a-\u200a\u6587\u672c\u751f\u6210\u8f6c\u6362\u5668\uff1a\u5982\u4f55\u4f7f\u7528\u548c\u542f\u52a8\u670d\u52a1 \u00b6 \u4ec0\u4e48\u662f\u6587\u672c\u751f\u6210\uff1f\u8f93\u5165\u4e00\u4e9b\u6587\u672c\uff0c\u6a21\u578b\u5c06\u9884\u6d4b\u540e\u7eed\u6587\u672c\u4f1a\u662f\u4ec0\u4e48\u3002 \u542c\u8d77\u6765\u4e0d\u9519\u3002\u4e0d\u8fc7\u4e0d\u4eb2\u81ea\u5c1d\u8bd5\u6a21\u578b\u600e\u4e48\u53ef\u80fd\u6709\u8da3\uff1f \u5982\u4f55\u4f7f\u7528 \u00b6 \u6a21\u578b\u5c06\u81ea\u52a8\u4e0b\u8f7d from transformers import pipeline , set_seed generator = pipeline ( \"text-generation\" , model = \"gpt2\" ) set_seed ( 42 ) def predict ( text ): return generator ( text , max_length = 50 , num_return_sequences = 3 ) \u5c31\u662f\u8fd9\u6837\uff01 \u8ba9\u6211\u4eec\u5c1d\u8bd5\u4e00\u4e0b\uff1a predict ( \"You look amazing today,\" ) \u7ed3\u679c\uff1a [{'generated_text': 'You look amazing today, guys. If you\\'re still in school and you still have a job where you work in the field\u2026 you\\'re going to look ridiculous by now, you\\'re going to look really ridiculous.\"\\n\\nHe turned to his friends'}, {'generated_text': 'You look amazing today, aren\\'t you?\"\\n\\nHe turned and looked at me. He had an expression that was full of worry as he looked at me. Even before he told me I\\'d have sex, he gave up after I told him'}, {'generated_text': 'You look amazing today, and look amazing in the sunset.\"\\n\\nGarry, then 33, won the London Marathon at age 15, and the World Triathlon in 2007, the two youngest Olympians to ride 100-meters. He also'}] \u8ba9\u6211\u4eec\u770b\u770b\u7b2c\u4e00\u4e2a\u7ed3\u679c\u3002 You look amazing today, guys. If you're still in school and you still have a job where you work in the field\u2026 you're going to look ridiculous by now, you're going to look really ridiculous.\" He turned to his friends \ud83e\udd23 \u8fd9\u5c31\u662f\u6211\u4eec\u8981\u627e\u7684\u4e1c\u897f\uff01\u5982\u679c\u518d\u6b21\u8fd0\u884c\u9884\u6d4b\uff0c\u6bcf\u6b21\u90fd\u4f1a\u7ed9\u51fa\u4e0d\u540c\u7684\u7ed3\u679c\u3002 \u5982\u4f55\u90e8\u7f72 \u00b6 \u5b89\u88c5 Pinferencia \u00b6 $ pip install \"pinferencia[uvicorn]\" ---> 100% \u521b\u5efa\u670d\u52a1 \u00b6 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from transformers import pipeline , set_seed from pinferencia import Server generator = pipeline ( \"text-generation\" , model = \"gpt2\" ) set_seed ( 42 ) def predict ( text ): return generator ( text , max_length = 50 , num_return_sequences = 3 ) service = Server () service . register ( model_name = \"gpt2\" , model = predict ) \u542f\u52a8\u670d\u52a1 \u00b6 $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) \u6d4b\u8bd5\u670d\u52a1 \u00b6 Curl Python requests curl -X 'POST' \\ 'http://127.0.0.1:8000/v1/models/gpt2/predict' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"id\": \"string\", \"parameters\": {}, \"data\": \"You look amazing today,\" }' \u7ed3\u679c: { \"id\" : \"string\" , \"model_name\" : \"gpt2\" , \"data\" : [ { \"generated_text\" : \"You look amazing today, I was in front of my friends. I wanted everyone to see me. But that's all. No one really cares about me in the eyes of the whole world unless I love them.\\\"\\n\\nIn a second Facebook post\" }, { \"generated_text\" : \"You look amazing today, and I know I am going to get the job done! So thank you all for all those donations, money, help, and hugs. I hope to see you again soon.\" }, { \"generated_text\" : \"You look amazing today, but I will have to wait until early June for what will go down as the first NBA championship (a thing I had been expecting). If it's not the biggest, it is also not great. Now let's look at\" } ] } test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/gpt2/predict\" , json = { \"data\" : \"You look amazing today,\" }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) \u8fd0\u884c python test.py \u5e76\u6253\u5370\u7ed3\u679c\uff1a Prediction: [ { \"generated_text\": \"You look amazing today, I was in front of my friends. I wanted everyone to see me. But that's all. No one really cares about me in the eyes of the whole world unless I love them.\\\"\\n\\nIn a second Facebook post\" }, { \"generated_text\": \"You look amazing today, and I know I am going to get the job done! So thank you all for all those donations, money, help, and hugs. I hope to see you again soon.\" }, { \"generated_text\": \"You look amazing today, but I will have to wait until early June for what will go down as the first NBA championship (a thing I had been expecting). If it's not the biggest, it is also not great. Now let's look at\" } ] \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8000 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u4ea4\u4e92\u5f0fUI\u3002 \u60a8\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u6587\u672c\u751f\u6210 - GPT2"},{"location":"zh/ml/huggingface/pipeline/nlp/text-generation/#gpt2-","text":"\u4ec0\u4e48\u662f\u6587\u672c\u751f\u6210\uff1f\u8f93\u5165\u4e00\u4e9b\u6587\u672c\uff0c\u6a21\u578b\u5c06\u9884\u6d4b\u540e\u7eed\u6587\u672c\u4f1a\u662f\u4ec0\u4e48\u3002 \u542c\u8d77\u6765\u4e0d\u9519\u3002\u4e0d\u8fc7\u4e0d\u4eb2\u81ea\u5c1d\u8bd5\u6a21\u578b\u600e\u4e48\u53ef\u80fd\u6709\u8da3\uff1f","title":"GPT2\u200a-\u200a\u6587\u672c\u751f\u6210\u8f6c\u6362\u5668\uff1a\u5982\u4f55\u4f7f\u7528\u548c\u542f\u52a8\u670d\u52a1"},{"location":"zh/ml/huggingface/pipeline/nlp/text-generation/#_1","text":"\u6a21\u578b\u5c06\u81ea\u52a8\u4e0b\u8f7d from transformers import pipeline , set_seed generator = pipeline ( \"text-generation\" , model = \"gpt2\" ) set_seed ( 42 ) def predict ( text ): return generator ( text , max_length = 50 , num_return_sequences = 3 ) \u5c31\u662f\u8fd9\u6837\uff01 \u8ba9\u6211\u4eec\u5c1d\u8bd5\u4e00\u4e0b\uff1a predict ( \"You look amazing today,\" ) \u7ed3\u679c\uff1a [{'generated_text': 'You look amazing today, guys. If you\\'re still in school and you still have a job where you work in the field\u2026 you\\'re going to look ridiculous by now, you\\'re going to look really ridiculous.\"\\n\\nHe turned to his friends'}, {'generated_text': 'You look amazing today, aren\\'t you?\"\\n\\nHe turned and looked at me. He had an expression that was full of worry as he looked at me. Even before he told me I\\'d have sex, he gave up after I told him'}, {'generated_text': 'You look amazing today, and look amazing in the sunset.\"\\n\\nGarry, then 33, won the London Marathon at age 15, and the World Triathlon in 2007, the two youngest Olympians to ride 100-meters. He also'}] \u8ba9\u6211\u4eec\u770b\u770b\u7b2c\u4e00\u4e2a\u7ed3\u679c\u3002 You look amazing today, guys. If you're still in school and you still have a job where you work in the field\u2026 you're going to look ridiculous by now, you're going to look really ridiculous.\" He turned to his friends \ud83e\udd23 \u8fd9\u5c31\u662f\u6211\u4eec\u8981\u627e\u7684\u4e1c\u897f\uff01\u5982\u679c\u518d\u6b21\u8fd0\u884c\u9884\u6d4b\uff0c\u6bcf\u6b21\u90fd\u4f1a\u7ed9\u51fa\u4e0d\u540c\u7684\u7ed3\u679c\u3002","title":"\u5982\u4f55\u4f7f\u7528"},{"location":"zh/ml/huggingface/pipeline/nlp/text-generation/#_2","text":"","title":"\u5982\u4f55\u90e8\u7f72"},{"location":"zh/ml/huggingface/pipeline/nlp/text-generation/#pinferencia","text":"$ pip install \"pinferencia[uvicorn]\" ---> 100%","title":"\u5b89\u88c5Pinferencia"},{"location":"zh/ml/huggingface/pipeline/nlp/text-generation/#_3","text":"app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from transformers import pipeline , set_seed from pinferencia import Server generator = pipeline ( \"text-generation\" , model = \"gpt2\" ) set_seed ( 42 ) def predict ( text ): return generator ( text , max_length = 50 , num_return_sequences = 3 ) service = Server () service . register ( model_name = \"gpt2\" , model = predict )","title":"\u521b\u5efa\u670d\u52a1"},{"location":"zh/ml/huggingface/pipeline/nlp/text-generation/#_4","text":"$ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)","title":"\u542f\u52a8\u670d\u52a1"},{"location":"zh/ml/huggingface/pipeline/nlp/text-generation/#_5","text":"Curl Python requests curl -X 'POST' \\ 'http://127.0.0.1:8000/v1/models/gpt2/predict' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"id\": \"string\", \"parameters\": {}, \"data\": \"You look amazing today,\" }' \u7ed3\u679c: { \"id\" : \"string\" , \"model_name\" : \"gpt2\" , \"data\" : [ { \"generated_text\" : \"You look amazing today, I was in front of my friends. I wanted everyone to see me. But that's all. No one really cares about me in the eyes of the whole world unless I love them.\\\"\\n\\nIn a second Facebook post\" }, { \"generated_text\" : \"You look amazing today, and I know I am going to get the job done! So thank you all for all those donations, money, help, and hugs. I hope to see you again soon.\" }, { \"generated_text\" : \"You look amazing today, but I will have to wait until early June for what will go down as the first NBA championship (a thing I had been expecting). If it's not the biggest, it is also not great. Now let's look at\" } ] } test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/gpt2/predict\" , json = { \"data\" : \"You look amazing today,\" }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) \u8fd0\u884c python test.py \u5e76\u6253\u5370\u7ed3\u679c\uff1a Prediction: [ { \"generated_text\": \"You look amazing today, I was in front of my friends. I wanted everyone to see me. But that's all. No one really cares about me in the eyes of the whole world unless I love them.\\\"\\n\\nIn a second Facebook post\" }, { \"generated_text\": \"You look amazing today, and I know I am going to get the job done! So thank you all for all those donations, money, help, and hugs. I hope to see you again soon.\" }, { \"generated_text\": \"You look amazing today, but I will have to wait until early June for what will go down as the first NBA championship (a thing I had been expecting). If it's not the biggest, it is also not great. Now let's look at\" } ] \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8000 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u4ea4\u4e92\u5f0fUI\u3002 \u60a8\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u6d4b\u8bd5\u670d\u52a1"},{"location":"zh/ml/huggingface/pipeline/nlp/translation/","text":"Google T5 \u7ffb\u8bd1\u5373\u670d\u52a1\uff0c\u53ea\u9700 7 \u884c\u4ee3\u7801 \u00b6 \u4ec0\u4e48\u662fT5\uff1f Google \u7684 Text-To-Text Transfer Transformer (T5) \u63d0\u4f9b\u4e86\u7ffb\u8bd1\u529f\u80fd\u3002 \u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5c06 Google T5 \u6a21\u578b\u90e8\u7f72\u4e3a REST API \u670d\u52a1\u3002 \u96be\u7684\uff1f \u6211\u544a\u8bc9\u4f60\u600e\u4e48\u6837\uff1a\u4f60\u53ea\u9700\u8981\u5199 7 \u884c\u4ee3\u7801\uff1f \u5b89\u88c5\u4f9d\u8d56 \u00b6 HuggingFace \u00b6 pip install \"transformers[pytorch]\" \u5982\u679c\u4e0d\u8d77\u4f5c\u7528\uff0c\u8bf7\u8bbf\u95ee Installation \u5e76\u67e5\u770b\u5176\u5b98\u65b9\u6587\u6863\u3002 Pinferencia \u00b6 pip install \"pinferencia[uvicorn]\" \u5b9a\u4e49\u670d\u52a1 \u00b6 \u9996\u5148\u8ba9\u6211\u4eec\u521b\u5efa app.py \u6765\u5b9a\u4e49\u670d\u52a1\uff1a app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from transformers import pipeline from pinferencia import Server t5 = pipeline ( model = \"t5-base\" , tokenizer = \"t5-base\" ) def translate ( text ): return t5 ( text ) service = Server () service . register ( model_name = \"t5\" , model = translate ) \u542f\u52a8\u670d\u52a1 \u00b6 $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) \u6d4b\u8bd5\u670d\u52a1 \u00b6 Curl Python requests curl -X 'POST' \\ 'http://localhost:8000/v1/models/t5/predict' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"parameters\": {}, \"data\": \"translate English to German: Good morning, my love.\" }' \u7ed3\u679c: { \"model_name\" : \"t5\" , \"data\" : [ { \"translation_text\" : \"Guten Morgen, liebe Liebe.\" } ] } test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/gpt2/predict\" , json = { \"data\" : \"translate English to German: Good morning, my love.\" }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) \u8fd0\u884c python test.py \u5e76\u6253\u5370\u7ed3\u679c\uff1a Prediction: { \"translation_text\": \"Guten Morgen, liebe Liebe.\" } \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8000 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u4ea4\u4e92\u5f0f ui\u3002 \u60a8\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u7ffb\u8bd1 - Google T5"},{"location":"zh/ml/huggingface/pipeline/nlp/translation/#google-t5-7","text":"\u4ec0\u4e48\u662fT5\uff1f Google \u7684 Text-To-Text Transfer Transformer (T5) \u63d0\u4f9b\u4e86\u7ffb\u8bd1\u529f\u80fd\u3002 \u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5c06 Google T5 \u6a21\u578b\u90e8\u7f72\u4e3a REST API \u670d\u52a1\u3002 \u96be\u7684\uff1f \u6211\u544a\u8bc9\u4f60\u600e\u4e48\u6837\uff1a\u4f60\u53ea\u9700\u8981\u5199 7 \u884c\u4ee3\u7801\uff1f","title":"Google T5 \u7ffb\u8bd1\u5373\u670d\u52a1\uff0c\u53ea\u9700 7 \u884c\u4ee3\u7801"},{"location":"zh/ml/huggingface/pipeline/nlp/translation/#_1","text":"","title":"\u5b89\u88c5\u4f9d\u8d56"},{"location":"zh/ml/huggingface/pipeline/nlp/translation/#huggingface","text":"pip install \"transformers[pytorch]\" \u5982\u679c\u4e0d\u8d77\u4f5c\u7528\uff0c\u8bf7\u8bbf\u95ee Installation \u5e76\u67e5\u770b\u5176\u5b98\u65b9\u6587\u6863\u3002","title":"HuggingFace"},{"location":"zh/ml/huggingface/pipeline/nlp/translation/#pinferencia","text":"pip install \"pinferencia[uvicorn]\"","title":"Pinferencia"},{"location":"zh/ml/huggingface/pipeline/nlp/translation/#_2","text":"\u9996\u5148\u8ba9\u6211\u4eec\u521b\u5efa app.py \u6765\u5b9a\u4e49\u670d\u52a1\uff1a app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from transformers import pipeline from pinferencia import Server t5 = pipeline ( model = \"t5-base\" , tokenizer = \"t5-base\" ) def translate ( text ): return t5 ( text ) service = Server () service . register ( model_name = \"t5\" , model = translate )","title":"\u5b9a\u4e49\u670d\u52a1"},{"location":"zh/ml/huggingface/pipeline/nlp/translation/#_3","text":"$ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)","title":"\u542f\u52a8\u670d\u52a1"},{"location":"zh/ml/huggingface/pipeline/nlp/translation/#_4","text":"Curl Python requests curl -X 'POST' \\ 'http://localhost:8000/v1/models/t5/predict' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"parameters\": {}, \"data\": \"translate English to German: Good morning, my love.\" }' \u7ed3\u679c: { \"model_name\" : \"t5\" , \"data\" : [ { \"translation_text\" : \"Guten Morgen, liebe Liebe.\" } ] } test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/gpt2/predict\" , json = { \"data\" : \"translate English to German: Good morning, my love.\" }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) \u8fd0\u884c python test.py \u5e76\u6253\u5370\u7ed3\u679c\uff1a Prediction: { \"translation_text\": \"Guten Morgen, liebe Liebe.\" } \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8000 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u4ea4\u4e92\u5f0f ui\u3002 \u60a8\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u6d4b\u8bd5\u670d\u52a1"},{"location":"zh/ml/huggingface/pipeline/vision/","text":"\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u63a2\u8ba8\u5982\u4f55\u4f7f\u7528 Hugging Face \u7ba1\u9053\uff0c\u4ee5\u53ca\u5982\u4f55\u4f7f\u7528 Pinferencia \u4f5c\u4e3a REST API \u90e8\u7f72\u5b83\u3002 \u5148\u51b3\u6761\u4ef6 \u00b6 \u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879 \u4e0b\u8f7d\u6a21\u578b\u5e76\u9884\u6d4b \u00b6 \u6a21\u578b\u5c06\u81ea\u52a8\u4e0b\u8f7d\u3002 1 2 3 4 5 6 from transformers import pipeline vision_classifier = pipeline ( task = \"image-classification\" ) vision_classifier ( images = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\" ) \u7ed3\u679c: [{ 'label' : 'lynx, catamount' , 'score' : 0.4403027892112732 }, { 'label' : 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor' , 'score' : 0.03433405980467796 }, { 'label' : 'snow leopard, ounce, Panthera uncia' , 'score' : 0.032148055732250214 }, { 'label' : 'Egyptian cat' , 'score' : 0.02353910356760025 }, { 'label' : 'tiger cat' , 'score' : 0.023034192621707916 }] \u8ba9\u6211\u4eec\u5c1d\u8bd5\u53e6\u4e00\u4e2a\u56fe\u50cf\uff0c\u8ba9\u6211\u4eec\u5c1d\u8bd5\u5728\u4e00\u6279\u4e2d\u9884\u6d4b\u4e24\u4e2a\u56fe\u50cf\uff1a 1 2 3 4 image = \"https://cdn.pixabay.com/photo/2018/08/12/16/59/parrot-3601194_1280.jpg\" vision_classifier ( images = [ image , image ] ) \u7ed3\u679c: [[{ 'score' : 0.9489120244979858 , 'label' : 'macaw' }, { 'score' : 0.014800671488046646 , 'label' : 'broom' }, { 'score' : 0.009150494821369648 , 'label' : 'swab, swob, mop' }, { 'score' : 0.0018255198374390602 , 'label' : \"plunger, plumber's helper\" }, { 'score' : 0.0017631321679800749 , 'label' : 'African grey, African gray, Psittacus erithacus' }], [{ 'score' : 0.9489120244979858 , 'label' : 'macaw' }, { 'score' : 0.014800671488046646 , 'label' : 'broom' }, { 'score' : 0.009150494821369648 , 'label' : 'swab, swob, mop' }, { 'score' : 0.0018255198374390602 , 'label' : \"plunger, plumber's helper\" }, { 'score' : 0.0017631321679800749 , 'label' : 'African grey, African gray, Psittacus erithacus' }]] \u51fa\u4e4e\u610f\u6599\u7684\u5bb9\u6613\uff01 \u73b0\u5728\u8ba9\u6211\u4eec\u8bd5\u8bd5\uff1a \u90e8\u7f72\u6a21\u578b \u00b6 \u6ca1\u6709\u90e8\u7f72\uff0c\u673a\u5668\u5b66\u4e60\u6559\u7a0b\u600e\u4e48\u53ef\u80fd\u5b8c\u6574\uff1f \u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[uvicorn]\" \u73b0\u5728\u8ba9\u6211\u4eec\u7528\u4ee3\u7801\u521b\u5efa\u4e00\u4e2a app.py \u6587\u4ef6\uff1a app.py 1 2 3 4 5 6 7 8 9 10 11 from transformers import pipeline from pinferencia import Server vision_classifier = pipeline ( task = \"image-classification\" ) def classify ( data ): return vision_classifier ( images = data ) service = Server () service . register ( model_name = \"vision\" , model = classify ) \u5bb9\u6613\uff0c\u5bf9\u5427\uff1f \u9884\u6d4b \u00b6 Curl Python requests curl --location --request POST 'http://127.0.0.1:8000/v1/models/vision/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"https://cdn.pixabay.com/photo/2018/08/12/16/59/parrot-3601194_1280.jpg\" }' \u7ed3\u679c: Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/vision/predict\" , json = { \"data\" : \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\" # noqa }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) \u8fd0\u884c python test.py \uff0c\u67e5\u770b\u7ed3\u679c\uff1a Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8000 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u4ea4\u4e92\u5f0f ui\u3002 \u60a8\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01 \u8fdb\u4e00\u6b65\u6539\u8fdb \u00b6 \u4f46\u662f\uff0c\u6709\u65f6\u4f7f\u7528\u56fe\u50cf\u7684 url \u6765\u9884\u6d4b\u662f\u4e0d\u5408\u9002\u7684\u3002 \u8ba9\u6211\u4eec\u7a0d\u5fae\u4fee\u6539 app.py \u4ee5\u63a5\u53d7 Base64 Encoded String \u4f5c\u4e3a\u8f93\u5165\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import base64 from io import BytesIO from PIL import Image from transformers import pipeline from pinferencia import Server vision_classifier = pipeline ( task = \"image-classification\" ) def classify ( image_base64_str ): image = Image . open ( BytesIO ( base64 . b64decode ( image_base64_str ))) return vision_classifier ( images = image ) service = Server () service . register ( model_name = \"vision\" , model = classify ) \u518d\u6b21\u9884\u6d4b \u00b6 Curl Python requests curl --location --request POST 'http://127.0.0.1:8000/v1/models/vision/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"...\" }' \u7ed3\u679c\uff1a Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/vision/predict\" , json = { \"data\" : \"...\" # noqa }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) \u8fd0\u884c python test.py \u5e76\u67e5\u770b\u7ed3\u679c\uff1a Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ]","title":"\u56fe\u50cf\u8bc6\u522b"},{"location":"zh/ml/huggingface/pipeline/vision/#_1","text":"\u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"zh/ml/huggingface/pipeline/vision/#_2","text":"\u6a21\u578b\u5c06\u81ea\u52a8\u4e0b\u8f7d\u3002 1 2 3 4 5 6 from transformers import pipeline vision_classifier = pipeline ( task = \"image-classification\" ) vision_classifier ( images = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\" ) \u7ed3\u679c: [{ 'label' : 'lynx, catamount' , 'score' : 0.4403027892112732 }, { 'label' : 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor' , 'score' : 0.03433405980467796 }, { 'label' : 'snow leopard, ounce, Panthera uncia' , 'score' : 0.032148055732250214 }, { 'label' : 'Egyptian cat' , 'score' : 0.02353910356760025 }, { 'label' : 'tiger cat' , 'score' : 0.023034192621707916 }] \u8ba9\u6211\u4eec\u5c1d\u8bd5\u53e6\u4e00\u4e2a\u56fe\u50cf\uff0c\u8ba9\u6211\u4eec\u5c1d\u8bd5\u5728\u4e00\u6279\u4e2d\u9884\u6d4b\u4e24\u4e2a\u56fe\u50cf\uff1a 1 2 3 4 image = \"https://cdn.pixabay.com/photo/2018/08/12/16/59/parrot-3601194_1280.jpg\" vision_classifier ( images = [ image , image ] ) \u7ed3\u679c: [[{ 'score' : 0.9489120244979858 , 'label' : 'macaw' }, { 'score' : 0.014800671488046646 , 'label' : 'broom' }, { 'score' : 0.009150494821369648 , 'label' : 'swab, swob, mop' }, { 'score' : 0.0018255198374390602 , 'label' : \"plunger, plumber's helper\" }, { 'score' : 0.0017631321679800749 , 'label' : 'African grey, African gray, Psittacus erithacus' }], [{ 'score' : 0.9489120244979858 , 'label' : 'macaw' }, { 'score' : 0.014800671488046646 , 'label' : 'broom' }, { 'score' : 0.009150494821369648 , 'label' : 'swab, swob, mop' }, { 'score' : 0.0018255198374390602 , 'label' : \"plunger, plumber's helper\" }, { 'score' : 0.0017631321679800749 , 'label' : 'African grey, African gray, Psittacus erithacus' }]] \u51fa\u4e4e\u610f\u6599\u7684\u5bb9\u6613\uff01 \u73b0\u5728\u8ba9\u6211\u4eec\u8bd5\u8bd5\uff1a","title":"\u4e0b\u8f7d\u6a21\u578b\u5e76\u9884\u6d4b"},{"location":"zh/ml/huggingface/pipeline/vision/#_3","text":"\u6ca1\u6709\u90e8\u7f72\uff0c\u673a\u5668\u5b66\u4e60\u6559\u7a0b\u600e\u4e48\u53ef\u80fd\u5b8c\u6574\uff1f \u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[uvicorn]\" \u73b0\u5728\u8ba9\u6211\u4eec\u7528\u4ee3\u7801\u521b\u5efa\u4e00\u4e2a app.py \u6587\u4ef6\uff1a app.py 1 2 3 4 5 6 7 8 9 10 11 from transformers import pipeline from pinferencia import Server vision_classifier = pipeline ( task = \"image-classification\" ) def classify ( data ): return vision_classifier ( images = data ) service = Server () service . register ( model_name = \"vision\" , model = classify ) \u5bb9\u6613\uff0c\u5bf9\u5427\uff1f","title":"\u90e8\u7f72\u6a21\u578b"},{"location":"zh/ml/huggingface/pipeline/vision/#_4","text":"Curl Python requests curl --location --request POST 'http://127.0.0.1:8000/v1/models/vision/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"https://cdn.pixabay.com/photo/2018/08/12/16/59/parrot-3601194_1280.jpg\" }' \u7ed3\u679c: Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/vision/predict\" , json = { \"data\" : \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\" # noqa }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) \u8fd0\u884c python test.py \uff0c\u67e5\u770b\u7ed3\u679c\uff1a Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8000 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u4ea4\u4e92\u5f0f ui\u3002 \u60a8\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u9884\u6d4b"},{"location":"zh/ml/huggingface/pipeline/vision/#_5","text":"\u4f46\u662f\uff0c\u6709\u65f6\u4f7f\u7528\u56fe\u50cf\u7684 url \u6765\u9884\u6d4b\u662f\u4e0d\u5408\u9002\u7684\u3002 \u8ba9\u6211\u4eec\u7a0d\u5fae\u4fee\u6539 app.py \u4ee5\u63a5\u53d7 Base64 Encoded String \u4f5c\u4e3a\u8f93\u5165\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import base64 from io import BytesIO from PIL import Image from transformers import pipeline from pinferencia import Server vision_classifier = pipeline ( task = \"image-classification\" ) def classify ( image_base64_str ): image = Image . open ( BytesIO ( base64 . b64decode ( image_base64_str ))) return vision_classifier ( images = image ) service = Server () service . register ( model_name = \"vision\" , model = classify )","title":"\u8fdb\u4e00\u6b65\u6539\u8fdb"},{"location":"zh/ml/huggingface/pipeline/vision/#_6","text":"Curl Python requests curl --location --request POST 'http://127.0.0.1:8000/v1/models/vision/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"...\" }' \u7ed3\u679c\uff1a Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/vision/predict\" , json = { \"data\" : \"...\" # noqa }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) \u8fd0\u884c python test.py \u5e76\u67e5\u770b\u7ed3\u679c\uff1a Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ]","title":"\u518d\u6b21\u9884\u6d4b"},{"location":"zh/models/home/","text":"\u6a21\u578b? \u00b6 \u4ec0\u4e48\u662f \u6a21\u578b ? \u6982\u62ec\u7684\u6765\u8bf4\uff0c\u6a21\u578b\u662f\u4e00\u79cd\u8ba1\u7b97\u7684\u65b9\u6cd5\u3002\u901a\u5e38\u6765\u8bf4\uff0c\u6bd4\u4e00\u4e2a\u65b9\u7a0b\u590d\u6742\u4e00\u70b9\u3002 \u90a3\u6a21\u578b\u53ef\u4ee5\u662f\u4e00\u4e2a\u6587\u4ef6\u5417\uff1f\u53ef\u4ee5\u662f\u4e00\u4e2aPython\u5bf9\u8c61\u5417\uff1f\u5f53\u7136\u3002 \u5728 Pinferencia \uff0c \u4e00\u4e2a\u6a21\u578b\u5c31\u662f\u4e00\u4e2a\u53ef\u4ee5\u88ab\u8c03\u7528\u7684\u4ee3\u7801\uff0c\u5b83\u53ef\u4ee5\u662f\u4e00\u4e2a\u51fd\u6570\uff0c\u4e5f\u53ef\u4ee5\u662f\u4e00\u4e2a\u7c7b\u7684\u5b9e\u4f8b\u3002","title":"\u5173\u4e8e\u6a21\u578b"},{"location":"zh/models/home/#_1","text":"\u4ec0\u4e48\u662f \u6a21\u578b ? \u6982\u62ec\u7684\u6765\u8bf4\uff0c\u6a21\u578b\u662f\u4e00\u79cd\u8ba1\u7b97\u7684\u65b9\u6cd5\u3002\u901a\u5e38\u6765\u8bf4\uff0c\u6bd4\u4e00\u4e2a\u65b9\u7a0b\u590d\u6742\u4e00\u70b9\u3002 \u90a3\u6a21\u578b\u53ef\u4ee5\u662f\u4e00\u4e2a\u6587\u4ef6\u5417\uff1f\u53ef\u4ee5\u662f\u4e00\u4e2aPython\u5bf9\u8c61\u5417\uff1f\u5f53\u7136\u3002 \u5728 Pinferencia \uff0c \u4e00\u4e2a\u6a21\u578b\u5c31\u662f\u4e00\u4e2a\u53ef\u4ee5\u88ab\u8c03\u7528\u7684\u4ee3\u7801\uff0c\u5b83\u53ef\u4ee5\u662f\u4e00\u4e2a\u51fd\u6570\uff0c\u4e5f\u53ef\u4ee5\u662f\u4e00\u4e2a\u7c7b\u7684\u5b9e\u4f8b\u3002","title":"\u6a21\u578b?"},{"location":"zh/models/machine-learning/","text":"\u673a\u5668\u5b66\u4e60\u6846\u67b6 \u00b6 \u4e0b\u9762\u662f\u9488\u5bf9\u4e0d\u540c\u673a\u5668\u5b66\u4e60\u6846\u67b6\u7684\u5e38\u89c1\u6a21\u578b\u8f7d\u5165\u65b9\u6cd5\uff1a Scikit-Learn PyTorch Tensorflow \u4efb\u4f55\u6a21\u578b \u4efb\u610f\u51fd\u6570 app.py import joblib from pinferencia import Server # train your model model = \"...\" # or load your model model = joblib . load ( \"/path/to/model.joblib\" ) # (1) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , # (2) ) \u66f4\u591a\u8be6\u60c5\uff0c\u8bf7\u8bbf\u95ee https://scikit-learn.org/stable/modules/model_persistence.html entrypoint \u662f model \u6267\u884c\u9884\u6d4b\u7684\u51fd\u6570\u540d\u3002 \u8fd9\u91cc\u6570\u636e\u5c06\u88ab\u53d1\u9001\u5230 predict \u51fd\u6570\uff1a model.predict(data) \u3002 app.py import torch from pinferencia import Server # train your models model = \"...\" # or load your models (1) # from state_dict model = TheModelClass ( * args , ** kwargs ) model . load_state_dict ( torch . load ( PATH )) # entire model model = torch . load ( PATH ) # torchscript model = torch . jit . load ( 'model_scripted.pt' ) model . eval () service = Server () service . register ( model_name = \"mymodel\" , model = model , ) \u66f4\u591a\u8be6\u60c5\uff0c\u8bf7\u8bbf\u95ee https://pytorch.org/tutorials/beginner/saving_loading_models.html app.py import tensorflow as tf from pinferencia import Server # train your models model = \"...\" # or load your models (1) # saved_model model = tf . keras . models . load_model ( 'saved_model/model' ) # HDF5 model = tf . keras . models . load_model ( 'model.h5' ) # from weights model = create_model () model . load_weights ( './checkpoints/my_checkpoint' ) loss , acc = model . evaluate ( test_images , test_labels , verbose = 2 ) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) \u66f4\u591a\u8be6\u60c5\uff0c\u8bf7\u8bbf\u95ee https://www.tensorflow.org/tutorials/keras/save_and_load app.py from pinferencia import Server class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) app.py from pinferencia import Server def model ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = model , )","title":"\u5176\u5b83\u673a\u5668\u5b66\u4e60\u6846\u67b6"},{"location":"zh/models/machine-learning/#_1","text":"\u4e0b\u9762\u662f\u9488\u5bf9\u4e0d\u540c\u673a\u5668\u5b66\u4e60\u6846\u67b6\u7684\u5e38\u89c1\u6a21\u578b\u8f7d\u5165\u65b9\u6cd5\uff1a Scikit-Learn PyTorch Tensorflow \u4efb\u4f55\u6a21\u578b \u4efb\u610f\u51fd\u6570 app.py import joblib from pinferencia import Server # train your model model = \"...\" # or load your model model = joblib . load ( \"/path/to/model.joblib\" ) # (1) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , # (2) ) \u66f4\u591a\u8be6\u60c5\uff0c\u8bf7\u8bbf\u95ee https://scikit-learn.org/stable/modules/model_persistence.html entrypoint \u662f model \u6267\u884c\u9884\u6d4b\u7684\u51fd\u6570\u540d\u3002 \u8fd9\u91cc\u6570\u636e\u5c06\u88ab\u53d1\u9001\u5230 predict \u51fd\u6570\uff1a model.predict(data) \u3002 app.py import torch from pinferencia import Server # train your models model = \"...\" # or load your models (1) # from state_dict model = TheModelClass ( * args , ** kwargs ) model . load_state_dict ( torch . load ( PATH )) # entire model model = torch . load ( PATH ) # torchscript model = torch . jit . load ( 'model_scripted.pt' ) model . eval () service = Server () service . register ( model_name = \"mymodel\" , model = model , ) \u66f4\u591a\u8be6\u60c5\uff0c\u8bf7\u8bbf\u95ee https://pytorch.org/tutorials/beginner/saving_loading_models.html app.py import tensorflow as tf from pinferencia import Server # train your models model = \"...\" # or load your models (1) # saved_model model = tf . keras . models . load_model ( 'saved_model/model' ) # HDF5 model = tf . keras . models . load_model ( 'model.h5' ) # from weights model = create_model () model . load_weights ( './checkpoints/my_checkpoint' ) loss , acc = model . evaluate ( test_images , test_labels , verbose = 2 ) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) \u66f4\u591a\u8be6\u60c5\uff0c\u8bf7\u8bbf\u95ee https://www.tensorflow.org/tutorials/keras/save_and_load app.py from pinferencia import Server class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) app.py from pinferencia import Server def model ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = model , )","title":"\u673a\u5668\u5b66\u4e60\u6846\u67b6"},{"location":"zh/models/register/","text":"\u6ce8\u518c\u6a21\u578b \u00b6 \u6ce8\u518c\u4e00\u4e2a\u6a21\u578b\u975e\u5e38\u7b80\u5355: 1 2 3 4 5 service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) \u5982\u679c\u6211\u6709\u591a\u4e2a\u6a21\u578b\uff0c\u6216\u8005\u6709\u591a\u4e2a\u7248\u672c\u5462? \u4f60\u53ef\u4ee5\u6ce8\u518c\u591a\u4e2a\u6a21\u578b\uff0c\u6bcf\u4e2a\u6a21\u578b\u53ef\u4ee5\u6709\u4e0d\u540c\u7684\u7248\u672c: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 service . register ( model_name = \"my-model\" , model = my_model , entrypoint = \"predict\" , ) service . register ( model_name = \"my-model\" , model = my_model_v1 , entrypoint = \"predict\" , version_name = \"v1, ) service . register ( model_name = \"your-model\" , model = your_model , entrypoint = \"predict\" , ) service . register ( model_name = \"your-model\" , model = your_model_v1 , entrypoint = \"predict\" , version_name = \"v1, ) service . register ( model_name = \"your-model\" , model = your_model_v2 , entrypoint = \"predict\" , version_name = \"v2, ) \u53c2\u6570 \u00b6 \u53c2\u6570 \u7c7b\u4f3c \u9ed8\u8ba4\u503c\uff08\u5982\u6709\uff09 \u7ec6\u8282 model_name str \u6a21\u578b\u540d\u79f0 model object \u6a21\u578bPython\u5bf9\u8c61\uff0c\u6216\u8005\u6a21\u578b\u6587\u4ef6\u8def\u5f84 version_name str None \u7248\u672c\u540d\u79f0 entrypoint str None \u7528\u6765\u9884\u6d4b\u7684\u51fd\u6570\u540d\u79f0 metadata dict None \u6a21\u578b\u57fa\u7840\u4fe1\u606f handler object None Hanlder \u7c7b load_now bool True \u662f\u5426\u7acb\u523b\u8f7d\u5165\u6a21\u578b \u4f8b\u5b50 \u00b6 Model Name \u00b6 1 2 3 4 5 6 7 8 9 10 11 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , ) Model \u00b6 Model Object Function 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from pinferencia import Server class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict ) 1 2 3 4 5 6 7 8 9 10 11 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , ) Version\u540d\u79f0 \u00b6 \u6ca1\u6709\u63d0\u4f9b\u7248\u672c\u540d\u7684\u6a21\u578b\u4f1a\u7528 default \u7248\u672c\u540d. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pinferencia import Server def add ( data ): return data [ 0 ] + data [ 1 ] def substract ( data ): return data [ 0 ] + data [ 1 ] service = Server () service . register ( model_name = \"mymodel\" , model = add , version_name = \"add\" , # (1) ) service . register ( model_name = \"mymodel\" , model = substract , version_name = \"substract\" , # (2) ) \u9884\u6d4b\u5730\u5740\u5728 http://127.0.0.1/v1/models/mymodel/versions/add/predict \u9884\u6d4b\u5730\u5740\u5728 http://127.0.0.1/v1/models/mymodel/versions/substract/predict Entrypoint \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pinferencia import Server class MyModel : def add ( self , data ): return data [ 0 ] + data [ 1 ] def substract ( self , data ): return data [ 0 ] - data [ 1 ] model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , version_name = \"add\" , # (1) entrypoint = \"add\" , # (3) ) service . register ( model_name = \"mymodel\" , model = model , version_name = \"substract\" , # (2) entrypoint = \"substract\" , # (4) ) \u9884\u6d4b\u5730\u5740\u5728 http://127.0.0.1/v1/models/mymodel/versions/add/predict \u9884\u6d4b\u5730\u5740\u5728 http://127.0.0.1/v1/models/mymodel/versions/substract/predict add \u51fd\u6570\u4f1a\u88ab\u7528\u6765\u9884\u6d4b. substract \u51fd\u6570\u4f1a\u88ab\u7528\u6765\u9884\u6d4b. Metadata \u00b6 \u9ed8\u8ba4API \u00b6 Pinferencia \u9ed8\u8ba4metadata\u67b6\u6784\u652f\u6301 platform \u548c device \u8fd9\u4e9b\u4fe1\u606f\u4ec5\u4f9b\u5c55\u793a\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , metadata = { \"platform\" : \"Linux\" , \"device\" : \"CPU+GPU\" , } ) Kserve API \u00b6 Pinferencia \u540c\u65f6\u652f\u6301 Kserve API. \u5bf9\u4e8e Kserve V2, \u6a21\u578bmetadata\u652f\u6301: - platform - inputs - outputs inputs \u548c outputs \u4f1a\u51b3\u5b9a\u6a21\u578b\u6536\u5230\u7684\u6570\u636e\u548c\u8fd4\u56de\u7684\u6570\u636e\u7c7b\u578b. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server ( api = \"kserve\" ) # (1) service . register ( model_name = \"mymodel\" , model = predict , metadata = { \"platform\" : \"mac os\" , \"inputs\" : [ { \"name\" : \"integers\" , # (2) \"datatype\" : \"int64\" , \"shape\" : [ 1 ], \"data\" : [ 1 , 2 , 3 ], } ], \"outputs\" : [ { \"name\" : \"sum\" , \"datatype\" : \"int64\" , \"shape\" : - 1 , \"data\" : 6 }, # (3) { \"name\" : \"product\" , \"datatype\" : \"int64\" , \"shape\" : - 1 , \"data\" : 6 }, ], } ) \u5982\u679c\u8981\u4f7f\u7528 Kserve API \u9700\u8981\u5728\u5b9e\u4f8b\u5316\u670d\u52a1\u65f6\u8bbe\u7f6e api=\"kserve\"\u3002 \u5982\u679c\u8bf7\u6c42\u5305\u542b\u591a\u7ec4\u6570\u636e\uff0c\u53ea\u6709 intergers \u6570\u636e\u4f1a\u88ab\u4f20\u9012\u7ed9\u6a21\u578b\u3002 \u8f93\u51fa\u6570\u636e\u4f1a\u88ab\u8f6c\u6362\u4e3a int64 \u3002 datatype \u5b57\u6bb5\u4ec5\u652f\u6301 numpy \u6570\u636e\u7c7b\u578b. \u5982\u679c\u7c7b\u578b\u8f6c\u6362\u5931\u8d25\uff0c\u54cd\u5e94\u91cc\u4f1a\u591a\u51fa error \u5b57\u6bb5\u3002 Handler \u00b6 \u5173\u4e8eHandler\u7684\u7ec6\u8282\uff0c\u8bf7\u67e5\u770b Handlers . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pinferencia import Server from pinferencia.handlers import PickleHandler class MyPrintHandler ( PickleHandler ): def predict ( self , data ): print ( data ) return self . model . predict ( data ) def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , handler = MyPrintHandler ) Load Now \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import joblib from pinferencia import Server class JoblibHandler ( BaseHandler ): def load_model ( self ): return joblib . load ( self . model_path ) service = Server ( model_dir = \"/opt/models\" ) service . register ( model_name = \"mymodel\" , model = \"/path/to/model.joblib\" , entrypoint = \"predict\" , handler = JoblibHandler , load_now = True , )","title":"\u6ce8\u518c\u6a21\u578b"},{"location":"zh/models/register/#_1","text":"\u6ce8\u518c\u4e00\u4e2a\u6a21\u578b\u975e\u5e38\u7b80\u5355: 1 2 3 4 5 service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) \u5982\u679c\u6211\u6709\u591a\u4e2a\u6a21\u578b\uff0c\u6216\u8005\u6709\u591a\u4e2a\u7248\u672c\u5462? \u4f60\u53ef\u4ee5\u6ce8\u518c\u591a\u4e2a\u6a21\u578b\uff0c\u6bcf\u4e2a\u6a21\u578b\u53ef\u4ee5\u6709\u4e0d\u540c\u7684\u7248\u672c: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 service . register ( model_name = \"my-model\" , model = my_model , entrypoint = \"predict\" , ) service . register ( model_name = \"my-model\" , model = my_model_v1 , entrypoint = \"predict\" , version_name = \"v1, ) service . register ( model_name = \"your-model\" , model = your_model , entrypoint = \"predict\" , ) service . register ( model_name = \"your-model\" , model = your_model_v1 , entrypoint = \"predict\" , version_name = \"v1, ) service . register ( model_name = \"your-model\" , model = your_model_v2 , entrypoint = \"predict\" , version_name = \"v2, )","title":"\u6ce8\u518c\u6a21\u578b"},{"location":"zh/models/register/#_2","text":"\u53c2\u6570 \u7c7b\u4f3c \u9ed8\u8ba4\u503c\uff08\u5982\u6709\uff09 \u7ec6\u8282 model_name str \u6a21\u578b\u540d\u79f0 model object \u6a21\u578bPython\u5bf9\u8c61\uff0c\u6216\u8005\u6a21\u578b\u6587\u4ef6\u8def\u5f84 version_name str None \u7248\u672c\u540d\u79f0 entrypoint str None \u7528\u6765\u9884\u6d4b\u7684\u51fd\u6570\u540d\u79f0 metadata dict None \u6a21\u578b\u57fa\u7840\u4fe1\u606f handler object None Hanlder \u7c7b load_now bool True \u662f\u5426\u7acb\u523b\u8f7d\u5165\u6a21\u578b","title":"\u53c2\u6570"},{"location":"zh/models/register/#_3","text":"","title":"\u4f8b\u5b50"},{"location":"zh/models/register/#model-name","text":"1 2 3 4 5 6 7 8 9 10 11 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , )","title":"Model Name"},{"location":"zh/models/register/#model","text":"Model Object Function 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from pinferencia import Server class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict ) 1 2 3 4 5 6 7 8 9 10 11 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , )","title":"Model"},{"location":"zh/models/register/#version","text":"\u6ca1\u6709\u63d0\u4f9b\u7248\u672c\u540d\u7684\u6a21\u578b\u4f1a\u7528 default \u7248\u672c\u540d. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pinferencia import Server def add ( data ): return data [ 0 ] + data [ 1 ] def substract ( data ): return data [ 0 ] + data [ 1 ] service = Server () service . register ( model_name = \"mymodel\" , model = add , version_name = \"add\" , # (1) ) service . register ( model_name = \"mymodel\" , model = substract , version_name = \"substract\" , # (2) ) \u9884\u6d4b\u5730\u5740\u5728 http://127.0.0.1/v1/models/mymodel/versions/add/predict \u9884\u6d4b\u5730\u5740\u5728 http://127.0.0.1/v1/models/mymodel/versions/substract/predict","title":"Version\u540d\u79f0"},{"location":"zh/models/register/#entrypoint","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pinferencia import Server class MyModel : def add ( self , data ): return data [ 0 ] + data [ 1 ] def substract ( self , data ): return data [ 0 ] - data [ 1 ] model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , version_name = \"add\" , # (1) entrypoint = \"add\" , # (3) ) service . register ( model_name = \"mymodel\" , model = model , version_name = \"substract\" , # (2) entrypoint = \"substract\" , # (4) ) \u9884\u6d4b\u5730\u5740\u5728 http://127.0.0.1/v1/models/mymodel/versions/add/predict \u9884\u6d4b\u5730\u5740\u5728 http://127.0.0.1/v1/models/mymodel/versions/substract/predict add \u51fd\u6570\u4f1a\u88ab\u7528\u6765\u9884\u6d4b. substract \u51fd\u6570\u4f1a\u88ab\u7528\u6765\u9884\u6d4b.","title":"Entrypoint"},{"location":"zh/models/register/#metadata","text":"","title":"Metadata"},{"location":"zh/models/register/#api","text":"Pinferencia \u9ed8\u8ba4metadata\u67b6\u6784\u652f\u6301 platform \u548c device \u8fd9\u4e9b\u4fe1\u606f\u4ec5\u4f9b\u5c55\u793a\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , metadata = { \"platform\" : \"Linux\" , \"device\" : \"CPU+GPU\" , } )","title":"\u9ed8\u8ba4API"},{"location":"zh/models/register/#kserve-api","text":"Pinferencia \u540c\u65f6\u652f\u6301 Kserve API. \u5bf9\u4e8e Kserve V2, \u6a21\u578bmetadata\u652f\u6301: - platform - inputs - outputs inputs \u548c outputs \u4f1a\u51b3\u5b9a\u6a21\u578b\u6536\u5230\u7684\u6570\u636e\u548c\u8fd4\u56de\u7684\u6570\u636e\u7c7b\u578b. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server ( api = \"kserve\" ) # (1) service . register ( model_name = \"mymodel\" , model = predict , metadata = { \"platform\" : \"mac os\" , \"inputs\" : [ { \"name\" : \"integers\" , # (2) \"datatype\" : \"int64\" , \"shape\" : [ 1 ], \"data\" : [ 1 , 2 , 3 ], } ], \"outputs\" : [ { \"name\" : \"sum\" , \"datatype\" : \"int64\" , \"shape\" : - 1 , \"data\" : 6 }, # (3) { \"name\" : \"product\" , \"datatype\" : \"int64\" , \"shape\" : - 1 , \"data\" : 6 }, ], } ) \u5982\u679c\u8981\u4f7f\u7528 Kserve API \u9700\u8981\u5728\u5b9e\u4f8b\u5316\u670d\u52a1\u65f6\u8bbe\u7f6e api=\"kserve\"\u3002 \u5982\u679c\u8bf7\u6c42\u5305\u542b\u591a\u7ec4\u6570\u636e\uff0c\u53ea\u6709 intergers \u6570\u636e\u4f1a\u88ab\u4f20\u9012\u7ed9\u6a21\u578b\u3002 \u8f93\u51fa\u6570\u636e\u4f1a\u88ab\u8f6c\u6362\u4e3a int64 \u3002 datatype \u5b57\u6bb5\u4ec5\u652f\u6301 numpy \u6570\u636e\u7c7b\u578b. \u5982\u679c\u7c7b\u578b\u8f6c\u6362\u5931\u8d25\uff0c\u54cd\u5e94\u91cc\u4f1a\u591a\u51fa error \u5b57\u6bb5\u3002","title":"Kserve API"},{"location":"zh/models/register/#handler","text":"\u5173\u4e8eHandler\u7684\u7ec6\u8282\uff0c\u8bf7\u67e5\u770b Handlers . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pinferencia import Server from pinferencia.handlers import PickleHandler class MyPrintHandler ( PickleHandler ): def predict ( self , data ): print ( data ) return self . model . predict ( data ) def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , handler = MyPrintHandler )","title":"Handler"},{"location":"zh/models/register/#load-now","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import joblib from pinferencia import Server class JoblibHandler ( BaseHandler ): def load_model ( self ): return joblib . load ( self . model_path ) service = Server ( model_dir = \"/opt/models\" ) service . register ( model_name = \"mymodel\" , model = \"/path/to/model.joblib\" , entrypoint = \"predict\" , handler = JoblibHandler , load_now = True , )","title":"Load Now"},{"location":"zh/overview/","text":"\u6b22\u8fce\u4f7f\u7528Pinferencia \u00b6 Pinferencia? \u00b6 \u6ca1\u542c\u8bf4\u8fc7 Pinferencia \uff0c\u8fd9\u4e0d\u662f\u4f60\u7684\u9519\u3002\u4e3b\u8981\u6211\u7684\u5ba3\u4f20\u7ecf\u8d39\uff0c\u5b9e\u5728\u662f\u4e0d\u591f\u591a\u3002 \u4f60\u662f\u4e0d\u662f\u8bad\u7ec3\u4e86\u4e00\u5806\u6a21\u578b\uff0c\u7136\u800c\u522b\u4eba\u8c01\u7528\u90fd\u4e0d\u884c\u3002\u4e0d\u662f\u73af\u5883\u641e\u4e0d\u5b9a\uff0c\u5c31\u662fbug\u547d\u592a\u786c\u3002 \u4f60\u60f3: \u8981\u662f\u6211\u80fd\u6709\u4e2aAPI\uff0c\u8c01\u80fd\u4e0d\u9677\u5165\u6211\u7684\u7231\u3002\u4e0d\u7528\u5b89\u88c5\u4e0d\u7528\u7b49\u5f85\uff0c\u53d1\u4e2a\u8bf7\u6c42\u7ed3\u679c\u81ea\u5df1\u5230\u6765\u3002 \u53ef\u662f\u4e16\u4e0aAPI\u5343\u767e\u4e07\uff0c\u5374\u6ca1\u6709\u54ea\u4e2a\u6211\u80fd\u73a9\u5f97\u8f6c\u3002\u7528\u6765\u7528\u53bb\uff0c\u770b\u6765\u8fd8\u662f\u6211\u5fc3\u592a\u8f6f\uff0c\u6709\u4e9b\u4ea7\u54c1\u771f\u7684\u4e0d\u80fd\u60ef\u3002 \u6211\u591a\u60f3\u8fd9\u4e2a\u4e16\u754c\u53d8\u5f97\u7b80\u5355\uff0c\u6211\u7684\u6a21\u578b1\u5206\u949f\u5c31\u80fd\u4e0a\u7ebf\u3002\u7136\u800c\u73b0\u5b9e\u8fd9\u4e48\u6b8b\u9177\uff0c\u4e00\u5929\u4e24\u5929\u8fc7\u53bb\uff0c\u6211\u7684\u773c\u6cea\u54d7\u54d7\u6b62\u4e0d\u4f4f\u3002 \u5230\u5e95\u8c01\u80fd\u7ed9\u4e88\u6211\u8fd9\u4e2a\u6069\u8d50\u554a\uff0c\u770b\u6765\u53ea\u6709Pinferencia\u3002 \u8fd8\u5acc\u4e0d\u591f? \u66f4\u591a\u6b22\u4e50\uff0c\u8bf7\u524d\u5f80 \u6b63\u895f\u5371\u5750\u7248\u6587\u6863 \u5f00\u59cb\u5c1d\u9c9c! \u00b6 $ pip install \"pinferencia[uvicorn]\" ---> 100% \u521b\u5efa\u5e94\u7528\u7a0b\u5e8f \u00b6 Scikit-Learn PyTorch Tensorflow HuggingFace Transformer Any Model Any Function app.py import joblib import uvicorn from pinferencia import Server # train your model model = \"...\" # or load your model model = joblib . load ( \"/path/to/model.joblib\" ) # (1) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , # (2) ) For more details, please visit https://scikit-learn.org/stable/modules/model_persistence.html entrypoint is the function name of the model to perform predictions. Here the data will be sent to the predict function: model.predict(data) . app.py import torch import uvicorn from pinferencia import Server # train your models model = \"...\" # or load your models (1) # from state_dict model = TheModelClass ( * args , ** kwargs ) model . load_state_dict ( torch . load ( PATH )) # entire model model = torch . load ( PATH ) # torchscript model = torch . jit . load ( 'model_scripted.pt' ) model . eval () service = Server () service . register ( model_name = \"mymodel\" , model = model , ) For more details, please visit https://pytorch.org/tutorials/beginner/saving_loading_models.html app.py import tensorflow as tf import uvicorn from pinferencia import Server # train your models model = \"...\" # or load your models (1) # saved_model model = tf . keras . models . load_model ( 'saved_model/model' ) # HDF5 model = tf . keras . models . load_model ( 'model.h5' ) # from weights model = create_model () model . load_weights ( './checkpoints/my_checkpoint' ) loss , acc = model . evaluate ( test_images , test_labels , verbose = 2 ) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) For more details, please visit https://www.tensorflow.org/tutorials/keras/save_and_load app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from transformers import pipeline from pinferencia import Server vision_classifier = pipeline ( task = \"image-classification\" ) def predict ( data ): return vision_classifier ( images = data ) service = Server () service . register ( model_name = \"vision\" , model = predict ) app.py import uvicorn from pinferencia import Server # train your models class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) app.py import uvicorn from pinferencia import Server # train your models def model ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = model , ) \u8fd0\u884c\u670d\u52a1! \u00b6 $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)","title":"\u6982\u8ff0"},{"location":"zh/overview/#pinferencia","text":"","title":"\u6b22\u8fce\u4f7f\u7528Pinferencia"},{"location":"zh/overview/#pinferencia_1","text":"\u6ca1\u542c\u8bf4\u8fc7 Pinferencia \uff0c\u8fd9\u4e0d\u662f\u4f60\u7684\u9519\u3002\u4e3b\u8981\u6211\u7684\u5ba3\u4f20\u7ecf\u8d39\uff0c\u5b9e\u5728\u662f\u4e0d\u591f\u591a\u3002 \u4f60\u662f\u4e0d\u662f\u8bad\u7ec3\u4e86\u4e00\u5806\u6a21\u578b\uff0c\u7136\u800c\u522b\u4eba\u8c01\u7528\u90fd\u4e0d\u884c\u3002\u4e0d\u662f\u73af\u5883\u641e\u4e0d\u5b9a\uff0c\u5c31\u662fbug\u547d\u592a\u786c\u3002 \u4f60\u60f3: \u8981\u662f\u6211\u80fd\u6709\u4e2aAPI\uff0c\u8c01\u80fd\u4e0d\u9677\u5165\u6211\u7684\u7231\u3002\u4e0d\u7528\u5b89\u88c5\u4e0d\u7528\u7b49\u5f85\uff0c\u53d1\u4e2a\u8bf7\u6c42\u7ed3\u679c\u81ea\u5df1\u5230\u6765\u3002 \u53ef\u662f\u4e16\u4e0aAPI\u5343\u767e\u4e07\uff0c\u5374\u6ca1\u6709\u54ea\u4e2a\u6211\u80fd\u73a9\u5f97\u8f6c\u3002\u7528\u6765\u7528\u53bb\uff0c\u770b\u6765\u8fd8\u662f\u6211\u5fc3\u592a\u8f6f\uff0c\u6709\u4e9b\u4ea7\u54c1\u771f\u7684\u4e0d\u80fd\u60ef\u3002 \u6211\u591a\u60f3\u8fd9\u4e2a\u4e16\u754c\u53d8\u5f97\u7b80\u5355\uff0c\u6211\u7684\u6a21\u578b1\u5206\u949f\u5c31\u80fd\u4e0a\u7ebf\u3002\u7136\u800c\u73b0\u5b9e\u8fd9\u4e48\u6b8b\u9177\uff0c\u4e00\u5929\u4e24\u5929\u8fc7\u53bb\uff0c\u6211\u7684\u773c\u6cea\u54d7\u54d7\u6b62\u4e0d\u4f4f\u3002 \u5230\u5e95\u8c01\u80fd\u7ed9\u4e88\u6211\u8fd9\u4e2a\u6069\u8d50\u554a\uff0c\u770b\u6765\u53ea\u6709Pinferencia\u3002 \u8fd8\u5acc\u4e0d\u591f? \u66f4\u591a\u6b22\u4e50\uff0c\u8bf7\u524d\u5f80 \u6b63\u895f\u5371\u5750\u7248\u6587\u6863","title":"Pinferencia?"},{"location":"zh/overview/#_1","text":"$ pip install \"pinferencia[uvicorn]\" ---> 100%","title":"\u5f00\u59cb\u5c1d\u9c9c!"},{"location":"zh/overview/#_2","text":"Scikit-Learn PyTorch Tensorflow HuggingFace Transformer Any Model Any Function app.py import joblib import uvicorn from pinferencia import Server # train your model model = \"...\" # or load your model model = joblib . load ( \"/path/to/model.joblib\" ) # (1) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , # (2) ) For more details, please visit https://scikit-learn.org/stable/modules/model_persistence.html entrypoint is the function name of the model to perform predictions. Here the data will be sent to the predict function: model.predict(data) . app.py import torch import uvicorn from pinferencia import Server # train your models model = \"...\" # or load your models (1) # from state_dict model = TheModelClass ( * args , ** kwargs ) model . load_state_dict ( torch . load ( PATH )) # entire model model = torch . load ( PATH ) # torchscript model = torch . jit . load ( 'model_scripted.pt' ) model . eval () service = Server () service . register ( model_name = \"mymodel\" , model = model , ) For more details, please visit https://pytorch.org/tutorials/beginner/saving_loading_models.html app.py import tensorflow as tf import uvicorn from pinferencia import Server # train your models model = \"...\" # or load your models (1) # saved_model model = tf . keras . models . load_model ( 'saved_model/model' ) # HDF5 model = tf . keras . models . load_model ( 'model.h5' ) # from weights model = create_model () model . load_weights ( './checkpoints/my_checkpoint' ) loss , acc = model . evaluate ( test_images , test_labels , verbose = 2 ) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) For more details, please visit https://www.tensorflow.org/tutorials/keras/save_and_load app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from transformers import pipeline from pinferencia import Server vision_classifier = pipeline ( task = \"image-classification\" ) def predict ( data ): return vision_classifier ( images = data ) service = Server () service . register ( model_name = \"vision\" , model = predict ) app.py import uvicorn from pinferencia import Server # train your models class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) app.py import uvicorn from pinferencia import Server # train your models def model ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = model , )","title":"\u521b\u5efa\u5e94\u7528\u7a0b\u5e8f"},{"location":"zh/overview/#_3","text":"$ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)","title":"\u8fd0\u884c\u670d\u52a1!"},{"location":"zh/pinferencia-is-different/","text":"Pinferencia \u6709\u4ec0\u4e48\u4e0d\u540c? \u00b6 \u4e0d\u540c? \u00b6 \u66f4\u51c6\u786e\u7684\u8bf4\uff0c\u4e0d\u540c\u5e76\u4e0d\u91cd\u8981\uff0c\u91cd\u8981\u7684\u662f\u66f4\u76f4\u63a5\uff0c\u66f4\u7b80\u5355\uff0c\u66f4\u7b26\u5408\u4f60\u7684\u9884\u671f\u3002 \u4f60\u73b0\u5728\u662f\u5982\u4f55\u4e0a\u7ebf\u6a21\u578b\u7684? \u4f60\u662f\u4e0d\u662f\u82b1\u4e86\u5f88\u591a\u65f6\u95f4\uff0c\u5199\u4ee3\u7801\uff0c\u4fdd\u5b58\u6587\u4ef6\uff0c\u4e3a\u4e86\u6ee1\u8db3\u90a3\u4e9b\u90e8\u7f72\u5de5\u5177\u7684\u8981\u6c42\u3002 \u5bf9\uff0c\u4f60\u8fd8\u82b1\u4e86\u5f88\u591a\u65f6\u95f4\u53bb\u7406\u89e3\u8fd9\u4e9b\u8981\u6c42\uff0c\u5f88\u591a\u65f6\u95f4\u77e5\u9053\u600e\u4e48\u505a\u662f\u6b63\u786e\u7684\u3002 \u4e0d\u8fc7\uff0c\u529f\u592b\u4e0d\u8d1f\u6709\u5fc3\u4eba\uff0c\u4f60\u8fd8\u662f\u641e\u5b9a\u4e86\u3002 \u597d\u666f\u4e0d\u957f\uff0c\u8fc7\u4e86\u5927\u534a\u5e74\uff0c\u53c8\u6709\u65b0\u7684\uff0c\u66f4\u590d\u6742\u7684\u6a21\u578b\u8981\u90e8\u7f72\uff0c\u5929\u554a\uff0c\u600e\u4e48\u529e\uff1f \u4f60\u73b0\u5728\u5728\u60f3\u4ec0\u4e48? \u4e0d\u8981\u554a\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01 \u6a21\u578b\u5728\u4f60\u624b\u91cc\uff0c\u4f60\u7528 Python \u8bad\u7ec3\uff0c\u7528 Python \u9884\u6d4b\uff0c\u751a\u81f3\u5199\u4e86\u5f88\u591a\u590d\u6742\u7684\u4ee3\u7801\uff0c\u53bb\u89e3\u51b3\u56f0\u96be\u53c8\u72ec\u7279\u7684\u9700\u6c42\u3002 \u800c\u5982\u4eca\uff0c\u4f60\u53c8\u8981\u591a\u5199\u591a\u5c11\u4ee3\u7801\uff0c\u591a\u505a\u591a\u5c11\u6539\u53d8\uff0c\u624d\u80fd\u8ba9\u4f60\u7684\u6a21\u578b\uff0c\u7528\u8fd9\u4e9b\u5de5\u5177\u6216\u8005\u5e73\u53f0\uff0c\u4ec5\u4ec5\u662f\u542f\u52a8\u4e00\u4e2aAPI\uff1f \u7b54\u6848\u662f\uff1a \u6570\u4e0d\u80dc\u6570 . \u6709\u4e86 Pinferencia \u00b6 \u4f60\u4e0d\u7528\u518d\u62c5\u5fc3\u8fd9\u4e9b\u3002\u4f60\u53ea\u9700\u8981\u8fd8\u662f\u7528\u4f60\u81ea\u5df1\u7684\u6a21\u578b\uff0c\u8fd8\u662f\u7528\u4f60\u81ea\u5df1\u7684\u4ee3\u7801\u3002 \u65e0\u6240\u8c13\u4f60\u7684\u6a21\u578b\u662f: PyTorch \u6a21\u578b Tensorflow \u6a21\u578b \u4efb\u4f55\u673a\u5668\u5b66\u4e60\u6a21\u578b \u4f60\u81ea\u5df1\u7684\u4ee3\u7801\uff0c\u7b97\u6cd5 \u751a\u81f3\u53ea\u662f\u4e00\u4e2a\u7b80\u7b80\u5355\u5355\u7684\u51fd\u6570 \u53ea\u9700\u8981\u7b80\u5355\u7684\u6ce8\u518c\uff0c Pinferencia \u5c31\u662f\u7acb\u523b\u4e0a\u7ebf\u5b83\u6765\u9884\u6d4b\uff0c\u5982\u4f60\u9884\u671f\uff0c\u6ca1\u6709\u60ca\u5413\u3002 \u7b80\u5355\uff0c\u4e14\u5f3a\u5927 \u00b6 Pinferencia \u81f4\u529b\u4e8e\u6210\u4e3a\u6700\u7b80\u5355\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u90e8\u7f72\u5de5\u5177\u3002 \u90e8\u7f72\u6a21\u578b\u4ece\u6765\u6ca1\u6709\u5982\u6b64\u7b80\u5355\u3002 \u5982\u679c\u4f60\u60f3\uff1a \u627e\u5230\u4e00\u4e2a\u7b80\u5355\u4e14\u53ef\u9760\u7684\u65b9\u6cd5\u6765\u4e0a\u7ebf\u4f60\u7684\u6a21\u578b \u7528\u6700\u5c11\u7684\u4ee3\u7801\uff0c\u638c\u63a7\u4f60\u7684\u670d\u52a1 \u8131\u79bb\u90a3\u4e9b\u91cd\u91cf\u7ea7\u3001\u800c\u5f88\u591a\u529f\u80fd\u4f60\u6839\u672c\u4e0d\u5728\u4e4e\u7684\u5de5\u5177\u548c\u5e73\u53f0 \u90a3\u4e48\uff0c\u4f60\u6765\u5bf9\u5730\u65b9\u4e86","title":"Pinferencia\u6709\u4f55\u4e0d\u540c?"},{"location":"zh/pinferencia-is-different/#pinferencia","text":"","title":"Pinferencia \u6709\u4ec0\u4e48\u4e0d\u540c?"},{"location":"zh/pinferencia-is-different/#_1","text":"\u66f4\u51c6\u786e\u7684\u8bf4\uff0c\u4e0d\u540c\u5e76\u4e0d\u91cd\u8981\uff0c\u91cd\u8981\u7684\u662f\u66f4\u76f4\u63a5\uff0c\u66f4\u7b80\u5355\uff0c\u66f4\u7b26\u5408\u4f60\u7684\u9884\u671f\u3002 \u4f60\u73b0\u5728\u662f\u5982\u4f55\u4e0a\u7ebf\u6a21\u578b\u7684? \u4f60\u662f\u4e0d\u662f\u82b1\u4e86\u5f88\u591a\u65f6\u95f4\uff0c\u5199\u4ee3\u7801\uff0c\u4fdd\u5b58\u6587\u4ef6\uff0c\u4e3a\u4e86\u6ee1\u8db3\u90a3\u4e9b\u90e8\u7f72\u5de5\u5177\u7684\u8981\u6c42\u3002 \u5bf9\uff0c\u4f60\u8fd8\u82b1\u4e86\u5f88\u591a\u65f6\u95f4\u53bb\u7406\u89e3\u8fd9\u4e9b\u8981\u6c42\uff0c\u5f88\u591a\u65f6\u95f4\u77e5\u9053\u600e\u4e48\u505a\u662f\u6b63\u786e\u7684\u3002 \u4e0d\u8fc7\uff0c\u529f\u592b\u4e0d\u8d1f\u6709\u5fc3\u4eba\uff0c\u4f60\u8fd8\u662f\u641e\u5b9a\u4e86\u3002 \u597d\u666f\u4e0d\u957f\uff0c\u8fc7\u4e86\u5927\u534a\u5e74\uff0c\u53c8\u6709\u65b0\u7684\uff0c\u66f4\u590d\u6742\u7684\u6a21\u578b\u8981\u90e8\u7f72\uff0c\u5929\u554a\uff0c\u600e\u4e48\u529e\uff1f \u4f60\u73b0\u5728\u5728\u60f3\u4ec0\u4e48?","title":"\u4e0d\u540c?"},{"location":"zh/pinferencia-is-different/#pinferencia_1","text":"\u4f60\u4e0d\u7528\u518d\u62c5\u5fc3\u8fd9\u4e9b\u3002\u4f60\u53ea\u9700\u8981\u8fd8\u662f\u7528\u4f60\u81ea\u5df1\u7684\u6a21\u578b\uff0c\u8fd8\u662f\u7528\u4f60\u81ea\u5df1\u7684\u4ee3\u7801\u3002 \u65e0\u6240\u8c13\u4f60\u7684\u6a21\u578b\u662f: PyTorch \u6a21\u578b Tensorflow \u6a21\u578b \u4efb\u4f55\u673a\u5668\u5b66\u4e60\u6a21\u578b \u4f60\u81ea\u5df1\u7684\u4ee3\u7801\uff0c\u7b97\u6cd5 \u751a\u81f3\u53ea\u662f\u4e00\u4e2a\u7b80\u7b80\u5355\u5355\u7684\u51fd\u6570 \u53ea\u9700\u8981\u7b80\u5355\u7684\u6ce8\u518c\uff0c Pinferencia \u5c31\u662f\u7acb\u523b\u4e0a\u7ebf\u5b83\u6765\u9884\u6d4b\uff0c\u5982\u4f60\u9884\u671f\uff0c\u6ca1\u6709\u60ca\u5413\u3002","title":"\u6709\u4e86 Pinferencia"},{"location":"zh/pinferencia-is-different/#_2","text":"Pinferencia \u81f4\u529b\u4e8e\u6210\u4e3a\u6700\u7b80\u5355\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u90e8\u7f72\u5de5\u5177\u3002 \u90e8\u7f72\u6a21\u578b\u4ece\u6765\u6ca1\u6709\u5982\u6b64\u7b80\u5355\u3002 \u5982\u679c\u4f60\u60f3\uff1a \u627e\u5230\u4e00\u4e2a\u7b80\u5355\u4e14\u53ef\u9760\u7684\u65b9\u6cd5\u6765\u4e0a\u7ebf\u4f60\u7684\u6a21\u578b \u7528\u6700\u5c11\u7684\u4ee3\u7801\uff0c\u638c\u63a7\u4f60\u7684\u670d\u52a1 \u8131\u79bb\u90a3\u4e9b\u91cd\u91cf\u7ea7\u3001\u800c\u5f88\u591a\u529f\u80fd\u4f60\u6839\u672c\u4e0d\u5728\u4e4e\u7684\u5de5\u5177\u548c\u5e73\u53f0 \u90a3\u4e48\uff0c\u4f60\u6765\u5bf9\u5730\u65b9\u4e86","title":"\u7b80\u5355\uff0c\u4e14\u5f3a\u5927"},{"location":"zh/restapi/","text":"REST API \u00b6 \u6982\u8ff0 \u00b6 Pinferencia \u6709\u4e24\u4e2a\u5185\u7f6e API\uff1a \u9ed8\u8ba4 API Kserve API from pinferencia import Server service = Server () # or service = Server ( api = \"default\" ) from pinferencia import Server service = Server ( api = \"kserve\" ) \u60a8\u73b0\u5728\u6b63\u5728\u4f7f\u7528\u5176\u4ed6\u6a21\u578b\u670d\u52a1\u5de5\u5177\u5417\uff1f \u5982\u679c\u60a8\u8fd8\u4f7f\u7528\u5176\u4ed6\u6a21\u578b\u670d\u52a1\u5de5\u5177\uff0c\u4ee5\u4e0b\u662f\u8fd9\u4e9b\u5de5\u5177\u652f\u6301\u7684 Kserve API \u7248\u672c\uff1a \u540d\u79f0 API Pinferencia Kserve V1 & V2 TF Serving Kserve V1 TorchServe Kserve V1 or V2 Triton Kserve V2 KServe Kserve V1 \u6ca1\u6709\u75db\u82e6\uff0c\u53ea\u6709\u6536\u83b7 \u00b6 \u5982\u4f60\u770b\u5230\u7684 \u60a8\u53ef\u4ee5\u5728 Pinferencia \u548c\u5176\u4ed6\u5de5\u5177\u4e4b\u95f4\u5207\u6362\uff0c\u51e0\u4e4e\u65e0\u9700\u5728\u5ba2\u6237\u7aef\u66f4\u6539\u4ee3\u7801\u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528 Pinferencia \u8fdb\u884c\u539f\u578b\u8bbe\u8ba1\u548c\u5ba2\u6237\u7aef\u6784\u5efa\uff0c\u7136\u540e\u5728\u751f\u4ea7\u4e2d\u4f7f\u7528\u5176\u4ed6\u5de5\u5177\u3002 \u60a8\u53ef\u4ee5\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u5c06 Pinferencia \u4e0e\u5177\u6709\u76f8\u540c API \u96c6\u7684\u5176\u4ed6\u5de5\u5177\u4e00\u8d77\u4f7f\u7528\u3002 \u5982\u679c\u60a8\u8981\u4ece Kserve V1 \u5207\u6362\u5230 Kserve V2\uff0c\u5e76\u4e14\u5728\u8fc7\u6e21\u671f\u95f4\u9700\u8981\u652f\u6301\u8fd9\u4e24\u8005\u7684\u670d\u52a1\u5668\uff0c\u90a3\u4e48\u60a8\u5c31\u53ef\u4ee5\u4f7f\u7528 Pinferencia \u3002 \u6240\u4ee5\uff0c\u6ca1\u6709\u75db\u82e6\uff0c\u53ea\u6709\u6536\u83b7\u3002 \u9ed8\u8ba4 API \u00b6 Path Method Summary /v1/healthz GET \u670d\u52a1\u5065\u5eb7 /v1/models GET \u6a21\u578b\u5217\u8868 /v1/models/{model_name} GET \u6a21\u578b\u7248\u672c\u5217\u8868 /v1/models/{model_name}/ready GET \u6a21\u578b\u662f\u5426\u53ef\u7528 /v1/models/{model_name}/versions/{version_name}/ready GET \u6a21\u578b\u7248\u672c\u662f\u5426\u53ef\u7528 /v1/models/{model_name}/load POST \u52a0\u8f7d\u6a21\u578b /v1/models/{model_name}/versions/{version_name}/load POST \u52a0\u8f7d\u7248\u672c /v1/models/{model_name}/unload POST \u5378\u8f7d\u6a21\u578b /v1/models/{model_name}/versions/{version_name}/unload POST \u5378\u8f7d\u7248\u672c /v1/models/{model_name}/predict POST \u6a21\u578b\u9884\u6d4b /v1/models/{model_name}/versions/{version_name}/predict POST \u6a21\u578b\u7248\u672c\u9884\u6d4b Kserve API \u00b6 Path Method Summary /v1/healthz GET \u670d\u52a1\u5065\u5eb7 /v1/models GET \u6a21\u578b\u5217\u8868 /v1/models/{model_name} GET \u6a21\u578b\u7248\u672c\u5217\u8868 /v1/models/{model_name}/ready GET \u6a21\u578b\u662f\u5426\u53ef\u7528 /v1/models/{model_name}/versions/{version_name}/ready GET \u6a21\u578b\u7248\u672c\u662f\u5426\u53ef\u7528 /v1/models/{model_name}/load POST \u52a0\u8f7d\u6a21\u578b /v1/models/{model_name}/versions/{version_name}/load POST \u52a0\u8f7d\u7248\u672c /v1/models/{model_name}/unload POST \u5378\u8f7d\u6a21\u578b /v1/models/{model_name}/versions/{version_name}/unload POST \u5378\u8f7d\u7248\u672c /v1/models/{model_name}/infer POST \u6a21\u578b\u9884\u6d4b /v1/models/{model_name}/versions/{version_name}/infer POST \u6a21\u578b\u7248\u672c\u9884\u6d4b /v2/healthz GET \u670d\u52a1\u5065\u5eb7 /v2/models GET \u6a21\u578b\u5217\u8868 /v2/models/{model_name} GET \u6a21\u578b\u7248\u672c\u5217\u8868 /v2/models/{model_name}/ready GET \u6a21\u578b\u662f\u5426\u53ef\u7528 /v2/models/{model_name}/versions/{version_name}/ready GET \u6a21\u578b\u7248\u672c\u662f\u5426\u53ef\u7528 /v2/models/{model_name}/load POST \u52a0\u8f7d\u6a21\u578b /v2/models/{model_name}/versions/{version_name}/load POST \u52a0\u8f7d\u7248\u672c /v2/models/{model_name}/unload POST \u5378\u8f7d\u6a21\u578b /v2/models/{model_name}/versions/{version_name}/unload POST \u5378\u8f7d\u7248\u672c /v2/models/{model_name}/infer POST \u6a21\u578b\u9884\u6d4b /v2/models/{model_name}/versions/{version_name}/infer POST \u6a21\u578b\u7248\u672c\u9884\u6d4b","title":"REST API"},{"location":"zh/restapi/#rest-api","text":"","title":"REST API"},{"location":"zh/restapi/#_1","text":"Pinferencia \u6709\u4e24\u4e2a\u5185\u7f6e API\uff1a \u9ed8\u8ba4 API Kserve API from pinferencia import Server service = Server () # or service = Server ( api = \"default\" ) from pinferencia import Server service = Server ( api = \"kserve\" ) \u60a8\u73b0\u5728\u6b63\u5728\u4f7f\u7528\u5176\u4ed6\u6a21\u578b\u670d\u52a1\u5de5\u5177\u5417\uff1f \u5982\u679c\u60a8\u8fd8\u4f7f\u7528\u5176\u4ed6\u6a21\u578b\u670d\u52a1\u5de5\u5177\uff0c\u4ee5\u4e0b\u662f\u8fd9\u4e9b\u5de5\u5177\u652f\u6301\u7684 Kserve API \u7248\u672c\uff1a \u540d\u79f0 API Pinferencia Kserve V1 & V2 TF Serving Kserve V1 TorchServe Kserve V1 or V2 Triton Kserve V2 KServe Kserve V1","title":"\u6982\u8ff0"},{"location":"zh/restapi/#_2","text":"\u5982\u4f60\u770b\u5230\u7684 \u60a8\u53ef\u4ee5\u5728 Pinferencia \u548c\u5176\u4ed6\u5de5\u5177\u4e4b\u95f4\u5207\u6362\uff0c\u51e0\u4e4e\u65e0\u9700\u5728\u5ba2\u6237\u7aef\u66f4\u6539\u4ee3\u7801\u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528 Pinferencia \u8fdb\u884c\u539f\u578b\u8bbe\u8ba1\u548c\u5ba2\u6237\u7aef\u6784\u5efa\uff0c\u7136\u540e\u5728\u751f\u4ea7\u4e2d\u4f7f\u7528\u5176\u4ed6\u5de5\u5177\u3002 \u60a8\u53ef\u4ee5\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u5c06 Pinferencia \u4e0e\u5177\u6709\u76f8\u540c API \u96c6\u7684\u5176\u4ed6\u5de5\u5177\u4e00\u8d77\u4f7f\u7528\u3002 \u5982\u679c\u60a8\u8981\u4ece Kserve V1 \u5207\u6362\u5230 Kserve V2\uff0c\u5e76\u4e14\u5728\u8fc7\u6e21\u671f\u95f4\u9700\u8981\u652f\u6301\u8fd9\u4e24\u8005\u7684\u670d\u52a1\u5668\uff0c\u90a3\u4e48\u60a8\u5c31\u53ef\u4ee5\u4f7f\u7528 Pinferencia \u3002 \u6240\u4ee5\uff0c\u6ca1\u6709\u75db\u82e6\uff0c\u53ea\u6709\u6536\u83b7\u3002","title":"\u6ca1\u6709\u75db\u82e6\uff0c\u53ea\u6709\u6536\u83b7"},{"location":"zh/restapi/#api","text":"Path Method Summary /v1/healthz GET \u670d\u52a1\u5065\u5eb7 /v1/models GET \u6a21\u578b\u5217\u8868 /v1/models/{model_name} GET \u6a21\u578b\u7248\u672c\u5217\u8868 /v1/models/{model_name}/ready GET \u6a21\u578b\u662f\u5426\u53ef\u7528 /v1/models/{model_name}/versions/{version_name}/ready GET \u6a21\u578b\u7248\u672c\u662f\u5426\u53ef\u7528 /v1/models/{model_name}/load POST \u52a0\u8f7d\u6a21\u578b /v1/models/{model_name}/versions/{version_name}/load POST \u52a0\u8f7d\u7248\u672c /v1/models/{model_name}/unload POST \u5378\u8f7d\u6a21\u578b /v1/models/{model_name}/versions/{version_name}/unload POST \u5378\u8f7d\u7248\u672c /v1/models/{model_name}/predict POST \u6a21\u578b\u9884\u6d4b /v1/models/{model_name}/versions/{version_name}/predict POST \u6a21\u578b\u7248\u672c\u9884\u6d4b","title":"\u9ed8\u8ba4 API"},{"location":"zh/restapi/#kserve-api","text":"Path Method Summary /v1/healthz GET \u670d\u52a1\u5065\u5eb7 /v1/models GET \u6a21\u578b\u5217\u8868 /v1/models/{model_name} GET \u6a21\u578b\u7248\u672c\u5217\u8868 /v1/models/{model_name}/ready GET \u6a21\u578b\u662f\u5426\u53ef\u7528 /v1/models/{model_name}/versions/{version_name}/ready GET \u6a21\u578b\u7248\u672c\u662f\u5426\u53ef\u7528 /v1/models/{model_name}/load POST \u52a0\u8f7d\u6a21\u578b /v1/models/{model_name}/versions/{version_name}/load POST \u52a0\u8f7d\u7248\u672c /v1/models/{model_name}/unload POST \u5378\u8f7d\u6a21\u578b /v1/models/{model_name}/versions/{version_name}/unload POST \u5378\u8f7d\u7248\u672c /v1/models/{model_name}/infer POST \u6a21\u578b\u9884\u6d4b /v1/models/{model_name}/versions/{version_name}/infer POST \u6a21\u578b\u7248\u672c\u9884\u6d4b /v2/healthz GET \u670d\u52a1\u5065\u5eb7 /v2/models GET \u6a21\u578b\u5217\u8868 /v2/models/{model_name} GET \u6a21\u578b\u7248\u672c\u5217\u8868 /v2/models/{model_name}/ready GET \u6a21\u578b\u662f\u5426\u53ef\u7528 /v2/models/{model_name}/versions/{version_name}/ready GET \u6a21\u578b\u7248\u672c\u662f\u5426\u53ef\u7528 /v2/models/{model_name}/load POST \u52a0\u8f7d\u6a21\u578b /v2/models/{model_name}/versions/{version_name}/load POST \u52a0\u8f7d\u7248\u672c /v2/models/{model_name}/unload POST \u5378\u8f7d\u6a21\u578b /v2/models/{model_name}/versions/{version_name}/unload POST \u5378\u8f7d\u7248\u672c /v2/models/{model_name}/infer POST \u6a21\u578b\u9884\u6d4b /v2/models/{model_name}/versions/{version_name}/infer POST \u6a21\u578b\u7248\u672c\u9884\u6d4b","title":"Kserve API"},{"location":"rc/","text":"","title":"\u9996\u9875"},{"location":"rc/frontend/requirements/","text":"Requirements \u00b6 To use Pinferencia's frontend with your model, there are some requirements of your model's predict function. Templates \u00b6 Currently, there are mainly two major catogory of the template's inputs and outputs. More (audio and video) will be supported in the future. Base Templates \u00b6 Template Input Output Text to Text Text Text Text to Image Text Image Image to Text Image Text Camera to Text Image Text Image to Image Image Image Camera to Image Image Image Derived Templates \u00b6 Template Input Output Translation Text Text Image Classification Image Text Image Style Transfer Image Image Input \u00b6 The predict function must be able to accept a list of data as inputs. For text input, the input will be a list of strings. For image input, the input will be a list of strings representing the base64 encoded images. Output \u00b6 The predict function must produce a list of data as outputs. For text output, the output must be a list. For image output, the output must be a list of strings representing the base64 encoded images. Text Output The frontend will try to parse the outputs into table, json or pure text. Table Text JSON If the output is similar to below: [ [ { \"a\" : 1 , \"b\" : 2 }, { \"a\" : 3 , \"b\" : 4 }, { \"a\" : 5 , \"b\" : 6 } ] ] It will be displayed as a table. If the output is similar to below: [ \"Text output.\" ] It will be displayed as a text. All other format of outputs will be displayed as a JSON.","title":"\u524d\u7aef\u4f7f\u7528\u6761\u4ef6"},{"location":"rc/frontend/requirements/#requirements","text":"To use Pinferencia's frontend with your model, there are some requirements of your model's predict function.","title":"Requirements"},{"location":"rc/frontend/requirements/#templates","text":"Currently, there are mainly two major catogory of the template's inputs and outputs. More (audio and video) will be supported in the future.","title":"Templates"},{"location":"rc/frontend/requirements/#base-templates","text":"Template Input Output Text to Text Text Text Text to Image Text Image Image to Text Image Text Camera to Text Image Text Image to Image Image Image Camera to Image Image Image","title":"Base Templates"},{"location":"rc/frontend/requirements/#derived-templates","text":"Template Input Output Translation Text Text Image Classification Image Text Image Style Transfer Image Image","title":"Derived Templates"},{"location":"rc/frontend/requirements/#input","text":"The predict function must be able to accept a list of data as inputs. For text input, the input will be a list of strings. For image input, the input will be a list of strings representing the base64 encoded images.","title":"Input"},{"location":"rc/frontend/requirements/#output","text":"The predict function must produce a list of data as outputs. For text output, the output must be a list. For image output, the output must be a list of strings representing the base64 encoded images. Text Output The frontend will try to parse the outputs into table, json or pure text. Table Text JSON If the output is similar to below: [ [ { \"a\" : 1 , \"b\" : 2 }, { \"a\" : 3 , \"b\" : 4 }, { \"a\" : 5 , \"b\" : 6 } ] ] It will be displayed as a table. If the output is similar to below: [ \"Text output.\" ] It will be displayed as a text. All other format of outputs will be displayed as a JSON.","title":"Output"},{"location":"rc/get-started/home/","text":"\u4ece\u9ad8\u624b\u5230\u5c0f\u767d\u4e4b\u8def \u00b6 Pinferencia\u7684\u76ee\u6807 Pinferencia, \u81f4\u529b\u4e8e\u63d0\u4f9b\u6700\u7b80\u5355\u76f4\u63a5\u7684\u65b9\u5f0f\u8ba9\u4f60\u7684\u6a21\u578b\u62e5\u6709API\u670d\u52a1\u3002 \u8fd9\u91cc\u4e0d\u662f\u4e3a\u4e86\u90a3\u4e9b\u7cbe\u901a\u7f51\u7edc\u7f16\u7a0b\u7684\u4eba\u51c6\u5907\u7684\uff0c\u66f4\u591a\u662f\u4e3a\u4e86\u5927\u591a\u6570\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u670b\u53cb\u3002 \u4e0d\u7ba1\u4f60\u662f\u60f3\u505ademo\uff0c\u8fd8\u662f\u60f3\u7ed9\u516c\u53f8\u6216\u5b66\u6821\u63d0\u4f9b\u5185\u90e8\u63a5\u53e3\uff0c\u751a\u81f3\u66f4\u8fdb\u4e00\u6b65\u96c6\u6210\u5230CICD\uff0c\u76f4\u63a5\u90e8\u7f72\u4e00\u4e2a\u670d\u52a1\u5230\u4e91\u7aef\uff0c\u4f60\u90fd\u80fd\u5728\u8fd9\u91cc\u627e\u5230\u89e3\u7b54\u3002 \u4eba\u4eba\u90fd\u60f3\u6210\u4e3a\u9ad8\u624b\uff0c\u5374\u4e0d\u77e5\uff0c\u524d\u8fdb\u53ea\u6709\u4e24\u6761\u817f\u8d70\u3002 \u8fd9\u4e2a\u4e16\u754c\u9ad8\u624b\u90a3\u4e48\u7a00\u5c11\uff0c\u4f46\u6211\u8fd8\u5728\u66ff\u9ad8\u624b\u82e6\u607c\u3002 \u5c0f\u767d\u867d\u7136\u904d\u5730\u90fd\u6709\uff0c\u800c\u6211\u5374\u8ba9\u5c0f\u767d\u6d88\u6101\u501f\u9152\u3002 \u4e0d\u4e0d\u4e0d\uff0c\u8fd9\u6837\u7b80\u76f4\u662f\u4e00\u6761\u6b7b\u8def\u3002 \u7eb5\u4f7f\u5929\u7a7a\u5982\u6b64\u7a7a\u65f7\uff0c\u6211\u5e94\u8be5\u5b89\u5fc3\u8d70\u5728\u8def\u4e0a\u3002\u4e0d\u8981\u56e0\u4e3a\u60f3\u53bb\u98de\u5411\u5929\u7a7a\uff0c\u800c\u5fd8\u8bb0\u81ea\u5df1\u7684\u521d\u8877\u3002 \u6211\u60f3\u8ba9\u5929\u4e0b\u5c0f\u767d\uff0c\u90fd\u4ece\u6b64\u4e00\u626b\u9634\u973e\uff0c \u800c\u6211\u8fd9\u4e2a\u5c0f\u767d\uff0c\u4e5f\u80fd\u591a\u7ed9\u4e16\u754c\u4e00\u70b9\u70b9\u7231\u3002 \u901a\u8fc7\u8fd9\u4e00\u7cfb\u5217\u6559\u7a0b\uff0c\u4f60\u5c06\u638c\u63e1\u5982\u4f55\uff1a \u542f\u52a8\u4e00\u4e2a JSONModel \u670d\u52a1 \u542f\u52a8\u4e00\u4e2a Function \u670d\u52a1 \u5982\u4f55\u4f7f\u7528\u7528\u4e24\u79cd\u65b9\u6cd5\u542f\u52a8 PyTorch MNIST \u6a21\u578b, \u7528 MNIST \u641e\u70b9\u4e50\u5b50","title":"\u4ecb\u7ecd"},{"location":"rc/get-started/home/#_1","text":"Pinferencia\u7684\u76ee\u6807 Pinferencia, \u81f4\u529b\u4e8e\u63d0\u4f9b\u6700\u7b80\u5355\u76f4\u63a5\u7684\u65b9\u5f0f\u8ba9\u4f60\u7684\u6a21\u578b\u62e5\u6709API\u670d\u52a1\u3002 \u8fd9\u91cc\u4e0d\u662f\u4e3a\u4e86\u90a3\u4e9b\u7cbe\u901a\u7f51\u7edc\u7f16\u7a0b\u7684\u4eba\u51c6\u5907\u7684\uff0c\u66f4\u591a\u662f\u4e3a\u4e86\u5927\u591a\u6570\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u670b\u53cb\u3002 \u4e0d\u7ba1\u4f60\u662f\u60f3\u505ademo\uff0c\u8fd8\u662f\u60f3\u7ed9\u516c\u53f8\u6216\u5b66\u6821\u63d0\u4f9b\u5185\u90e8\u63a5\u53e3\uff0c\u751a\u81f3\u66f4\u8fdb\u4e00\u6b65\u96c6\u6210\u5230CICD\uff0c\u76f4\u63a5\u90e8\u7f72\u4e00\u4e2a\u670d\u52a1\u5230\u4e91\u7aef\uff0c\u4f60\u90fd\u80fd\u5728\u8fd9\u91cc\u627e\u5230\u89e3\u7b54\u3002 \u4eba\u4eba\u90fd\u60f3\u6210\u4e3a\u9ad8\u624b\uff0c\u5374\u4e0d\u77e5\uff0c\u524d\u8fdb\u53ea\u6709\u4e24\u6761\u817f\u8d70\u3002 \u8fd9\u4e2a\u4e16\u754c\u9ad8\u624b\u90a3\u4e48\u7a00\u5c11\uff0c\u4f46\u6211\u8fd8\u5728\u66ff\u9ad8\u624b\u82e6\u607c\u3002 \u5c0f\u767d\u867d\u7136\u904d\u5730\u90fd\u6709\uff0c\u800c\u6211\u5374\u8ba9\u5c0f\u767d\u6d88\u6101\u501f\u9152\u3002 \u4e0d\u4e0d\u4e0d\uff0c\u8fd9\u6837\u7b80\u76f4\u662f\u4e00\u6761\u6b7b\u8def\u3002 \u7eb5\u4f7f\u5929\u7a7a\u5982\u6b64\u7a7a\u65f7\uff0c\u6211\u5e94\u8be5\u5b89\u5fc3\u8d70\u5728\u8def\u4e0a\u3002\u4e0d\u8981\u56e0\u4e3a\u60f3\u53bb\u98de\u5411\u5929\u7a7a\uff0c\u800c\u5fd8\u8bb0\u81ea\u5df1\u7684\u521d\u8877\u3002 \u6211\u60f3\u8ba9\u5929\u4e0b\u5c0f\u767d\uff0c\u90fd\u4ece\u6b64\u4e00\u626b\u9634\u973e\uff0c \u800c\u6211\u8fd9\u4e2a\u5c0f\u767d\uff0c\u4e5f\u80fd\u591a\u7ed9\u4e16\u754c\u4e00\u70b9\u70b9\u7231\u3002 \u901a\u8fc7\u8fd9\u4e00\u7cfb\u5217\u6559\u7a0b\uff0c\u4f60\u5c06\u638c\u63e1\u5982\u4f55\uff1a \u542f\u52a8\u4e00\u4e2a JSONModel \u670d\u52a1 \u542f\u52a8\u4e00\u4e2a Function \u670d\u52a1 \u5982\u4f55\u4f7f\u7528\u7528\u4e24\u79cd\u65b9\u6cd5\u542f\u52a8 PyTorch MNIST \u6a21\u578b, \u7528 MNIST \u641e\u70b9\u4e50\u5b50","title":"\u4ece\u9ad8\u624b\u5230\u5c0f\u767d\u4e4b\u8def"},{"location":"rc/get-started/other-models/","text":"\u4e0b\u4e00\u6b65 \u00b6 \u597d\u5427\uff0c\u6211\u6562\u6253\u8d4c\uff0c\u60a8\u5728\u4e0a\u4e00\u6559\u7a0b\u4e2d\u4f7f\u7528 PyTorch MNIST \u6a21\u578b\u4e00\u5b9a\u4f1a\u5f88\u6709\u8da3\u3002 \u6211\u60f3\u4f60\u73b0\u5728\u5bf9 Pinferencia \u5f88\u719f\u6089\u4e86\u3002 Pinferencia \u53ef\u4ee5\u4ee5\u975e\u5e38\u76f4\u63a5\u7684\u65b9\u5f0f\u4e3a\u4efb\u4f55\u53ef\u8c03\u7528\u5bf9\u8c61\u63d0\u4f9b\u670d\u52a1\u3002\u975e\u5e38\u7b80\u5355\u76f4\u63a5\u3002 \u5e76\u4e14\u8fd9\u5f88\u5bb9\u6613\u4e0e\u60a8\u73b0\u6709\u7684\u4ee3\u7801\u96c6\u6210\u3002 \u8fd9\u5c31\u662f Pinferencia \u7684\u8bbe\u8ba1\u76ee\u7684\u3002 \u6700\u5c11\u7684\u4ee3\u7801\u4fee\u6539 \u3002 \u73b0\u5728\u60a8\u53ef\u4ee5\u4ece \u4efb\u4f55\u6846\u67b6 \u63d0\u4f9b\u6a21\u578b\uff0c\u751a\u81f3\u53ef\u4ee5 \u5c06\u5b83\u4eec\u6df7\u5408\u5728\u4e00\u8d77 \u3002\u60a8\u53ef\u4ee5\u5728\u4e00\u4e2aAPI\u91cc \u540c\u65f6 \u4f7f\u7528\u6765\u81ea \u4e0d\u540c\u6846\u67b6 \u7684 \u4e0d\u540c\u6a21\u578b \uff01 \u73a9\u7684\u5f00\u5fc3\uff01 \u5982\u679c\u4f60\u559c\u6b22 Pinferencia \uff0c\u522b\u5fd8\u4e86\u53bb Github \u5e76\u7ed9\u4e00\u4e2a\u661f\u3002\u8c22\u8c22\u4f60\u3002","title":"\u4e0b\u4e00\u6b65"},{"location":"rc/get-started/other-models/#_1","text":"\u597d\u5427\uff0c\u6211\u6562\u6253\u8d4c\uff0c\u60a8\u5728\u4e0a\u4e00\u6559\u7a0b\u4e2d\u4f7f\u7528 PyTorch MNIST \u6a21\u578b\u4e00\u5b9a\u4f1a\u5f88\u6709\u8da3\u3002 \u6211\u60f3\u4f60\u73b0\u5728\u5bf9 Pinferencia \u5f88\u719f\u6089\u4e86\u3002 Pinferencia \u53ef\u4ee5\u4ee5\u975e\u5e38\u76f4\u63a5\u7684\u65b9\u5f0f\u4e3a\u4efb\u4f55\u53ef\u8c03\u7528\u5bf9\u8c61\u63d0\u4f9b\u670d\u52a1\u3002\u975e\u5e38\u7b80\u5355\u76f4\u63a5\u3002 \u5e76\u4e14\u8fd9\u5f88\u5bb9\u6613\u4e0e\u60a8\u73b0\u6709\u7684\u4ee3\u7801\u96c6\u6210\u3002 \u8fd9\u5c31\u662f Pinferencia \u7684\u8bbe\u8ba1\u76ee\u7684\u3002 \u6700\u5c11\u7684\u4ee3\u7801\u4fee\u6539 \u3002 \u73b0\u5728\u60a8\u53ef\u4ee5\u4ece \u4efb\u4f55\u6846\u67b6 \u63d0\u4f9b\u6a21\u578b\uff0c\u751a\u81f3\u53ef\u4ee5 \u5c06\u5b83\u4eec\u6df7\u5408\u5728\u4e00\u8d77 \u3002\u60a8\u53ef\u4ee5\u5728\u4e00\u4e2aAPI\u91cc \u540c\u65f6 \u4f7f\u7528\u6765\u81ea \u4e0d\u540c\u6846\u67b6 \u7684 \u4e0d\u540c\u6a21\u578b \uff01 \u73a9\u7684\u5f00\u5fc3\uff01 \u5982\u679c\u4f60\u559c\u6b22 Pinferencia \uff0c\u522b\u5fd8\u4e86\u53bb Github \u5e76\u7ed9\u4e00\u4e2a\u661f\u3002\u8c22\u8c22\u4f60\u3002","title":"\u4e0b\u4e00\u6b65"},{"location":"rc/get-started/pytorch-mnist/","text":"\u4e0a\u7ebf PyTorch MNIST \u6a21\u578b \u00b6 \u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u63d0\u4f9b PyTorch MNIST \u6a21\u578b\u3002 \u5b83\u63a5\u6536 Base64 \u7f16\u7801\u7684\u56fe\u50cf\u4f5c\u4e3a\u8bf7\u6c42\u6570\u636e\uff0c\u5e76\u5728\u54cd\u5e94\u4e2d\u8fd4\u56de\u9884\u6d4b\u3002 \u51c6\u5907\u5de5\u4f5c \u00b6 \u8bbf\u95ee PyTorch \u793a\u4f8b - MNIST \uff0c\u4e0b\u8f7d\u6587\u4ef6\u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6765\u5b89\u88c5\u548c\u8bad\u7ec3\u6a21\u578b\uff1a pip install -r requirements.txt python main.py --save-model \u8bad\u7ec3\u5b8c\u6210\u540e\uff0c\u60a8\u5c06\u62e5\u6709\u5982\u4e0b\u6587\u4ef6\u5939\u7ed3\u6784\u3002\u521b\u5efa\u4e86\u4e00\u4e2a mnist_cnn.pt \u6587\u4ef6 . \u251c\u2500\u2500 README.md \u251c\u2500\u2500 main.py \u251c\u2500\u2500 mnist_cnn.pt \u2514\u2500\u2500 requirements.txt \u90e8\u7f72\u65b9\u6cd5 \u00b6 \u6709\u4e24\u79cd\u65b9\u6cd5\u53ef\u4ee5\u90e8\u7f72\u6a21\u578b\u3002 \u76f4\u63a5\u6ce8\u518c\u4e00\u4e2a\u51fd\u6570\u3002 \u4ec5\u4f7f\u7528\u9644\u52a0\u5904\u7406\u7a0b\u5e8f Handler \u6ce8\u518c\u6a21\u578b\u8def\u5f84\u3002 \u6211\u4eec\u5c06\u5728\u672c\u6559\u7a0b\u4e2d\u9010\u6b65\u4ecb\u7ecd\u8fd9\u4e24\u79cd\u65b9\u6cd5\u3002 \u76f4\u63a5\u6ce8\u518c\u4e00\u4e2a\u51fd\u6570 \u00b6 \u521b\u5efa\u5e94\u7528\u7a0b\u5e8f \u00b6 \u8ba9\u6211\u4eec\u5728\u540c\u4e00\u4e2a\u6587\u4ef6\u5939\u4e2d\u521b\u5efa\u4e00\u4e2a\u6587\u4ef6 func_app.py \u3002 func_app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 import base64 from io import BytesIO import torch from main import Net # (1) from PIL import Image from torchvision import transforms from pinferencia import Server use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) # (2) model = Net () . to ( device ) # (3) model . load_state_dict ( torch . load ( \"mnist_cnn.pt\" )) model . eval () def preprocessing ( img_str ): image = Image . open ( BytesIO ( base64 . b64decode ( img_str ))) tensor = transform ( image ) return torch . stack ([ tensor ]) . to ( device ) def predict ( data ): return model ( preprocessing ( data )) . argmax ( 1 ) . tolist ()[ 0 ] service = Server () # (4) service . register ( model_name = \"mnist\" , model = predict ) \u786e\u4fdd\u60a8\u53ef\u4ee5\u5bfc\u5165\u7f51\u7edc\u6a21\u578b\u3002 \u9884\u5904\u7406\u8f6c\u6362\u4ee3\u7801\u3002 \u793a\u4f8b\u811a\u672c\u53ea\u4fdd\u5b58 state_dict \u3002\u8fd9\u91cc\u6211\u4eec\u9700\u8981\u521d\u59cb\u5316\u6a21\u578b\u5e76\u52a0\u8f7d state_dict \u3002 \u51c6\u5907\u597d\uff0c3\u30012\u30011\u3002 GO\uff01 \u542f\u52a8\u670d\u52a1 \u00b6 $ uvicorn func_app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. \u6d4b\u8bd5\u670d\u52a1 \u00b6 \u6d4b\u8bd5\u6570\u636e\u90a3\u91cc\u6765? \u56e0\u4e3a\u6211\u4eec\u7684\u8f93\u5165\u662f base64 \u7f16\u7801\u7684 MNIST \u56fe\u50cf\uff0c\u6211\u4eec\u4ece\u54ea\u91cc\u53ef\u4ee5\u83b7\u5f97\u8fd9\u4e9b\u6570\u636e\uff1f \u60a8\u53ef\u4ee5\u4f7f\u7528 PyTorch \u7684\u6570\u636e\u96c6\u3002\u5728\u540c\u4e00\u6587\u4ef6\u5939\u4e2d\u521b\u5efa\u4e00\u4e2a\u6587\u4ef6\u540d\u4e3a get-base64-img.oy \u3002 get-base64-img.py import base64 import random from io import BytesIO from PIL import Image from torchvision import datasets dataset = datasets . MNIST ( # (1) \"./data\" , train = True , download = True , transform = None , ) index = random . randint ( 0 , len ( dataset . data )) # (2) img = dataset . data [ index ] img = Image . fromarray ( img . numpy (), mode = \"L\" ) buffered = BytesIO () img . save ( buffered , format = \"JPEG\" ) base64_img_str = base64 . b64encode ( buffered . getvalue ()) . decode () print ( \"Base64 String:\" , base64_img_str ) # (3) print ( \"target:\" , dataset . targets [ index ] . tolist ()) \u8fd9\u662f\u8bad\u7ec3\u671f\u95f4\u4f7f\u7528\u7684 MNIST \u6570\u636e\u96c6\u3002 \u8ba9\u6211\u4eec\u4f7f\u7528\u968f\u673a\u56fe\u50cf\u3002 \u5b57\u7b26\u4e32\u548c\u76ee\u6807\u88ab\u6253\u5370\u5230\u6807\u51c6\u8f93\u51fa\u3002 \u8fd0\u884c\u811a\u672c\u5e76\u590d\u5236\u5b57\u7b26\u4e32\u3002 python get-base64-img.py \u8f93\u51fa: Base64 String: /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+uhfwXqy2Ph25VYnPiB3SzhUkPlXCfNkAAEsCCCeOeKx9RsLjStUu9Ou1C3NpM8Eqg5AdSVIz35FVqK9xl0HXhb/C20sdMubjTLMQXs11AhkRXmmDsCwzgAYPpz+XI/GrSLrTfiVqNzPapbw3xE8AWQNvUAKXOOmWVjg+teeUV2fgXxd4hsPE2hWEGuX8Vh9uhja3Fw3lbGcBhtzjGCad8XI7iL4p68twHDGcMm45+QqCuPbBFcVRRU97fXepXb3d9dT3VzJjfNPIXdsAAZY8nAAH4VBX/9k = target: 4 \u8ba9\u6211\u4eec\u521b\u5efa\u4e00\u4e2a\u6587\u4ef6 test.py test.py 1 2 3 4 5 6 7 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/mnist/predict\" , json = { \"data\" : \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+uhfwXqy2Ph25VYnPiB3SzhUkPlXCfNkAAEsCCCeOeKx9RsLjStUu9Ou1C3NpM8Eqg5AdSVIz35FVqK9xl0HXhb/C20sdMubjTLMQXs11AhkRXmmDsCwzgAYPpz+XI/GrSLrTfiVqNzPapbw3xE8AWQNvUAKXOOmWVjg+teeUV2fgXxd4hsPE2hWEGuX8Vh9uhja3Fw3lbGcBhtzjGCad8XI7iL4p68twHDGcMm45+QqCuPbBFcVRRU97fXepXb3d9dT3VzJjfNPIXdsAAZY8nAAH4VBX/9k=\" }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) \u8fd0\u884c\u6d4b\u8bd5: $ python test.py Prediction: 4 \u60a8\u53ef\u4ee5\u5c1d\u8bd5\u4f7f\u7528\u66f4\u591a\u56fe\u50cf\u6765\u6d4b\u8bd5\uff0c\u751a\u81f3\u53ef\u4ee5\u4f7f\u7528\u4ea4\u4e92\u5f0f API \u6587\u6863\u9875\u9762 http://127.0.0.1 \u4f7f\u7528 Handler \u6ce8\u518c\u6a21\u578b\u8def\u5f84 \u00b6 Handler \u5982\u679c\u60a8\u66f4\u559c\u6b22\u4f7f\u7528\u6587\u4ef6\u63d0\u4f9b\u6a21\u578b\u7684\u7ecf\u5178\u65b9\u5f0f\uff0c\u5219\u4f7f\u7528\u201cHandlers\u201d\u662f\u60a8\u7684\u9009\u62e9\u3002 \u5904\u7406\u7a0b\u5e8f\u7684\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u8bbf\u95ee Handlers \u521b\u5efa\u5e94\u7528\u7a0b\u5e8f \u00b6 \u8ba9\u6211\u4eec\u5728\u540c\u4e00\u4e2a\u6587\u4ef6\u5939\u4e2d\u521b\u5efa\u4e00\u4e2a\u6587\u4ef6 func_app.py \u3002 \u4e0b\u9762\u7684\u4ee3\u7801\u88ab\u91cd\u6784\u4e3a MNISTHandler \u3002\u770b\u8d77\u6765\u66f4\u5e72\u51c0\uff01 path_app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 import base64 import pathlib from io import BytesIO import torch from main import Net from PIL import Image from torchvision import transforms from pinferencia import Server from pinferencia.handlers import BaseHandler class MNISTHandler ( BaseHandler ): transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) def load_model ( self ): # (1) model = Net () . to ( self . device ) model . load_state_dict ( torch . load ( self . model_path )) model . eval () return model def predict ( self , data ): # (2) image = Image . open ( BytesIO ( base64 . b64decode ( data ))) tensor = self . transform ( image ) input_data = torch . stack ([ tensor ]) . to ( self . device ) return self . model ( input_data ) . argmax ( 1 ) . tolist ()[ 0 ] service = Server ( model_dir = pathlib . Path ( __file__ ) . parent . resolve ()) # (3) service . register ( model_name = \"mnist\" , model = \"mnist_cnn.pt\" , handler = MNISTHandler , load_now = True , # (4) ) \u6211\u4eec\u5c06\u52a0\u8f7d\u6a21\u578b\u7684\u4ee3\u7801\u79fb\u5230 load_model \u51fd\u6570\u4e2d\u3002\u6a21\u578b\u8def\u5f84\u53ef\u4ee5\u901a\u8fc7 self.model_path \u8bbf\u95ee\u3002 \u6211\u4eec\u5c06\u9884\u6d4b\u4ee3\u7801\u79fb\u5230 predict \u51fd\u6570\u4e2d\u3002\u8be5\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7 self.model \u8bbf\u95ee\u3002 model_dir \u662f Pinferencia \u67e5\u627e\u6a21\u578b\u6587\u4ef6\u7684\u5730\u65b9\u3002\u5c06 model_dir \u8bbe\u7f6e\u4e3a\u5305\u542b mnist_cnn.pt \u548c\u6b64\u811a\u672c\u7684\u6587\u4ef6\u5939\u3002 load_now \u786e\u5b9a\u6a21\u578b\u662f\u5426\u4f1a\u5728\u6ce8\u518c\u671f\u95f4\u7acb\u5373\u52a0\u8f7d\u3002\u9ed8\u8ba4\u503c\u4e3a\u201c\u771f\u201d\u3002\u5982\u679c\u8bbe\u7f6e\u4e3a False \uff0c\u5219\u9700\u8981\u8c03\u7528 load API \u52a0\u8f7d\u6a21\u578b\u624d\u80fd\u8fdb\u884c\u9884\u6d4b\u3002 \u542f\u52a8\u670d\u52a1 \u00b6 $ uvicorn path_app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. \u6d4b\u8bd5\u670d\u52a1 \u00b6 \u8fd0\u884c\u6d4b\u8bd5: $ python test.py Prediction: 4 \u4e0d\u51fa\u610f\u5916\uff0c\u7ed3\u679c\u4e00\u6837\u3002 \u6700\u540e \u00b6 \u4f7f\u7528 Pinferencia \uff0c\u60a8\u53ef\u4ee5\u4e3a\u4efb\u4f55\u6a21\u578b\u63d0\u4f9b\u670d\u52a1\u3002 \u60a8\u53ef\u4ee5\u81ea\u5df1\u52a0\u8f7d\u6a21\u578b\uff0c\u5c31\u50cf\u60a8\u5728\u8fdb\u884c\u79bb\u7ebf\u9884\u6d4b\u65f6\u6240\u505a\u7684\u90a3\u6837\u3002 \u8fd9\u90e8\u5206\u4ee3\u7801\u4f60\u65e9\u5c31\u5df2\u7ecf\u5199\u597d\u4e86\u3002 \u7136\u540e\uff0c\u53ea\u9700\u4f7f\u7528 Pinferencia \u6ce8\u518c\u6a21\u578b\uff0c\u60a8\u7684\u6a21\u578b\u5c31\u4f1a\u751f\u6548\u3002 \u6216\u8005\uff0c\u60a8\u53ef\u4ee5\u9009\u62e9\u5c06\u4ee3\u7801\u91cd\u6784\u4e3a Handler Class \u3002\u65e7\u7684\u7ecf\u5178\u65b9\u5f0f\u4e5f\u9002\u7528\u4e8e Pinferencia \u3002 \u8fd9\u4e24\u4e2a\u4e16\u754c\u90fd\u9002\u7528\u4e8e\u60a8\u7684\u6a21\u578b\uff0c \u7ecf\u5178\u200b\u200b\u97f3\u4e50**\u548c**\u6447\u6eda\u4e50 \u3002 \u662f\u4e0d\u662f\u5f88\u68d2\uff01 \u73b0\u5728\u60a8\u5df2\u7ecf\u638c\u63e1\u4e86\u5982\u4f55\u4f7f\u7528 Pinferencia \u6765\uff1a \u6ce8\u518c\u4efb\u4f55\u6a21\u578b\u3001\u4efb\u4f55\u51fd\u6570\u5e76\u628a\u5b83\u4eec\u4e0a\u7ebf\u3002 \u4f7f\u7528\u60a8\u7684\u81ea\u5b9a\u4e49\u5904\u7406\u7a0b\u5e8f\u4e3a\u60a8\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u63d0\u4f9b\u670d\u52a1\u3002 \u5982\u679c\u4f60\u8fd8\u6709\u65f6\u95f4\uff0c\u8ba9\u6211\u4eec\u5c1d\u8bd5\u4e00\u4e9b\u6709\u8da3\u7684\u4e8b\u60c5\u3002 \u989d\u5916\uff1a MNIST \u56fe\u50cf\u7684\u52a0\u6cd5 \u00b6 \u8ba9\u6211\u4eec\u521b\u5efa\u4e00\u4e2a\u201csum_mnist.py\u201d\u3002\u5b83\u63a5\u53d7\u4e00\u7ec4\u56fe\u50cf\uff0c\u9884\u6d4b\u5b83\u4eec\u7684\u6570\u5b57\u5e76\u628a\u5b83\u4eec\u52a0\u8d77\u6765\u3002 sum_mnist.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 import base64 import pathlib from io import BytesIO import torch from main import Net from PIL import Image from torchvision import transforms from pinferencia import Server from pinferencia.handlers import BaseHandler class MNISTHandler ( BaseHandler ): transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) def load_model ( self ): model = Net () . to ( self . device ) model . load_state_dict ( torch . load ( self . model_path )) model . eval () return model def predict ( self , data ): tensors = [] # (1) for img in data : image = Image . open ( BytesIO ( base64 . b64decode ( img ))) tensors . append ( self . transform ( image )) input_data = torch . stack ( tensors ) . to ( self . device ) return sum ( self . model ( input_data ) . argmax ( 1 ) . tolist ()) service = Server ( model_dir = pathlib . Path ( __file__ ) . parent . resolve ()) service . register ( model_name = \"mnist\" , model = \"mnist_cnn.pt\" , handler = MNISTHandler , load_now = True , ) \u8fd9\u91cc\u6211\u4eec\u5bf9\u6bcf\u5f20\u56fe\u50cf\u8fdb\u884c\u9884\u5904\u7406\uff0c\u9884\u6d4b\u5176\u4f4d\u6570\u5e76\u8fdb\u884c\u603b\u7ed3\u3002 \u5e0c\u671b\u4f60\u5728 Pinferencia \u4e16\u754c\u73a9\u5f97\u5f00\u5fc3\uff01","title":"\u542f\u52a8PyTorch MNIST\u6a21\u578b"},{"location":"rc/get-started/pytorch-mnist/#pytorch-mnist","text":"\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u63d0\u4f9b PyTorch MNIST \u6a21\u578b\u3002 \u5b83\u63a5\u6536 Base64 \u7f16\u7801\u7684\u56fe\u50cf\u4f5c\u4e3a\u8bf7\u6c42\u6570\u636e\uff0c\u5e76\u5728\u54cd\u5e94\u4e2d\u8fd4\u56de\u9884\u6d4b\u3002","title":"\u4e0a\u7ebf PyTorch MNIST \u6a21\u578b"},{"location":"rc/get-started/pytorch-mnist/#_1","text":"\u8bbf\u95ee PyTorch \u793a\u4f8b - MNIST \uff0c\u4e0b\u8f7d\u6587\u4ef6\u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6765\u5b89\u88c5\u548c\u8bad\u7ec3\u6a21\u578b\uff1a pip install -r requirements.txt python main.py --save-model \u8bad\u7ec3\u5b8c\u6210\u540e\uff0c\u60a8\u5c06\u62e5\u6709\u5982\u4e0b\u6587\u4ef6\u5939\u7ed3\u6784\u3002\u521b\u5efa\u4e86\u4e00\u4e2a mnist_cnn.pt \u6587\u4ef6 . \u251c\u2500\u2500 README.md \u251c\u2500\u2500 main.py \u251c\u2500\u2500 mnist_cnn.pt \u2514\u2500\u2500 requirements.txt","title":"\u51c6\u5907\u5de5\u4f5c"},{"location":"rc/get-started/pytorch-mnist/#_2","text":"\u6709\u4e24\u79cd\u65b9\u6cd5\u53ef\u4ee5\u90e8\u7f72\u6a21\u578b\u3002 \u76f4\u63a5\u6ce8\u518c\u4e00\u4e2a\u51fd\u6570\u3002 \u4ec5\u4f7f\u7528\u9644\u52a0\u5904\u7406\u7a0b\u5e8f Handler \u6ce8\u518c\u6a21\u578b\u8def\u5f84\u3002 \u6211\u4eec\u5c06\u5728\u672c\u6559\u7a0b\u4e2d\u9010\u6b65\u4ecb\u7ecd\u8fd9\u4e24\u79cd\u65b9\u6cd5\u3002","title":"\u90e8\u7f72\u65b9\u6cd5"},{"location":"rc/get-started/pytorch-mnist/#_3","text":"","title":"\u76f4\u63a5\u6ce8\u518c\u4e00\u4e2a\u51fd\u6570"},{"location":"rc/get-started/pytorch-mnist/#_4","text":"\u8ba9\u6211\u4eec\u5728\u540c\u4e00\u4e2a\u6587\u4ef6\u5939\u4e2d\u521b\u5efa\u4e00\u4e2a\u6587\u4ef6 func_app.py \u3002 func_app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 import base64 from io import BytesIO import torch from main import Net # (1) from PIL import Image from torchvision import transforms from pinferencia import Server use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) # (2) model = Net () . to ( device ) # (3) model . load_state_dict ( torch . load ( \"mnist_cnn.pt\" )) model . eval () def preprocessing ( img_str ): image = Image . open ( BytesIO ( base64 . b64decode ( img_str ))) tensor = transform ( image ) return torch . stack ([ tensor ]) . to ( device ) def predict ( data ): return model ( preprocessing ( data )) . argmax ( 1 ) . tolist ()[ 0 ] service = Server () # (4) service . register ( model_name = \"mnist\" , model = predict ) \u786e\u4fdd\u60a8\u53ef\u4ee5\u5bfc\u5165\u7f51\u7edc\u6a21\u578b\u3002 \u9884\u5904\u7406\u8f6c\u6362\u4ee3\u7801\u3002 \u793a\u4f8b\u811a\u672c\u53ea\u4fdd\u5b58 state_dict \u3002\u8fd9\u91cc\u6211\u4eec\u9700\u8981\u521d\u59cb\u5316\u6a21\u578b\u5e76\u52a0\u8f7d state_dict \u3002 \u51c6\u5907\u597d\uff0c3\u30012\u30011\u3002 GO\uff01","title":"\u521b\u5efa\u5e94\u7528\u7a0b\u5e8f"},{"location":"rc/get-started/pytorch-mnist/#_5","text":"$ uvicorn func_app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete.","title":"\u542f\u52a8\u670d\u52a1"},{"location":"rc/get-started/pytorch-mnist/#_6","text":"\u6d4b\u8bd5\u6570\u636e\u90a3\u91cc\u6765? \u56e0\u4e3a\u6211\u4eec\u7684\u8f93\u5165\u662f base64 \u7f16\u7801\u7684 MNIST \u56fe\u50cf\uff0c\u6211\u4eec\u4ece\u54ea\u91cc\u53ef\u4ee5\u83b7\u5f97\u8fd9\u4e9b\u6570\u636e\uff1f \u60a8\u53ef\u4ee5\u4f7f\u7528 PyTorch \u7684\u6570\u636e\u96c6\u3002\u5728\u540c\u4e00\u6587\u4ef6\u5939\u4e2d\u521b\u5efa\u4e00\u4e2a\u6587\u4ef6\u540d\u4e3a get-base64-img.oy \u3002 get-base64-img.py import base64 import random from io import BytesIO from PIL import Image from torchvision import datasets dataset = datasets . MNIST ( # (1) \"./data\" , train = True , download = True , transform = None , ) index = random . randint ( 0 , len ( dataset . data )) # (2) img = dataset . data [ index ] img = Image . fromarray ( img . numpy (), mode = \"L\" ) buffered = BytesIO () img . save ( buffered , format = \"JPEG\" ) base64_img_str = base64 . b64encode ( buffered . getvalue ()) . decode () print ( \"Base64 String:\" , base64_img_str ) # (3) print ( \"target:\" , dataset . targets [ index ] . tolist ()) \u8fd9\u662f\u8bad\u7ec3\u671f\u95f4\u4f7f\u7528\u7684 MNIST \u6570\u636e\u96c6\u3002 \u8ba9\u6211\u4eec\u4f7f\u7528\u968f\u673a\u56fe\u50cf\u3002 \u5b57\u7b26\u4e32\u548c\u76ee\u6807\u88ab\u6253\u5370\u5230\u6807\u51c6\u8f93\u51fa\u3002 \u8fd0\u884c\u811a\u672c\u5e76\u590d\u5236\u5b57\u7b26\u4e32\u3002 python get-base64-img.py \u8f93\u51fa: Base64 String: /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+uhfwXqy2Ph25VYnPiB3SzhUkPlXCfNkAAEsCCCeOeKx9RsLjStUu9Ou1C3NpM8Eqg5AdSVIz35FVqK9xl0HXhb/C20sdMubjTLMQXs11AhkRXmmDsCwzgAYPpz+XI/GrSLrTfiVqNzPapbw3xE8AWQNvUAKXOOmWVjg+teeUV2fgXxd4hsPE2hWEGuX8Vh9uhja3Fw3lbGcBhtzjGCad8XI7iL4p68twHDGcMm45+QqCuPbBFcVRRU97fXepXb3d9dT3VzJjfNPIXdsAAZY8nAAH4VBX/9k = target: 4 \u8ba9\u6211\u4eec\u521b\u5efa\u4e00\u4e2a\u6587\u4ef6 test.py test.py 1 2 3 4 5 6 7 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/mnist/predict\" , json = { \"data\" : \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+uhfwXqy2Ph25VYnPiB3SzhUkPlXCfNkAAEsCCCeOeKx9RsLjStUu9Ou1C3NpM8Eqg5AdSVIz35FVqK9xl0HXhb/C20sdMubjTLMQXs11AhkRXmmDsCwzgAYPpz+XI/GrSLrTfiVqNzPapbw3xE8AWQNvUAKXOOmWVjg+teeUV2fgXxd4hsPE2hWEGuX8Vh9uhja3Fw3lbGcBhtzjGCad8XI7iL4p68twHDGcMm45+QqCuPbBFcVRRU97fXepXb3d9dT3VzJjfNPIXdsAAZY8nAAH4VBX/9k=\" }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) \u8fd0\u884c\u6d4b\u8bd5: $ python test.py Prediction: 4 \u60a8\u53ef\u4ee5\u5c1d\u8bd5\u4f7f\u7528\u66f4\u591a\u56fe\u50cf\u6765\u6d4b\u8bd5\uff0c\u751a\u81f3\u53ef\u4ee5\u4f7f\u7528\u4ea4\u4e92\u5f0f API \u6587\u6863\u9875\u9762 http://127.0.0.1","title":"\u6d4b\u8bd5\u670d\u52a1"},{"location":"rc/get-started/pytorch-mnist/#handler","text":"Handler \u5982\u679c\u60a8\u66f4\u559c\u6b22\u4f7f\u7528\u6587\u4ef6\u63d0\u4f9b\u6a21\u578b\u7684\u7ecf\u5178\u65b9\u5f0f\uff0c\u5219\u4f7f\u7528\u201cHandlers\u201d\u662f\u60a8\u7684\u9009\u62e9\u3002 \u5904\u7406\u7a0b\u5e8f\u7684\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u8bbf\u95ee Handlers","title":"\u4f7f\u7528 Handler \u6ce8\u518c\u6a21\u578b\u8def\u5f84"},{"location":"rc/get-started/pytorch-mnist/#_7","text":"\u8ba9\u6211\u4eec\u5728\u540c\u4e00\u4e2a\u6587\u4ef6\u5939\u4e2d\u521b\u5efa\u4e00\u4e2a\u6587\u4ef6 func_app.py \u3002 \u4e0b\u9762\u7684\u4ee3\u7801\u88ab\u91cd\u6784\u4e3a MNISTHandler \u3002\u770b\u8d77\u6765\u66f4\u5e72\u51c0\uff01 path_app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 import base64 import pathlib from io import BytesIO import torch from main import Net from PIL import Image from torchvision import transforms from pinferencia import Server from pinferencia.handlers import BaseHandler class MNISTHandler ( BaseHandler ): transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) def load_model ( self ): # (1) model = Net () . to ( self . device ) model . load_state_dict ( torch . load ( self . model_path )) model . eval () return model def predict ( self , data ): # (2) image = Image . open ( BytesIO ( base64 . b64decode ( data ))) tensor = self . transform ( image ) input_data = torch . stack ([ tensor ]) . to ( self . device ) return self . model ( input_data ) . argmax ( 1 ) . tolist ()[ 0 ] service = Server ( model_dir = pathlib . Path ( __file__ ) . parent . resolve ()) # (3) service . register ( model_name = \"mnist\" , model = \"mnist_cnn.pt\" , handler = MNISTHandler , load_now = True , # (4) ) \u6211\u4eec\u5c06\u52a0\u8f7d\u6a21\u578b\u7684\u4ee3\u7801\u79fb\u5230 load_model \u51fd\u6570\u4e2d\u3002\u6a21\u578b\u8def\u5f84\u53ef\u4ee5\u901a\u8fc7 self.model_path \u8bbf\u95ee\u3002 \u6211\u4eec\u5c06\u9884\u6d4b\u4ee3\u7801\u79fb\u5230 predict \u51fd\u6570\u4e2d\u3002\u8be5\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7 self.model \u8bbf\u95ee\u3002 model_dir \u662f Pinferencia \u67e5\u627e\u6a21\u578b\u6587\u4ef6\u7684\u5730\u65b9\u3002\u5c06 model_dir \u8bbe\u7f6e\u4e3a\u5305\u542b mnist_cnn.pt \u548c\u6b64\u811a\u672c\u7684\u6587\u4ef6\u5939\u3002 load_now \u786e\u5b9a\u6a21\u578b\u662f\u5426\u4f1a\u5728\u6ce8\u518c\u671f\u95f4\u7acb\u5373\u52a0\u8f7d\u3002\u9ed8\u8ba4\u503c\u4e3a\u201c\u771f\u201d\u3002\u5982\u679c\u8bbe\u7f6e\u4e3a False \uff0c\u5219\u9700\u8981\u8c03\u7528 load API \u52a0\u8f7d\u6a21\u578b\u624d\u80fd\u8fdb\u884c\u9884\u6d4b\u3002","title":"\u521b\u5efa\u5e94\u7528\u7a0b\u5e8f"},{"location":"rc/get-started/pytorch-mnist/#_8","text":"$ uvicorn path_app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete.","title":"\u542f\u52a8\u670d\u52a1"},{"location":"rc/get-started/pytorch-mnist/#_9","text":"\u8fd0\u884c\u6d4b\u8bd5: $ python test.py Prediction: 4 \u4e0d\u51fa\u610f\u5916\uff0c\u7ed3\u679c\u4e00\u6837\u3002","title":"\u6d4b\u8bd5\u670d\u52a1"},{"location":"rc/get-started/pytorch-mnist/#_10","text":"\u4f7f\u7528 Pinferencia \uff0c\u60a8\u53ef\u4ee5\u4e3a\u4efb\u4f55\u6a21\u578b\u63d0\u4f9b\u670d\u52a1\u3002 \u60a8\u53ef\u4ee5\u81ea\u5df1\u52a0\u8f7d\u6a21\u578b\uff0c\u5c31\u50cf\u60a8\u5728\u8fdb\u884c\u79bb\u7ebf\u9884\u6d4b\u65f6\u6240\u505a\u7684\u90a3\u6837\u3002 \u8fd9\u90e8\u5206\u4ee3\u7801\u4f60\u65e9\u5c31\u5df2\u7ecf\u5199\u597d\u4e86\u3002 \u7136\u540e\uff0c\u53ea\u9700\u4f7f\u7528 Pinferencia \u6ce8\u518c\u6a21\u578b\uff0c\u60a8\u7684\u6a21\u578b\u5c31\u4f1a\u751f\u6548\u3002 \u6216\u8005\uff0c\u60a8\u53ef\u4ee5\u9009\u62e9\u5c06\u4ee3\u7801\u91cd\u6784\u4e3a Handler Class \u3002\u65e7\u7684\u7ecf\u5178\u65b9\u5f0f\u4e5f\u9002\u7528\u4e8e Pinferencia \u3002 \u8fd9\u4e24\u4e2a\u4e16\u754c\u90fd\u9002\u7528\u4e8e\u60a8\u7684\u6a21\u578b\uff0c \u7ecf\u5178\u200b\u200b\u97f3\u4e50**\u548c**\u6447\u6eda\u4e50 \u3002 \u662f\u4e0d\u662f\u5f88\u68d2\uff01 \u73b0\u5728\u60a8\u5df2\u7ecf\u638c\u63e1\u4e86\u5982\u4f55\u4f7f\u7528 Pinferencia \u6765\uff1a \u6ce8\u518c\u4efb\u4f55\u6a21\u578b\u3001\u4efb\u4f55\u51fd\u6570\u5e76\u628a\u5b83\u4eec\u4e0a\u7ebf\u3002 \u4f7f\u7528\u60a8\u7684\u81ea\u5b9a\u4e49\u5904\u7406\u7a0b\u5e8f\u4e3a\u60a8\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u63d0\u4f9b\u670d\u52a1\u3002 \u5982\u679c\u4f60\u8fd8\u6709\u65f6\u95f4\uff0c\u8ba9\u6211\u4eec\u5c1d\u8bd5\u4e00\u4e9b\u6709\u8da3\u7684\u4e8b\u60c5\u3002","title":"\u6700\u540e"},{"location":"rc/get-started/pytorch-mnist/#mnist","text":"\u8ba9\u6211\u4eec\u521b\u5efa\u4e00\u4e2a\u201csum_mnist.py\u201d\u3002\u5b83\u63a5\u53d7\u4e00\u7ec4\u56fe\u50cf\uff0c\u9884\u6d4b\u5b83\u4eec\u7684\u6570\u5b57\u5e76\u628a\u5b83\u4eec\u52a0\u8d77\u6765\u3002 sum_mnist.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 import base64 import pathlib from io import BytesIO import torch from main import Net from PIL import Image from torchvision import transforms from pinferencia import Server from pinferencia.handlers import BaseHandler class MNISTHandler ( BaseHandler ): transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) def load_model ( self ): model = Net () . to ( self . device ) model . load_state_dict ( torch . load ( self . model_path )) model . eval () return model def predict ( self , data ): tensors = [] # (1) for img in data : image = Image . open ( BytesIO ( base64 . b64decode ( img ))) tensors . append ( self . transform ( image )) input_data = torch . stack ( tensors ) . to ( self . device ) return sum ( self . model ( input_data ) . argmax ( 1 ) . tolist ()) service = Server ( model_dir = pathlib . Path ( __file__ ) . parent . resolve ()) service . register ( model_name = \"mnist\" , model = \"mnist_cnn.pt\" , handler = MNISTHandler , load_now = True , ) \u8fd9\u91cc\u6211\u4eec\u5bf9\u6bcf\u5f20\u56fe\u50cf\u8fdb\u884c\u9884\u5904\u7406\uff0c\u9884\u6d4b\u5176\u4f4d\u6570\u5e76\u8fdb\u884c\u603b\u7ed3\u3002 \u5e0c\u671b\u4f60\u5728 Pinferencia \u4e16\u754c\u73a9\u5f97\u5f00\u5fc3\uff01","title":"\u989d\u5916\uff1a MNIST \u56fe\u50cf\u7684\u52a0\u6cd5"},{"location":"rc/get-started/serve-a-function/","text":"\u542f\u52a8\u4e00\u4e2a\u51fd\u6570 \u00b6 \u597d\u5427\uff0c\u670d\u52a1\u4e00\u4e2a\u51fd\u6570\uff1f\u6709\u7528\u5417\uff1f \u5f53\u7136\u662f\u7684\u3002 \u5982\u679c\u60a8\u6709 \u4e00\u4e2a\u5b8c\u6574\u7684\u63a8\u7406\u5de5\u4f5c\u6d41\u7a0b \uff0c\u5b83\u5305\u542b\u8bb8\u591a\u6b65\u9aa4\u3002\u5927\u591a\u6570\u65f6\u5019\uff0c\u60a8\u5c06\u5b9e\u73b0\u4e00\u4e2a\u51fd\u6570\u6765\u5b8c\u6210\u8fd9\u9879\u5de5\u4f5c\u3002\u73b0\u5728\u60a8\u53ef\u4ee5\u7acb\u5373\u6ce8\u518c\u8be5\u51fd\u6570\u3002 \u5982\u679c\u4f60\u60f3\u5206\u4eab\u4e00\u4e9b\u9884\u5904\u7406\u6216\u540e\u5904\u7406\u529f\u80fd\uff0c\u73b0\u5728\u4f60\u6709\u4f60\u7684\u7f57\u5bbe\u4e86\uff0c \u8759\u8760\u4fa0 \uff01 \u6216\u8005\u4e00\u4e2a\u51fd\u6570\u5bf9\u4f60\u7684\u5de5\u4f5c\u6765\u8bf4\u5c31\u8db3\u591f\u4e86\u3002 \u4efb\u52a1 \u00b6 \u6211\u4eec\u5f97\u5230\u4e86\u4e00\u4efd\u5c71\u8109\u9ad8\u5ea6\u7684\u5217\u8868\u3002\u6211\u4eec\u9700\u8981\u627e\u51fa\u6700\u9ad8\u3001\u6700\u4f4e\u4ee5\u53ca\u6700\u9ad8\u548c\u6700\u4f4e\u4e4b\u95f4\u7684\u5dee\u5f02\u3002 \u8fd9\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u95ee\u9898\uff0c\u8ba9\u6211\u4eec\u5728\u4e00\u4e2a\u51fd\u6570\u4e2d\u89e3\u51b3\u5b83\uff0c\u8ba9\u60a8\u66f4\u719f\u6089\u8fd9\u4e2a\u6982\u5ff5\u3002 graph LR heights(\u5c71\u7684\u9ad8\u5ea6) --> max(\u627e\u51fa\u6700\u9ad8\u7684&nbsp&nbsp) heights --> min(\u627e\u51fa\u6700\u4f4e\u7684&nbsp&nbsp) min --> diff(\u8ba1\u7b97\u5dee\u5f02) max --> diff diff --> output(\u8f93\u51fa) subgraph Workflow max min diff end \u521b\u5efa\u670d\u52a1\u5e76\u6ce8\u518c\u6a21\u578b \u00b6 \u5c06\u4ee5\u4e0b\u4ee3\u7801\u4fdd\u5b58\u5728 app.py \u4e2d\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from typing import List from pinferencia import Server def calc ( data : List [ int ]) -> int : highest = max ( data ) lowest = min ( data ) return highest - lowest service = Server () service . register ( model_name = \"mountain\" , model = calc ) \u542f\u52a8\u670d\u52a1\u5668 \u00b6 $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) \u6d4b\u8bd5 API \u00b6 \u4f7f\u7528\u4e0b\u9762\u7684\u4ee3\u7801\u521b\u5efa\u4e00\u4e2a test.py \u3002 \u63d0\u793a You need to have requests installed. pip install requests test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/mountain/predict\" , json = { \"data\" : [ 1000 , 2000 , 3000 ]}, ) difference = response . json ()[ \"data\" ] print ( f \"Difference between the highest and lowest is { difference } m.\" ) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py Difference between the highest and lowest is 2000m. \u6b64\u5916 \u00b6 \u73b0\u5728\u4f60\u5df2\u7ecf\u5b66\u4f1a\u4e86\u5982\u4f55\u5c06\u5b9a\u4e49\u4e3a\u201c\u7c7b\u201d\u6216\u201c\u51fd\u6570\u201d\u6a21\u578b\u4e0a\u7ebf\u3002 \u5982\u679c\u60a8\u53ea\u6709\u4e00\u4e2a\u6a21\u578b\u8981\u670d\u52a1\uff0c\u90a3\u5f88\u5bb9\u6613\u3002 \u4f46\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\uff0c\u60a8\u6709\u81ea\u5b9a\u4e49\u4ee3\u7801\uff0c\u4f8b\u5982\u9884\u5904\u7406\u548c\u540e\u5904\u7406\u3002\u6709\u4e9b\u4efb\u52a1\u9700\u8981\u591a\u4e2a\u6a21\u578b\u534f\u540c\u5de5\u4f5c\u3002 \u4f8b\u5982\uff0c\u5982\u679c\u60a8\u60f3\u9884\u6d4b\u52a8\u7269\u7684\u54c1\u79cd\uff0c\u60a8\u53ef\u80fd\u9700\u8981\u4ee5\u4e0b\u5de5\u4f5c\u6d41\u7a0b\uff1a graph LR pic(\u56fe\u7247) --> species(\u7269\u79cd\u5206\u7c7b) species --> cat(Cat) --> cat_breed(\u732b\u54c1\u79cd\u5206\u7c7b&nbsp&nbsp) --> Persian(\u6ce2\u65af\u732b) species-->\u72d7(\u72d7)--> dog_breed(\u72d7\u54c1\u79cd\u5206\u7c7b&nbsp&nbsp)-->\u62c9\u5e03\u62c9\u591a(\u62c9\u5e03\u62c9\u591a) species-->\u7334\u5b50(\u7334\u5b50)-->\u7334\u5b50\u54c1\u79cd(\u7334\u5b50\u54c1\u79cd\u5206\u7c7b&nbsp&nbsp)-->\u8718\u86db(\u8718\u86db\u7334) \u5728\u8bb8\u591a\u5e73\u53f0\u6216\u5de5\u5177\u4e0a\u90e8\u7f72\u5b83\u5e76\u4e0d\u5bb9\u6613\u3002 \u4f46\u662f\uff0c\u73b0\u5728\u60a8\u62e5\u6709 Pinferencia \uff0c\u60a8\u591a\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u9009\u62e9\uff01","title":"\u542f\u52a8\u4e00\u4e2a\u51fd\u6570"},{"location":"rc/get-started/serve-a-function/#_1","text":"\u597d\u5427\uff0c\u670d\u52a1\u4e00\u4e2a\u51fd\u6570\uff1f\u6709\u7528\u5417\uff1f \u5f53\u7136\u662f\u7684\u3002 \u5982\u679c\u60a8\u6709 \u4e00\u4e2a\u5b8c\u6574\u7684\u63a8\u7406\u5de5\u4f5c\u6d41\u7a0b \uff0c\u5b83\u5305\u542b\u8bb8\u591a\u6b65\u9aa4\u3002\u5927\u591a\u6570\u65f6\u5019\uff0c\u60a8\u5c06\u5b9e\u73b0\u4e00\u4e2a\u51fd\u6570\u6765\u5b8c\u6210\u8fd9\u9879\u5de5\u4f5c\u3002\u73b0\u5728\u60a8\u53ef\u4ee5\u7acb\u5373\u6ce8\u518c\u8be5\u51fd\u6570\u3002 \u5982\u679c\u4f60\u60f3\u5206\u4eab\u4e00\u4e9b\u9884\u5904\u7406\u6216\u540e\u5904\u7406\u529f\u80fd\uff0c\u73b0\u5728\u4f60\u6709\u4f60\u7684\u7f57\u5bbe\u4e86\uff0c \u8759\u8760\u4fa0 \uff01 \u6216\u8005\u4e00\u4e2a\u51fd\u6570\u5bf9\u4f60\u7684\u5de5\u4f5c\u6765\u8bf4\u5c31\u8db3\u591f\u4e86\u3002","title":"\u542f\u52a8\u4e00\u4e2a\u51fd\u6570"},{"location":"rc/get-started/serve-a-function/#_2","text":"\u6211\u4eec\u5f97\u5230\u4e86\u4e00\u4efd\u5c71\u8109\u9ad8\u5ea6\u7684\u5217\u8868\u3002\u6211\u4eec\u9700\u8981\u627e\u51fa\u6700\u9ad8\u3001\u6700\u4f4e\u4ee5\u53ca\u6700\u9ad8\u548c\u6700\u4f4e\u4e4b\u95f4\u7684\u5dee\u5f02\u3002 \u8fd9\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u95ee\u9898\uff0c\u8ba9\u6211\u4eec\u5728\u4e00\u4e2a\u51fd\u6570\u4e2d\u89e3\u51b3\u5b83\uff0c\u8ba9\u60a8\u66f4\u719f\u6089\u8fd9\u4e2a\u6982\u5ff5\u3002 graph LR heights(\u5c71\u7684\u9ad8\u5ea6) --> max(\u627e\u51fa\u6700\u9ad8\u7684&nbsp&nbsp) heights --> min(\u627e\u51fa\u6700\u4f4e\u7684&nbsp&nbsp) min --> diff(\u8ba1\u7b97\u5dee\u5f02) max --> diff diff --> output(\u8f93\u51fa) subgraph Workflow max min diff end","title":"\u4efb\u52a1"},{"location":"rc/get-started/serve-a-function/#_3","text":"\u5c06\u4ee5\u4e0b\u4ee3\u7801\u4fdd\u5b58\u5728 app.py \u4e2d\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from typing import List from pinferencia import Server def calc ( data : List [ int ]) -> int : highest = max ( data ) lowest = min ( data ) return highest - lowest service = Server () service . register ( model_name = \"mountain\" , model = calc )","title":"\u521b\u5efa\u670d\u52a1\u5e76\u6ce8\u518c\u6a21\u578b"},{"location":"rc/get-started/serve-a-function/#_4","text":"$ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)","title":"\u542f\u52a8\u670d\u52a1\u5668"},{"location":"rc/get-started/serve-a-function/#api","text":"\u4f7f\u7528\u4e0b\u9762\u7684\u4ee3\u7801\u521b\u5efa\u4e00\u4e2a test.py \u3002 \u63d0\u793a You need to have requests installed. pip install requests test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/mountain/predict\" , json = { \"data\" : [ 1000 , 2000 , 3000 ]}, ) difference = response . json ()[ \"data\" ] print ( f \"Difference between the highest and lowest is { difference } m.\" ) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py Difference between the highest and lowest is 2000m.","title":"\u6d4b\u8bd5 API"},{"location":"rc/get-started/serve-a-function/#_5","text":"\u73b0\u5728\u4f60\u5df2\u7ecf\u5b66\u4f1a\u4e86\u5982\u4f55\u5c06\u5b9a\u4e49\u4e3a\u201c\u7c7b\u201d\u6216\u201c\u51fd\u6570\u201d\u6a21\u578b\u4e0a\u7ebf\u3002 \u5982\u679c\u60a8\u53ea\u6709\u4e00\u4e2a\u6a21\u578b\u8981\u670d\u52a1\uff0c\u90a3\u5f88\u5bb9\u6613\u3002 \u4f46\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\uff0c\u60a8\u6709\u81ea\u5b9a\u4e49\u4ee3\u7801\uff0c\u4f8b\u5982\u9884\u5904\u7406\u548c\u540e\u5904\u7406\u3002\u6709\u4e9b\u4efb\u52a1\u9700\u8981\u591a\u4e2a\u6a21\u578b\u534f\u540c\u5de5\u4f5c\u3002 \u4f8b\u5982\uff0c\u5982\u679c\u60a8\u60f3\u9884\u6d4b\u52a8\u7269\u7684\u54c1\u79cd\uff0c\u60a8\u53ef\u80fd\u9700\u8981\u4ee5\u4e0b\u5de5\u4f5c\u6d41\u7a0b\uff1a graph LR pic(\u56fe\u7247) --> species(\u7269\u79cd\u5206\u7c7b) species --> cat(Cat) --> cat_breed(\u732b\u54c1\u79cd\u5206\u7c7b&nbsp&nbsp) --> Persian(\u6ce2\u65af\u732b) species-->\u72d7(\u72d7)--> dog_breed(\u72d7\u54c1\u79cd\u5206\u7c7b&nbsp&nbsp)-->\u62c9\u5e03\u62c9\u591a(\u62c9\u5e03\u62c9\u591a) species-->\u7334\u5b50(\u7334\u5b50)-->\u7334\u5b50\u54c1\u79cd(\u7334\u5b50\u54c1\u79cd\u5206\u7c7b&nbsp&nbsp)-->\u8718\u86db(\u8718\u86db\u7334) \u5728\u8bb8\u591a\u5e73\u53f0\u6216\u5de5\u5177\u4e0a\u90e8\u7f72\u5b83\u5e76\u4e0d\u5bb9\u6613\u3002 \u4f46\u662f\uff0c\u73b0\u5728\u60a8\u62e5\u6709 Pinferencia \uff0c\u60a8\u591a\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u9009\u62e9\uff01","title":"\u6b64\u5916"},{"location":"rc/get-started/serve-a-json-model/","text":"\u542f\u52a8\u4e00\u4e2a JSON \u6a21\u578b \u00b6 \u73b0\u5728\u5148\u8ba9\u6211\u4eec\u5c1d\u8bd5\u4e00\u4e2a\u7b80\u5355\u7684\u4f8b\u5b50\uff0c\u8ba9\u4f60\u6765\u719f\u6089 Pinferecia . \u592a\u957f\u4e0d\u770b \u719f\u6089\u5982\u4f55\u901a\u8fc7 Pinferencia \u6ce8\u518c\u548c\u4e0a\u7ebf\u4e00\u4e2a\u6a21\u578b\u975e\u5e38\u91cd\u8981\u3002 \u4e0d\u8fc7\uff0c\u5982\u679c\u4f60\u60f3\u73b0\u5728\u5c31\u5c1d\u8bd5\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u4f60\u53ef\u4ee5\u79fb\u6b65 \u542f\u52a8 Pytorch MNIST Model \u5b9a\u4e49 JSON \u6a21\u578b \u00b6 \u8ba9\u6211\u4eec\u5148\u521b\u5efa\u4e00\u4e2a\u6587\u4ef6 app.py . \u4e0b\u9762\u5c31\u662f\u8fd9\u4e2a JSON \u6a21\u578b. \u8f93\u5165\u662f a \u8fd4\u56de 1 , \u8f93\u5165 b \u8fd4\u56de 2 , \u5176\u4ed6\u8f93\u5165\u8fd4\u56de 0 \u3002 app.py 1 2 3 4 class JSONModel : def predict ( self , data ): knowledge = { \"a\" : 1 , \"b\" : 2 } return knowledge . get ( data , 0 ) \u521b\u5efa\u670d\u52a1\u5e76\u6ce8\u518c\u6a21\u578b \u00b6 \u9996\u5148\u4ece pinferencia \u5bfc\u5165 Server , \u7136\u540e\u521b\u5efa\u4e00\u4e2aserver\u5b9e\u4f8b\u5e76\u6ce8\u518c JSON \u6a21\u578b . app.py 1 2 3 4 5 6 7 8 9 10 11 12 from pinferencia import Server class JSONModel : def predict ( self , data : str ) -> int : knowledge = { \"a\" : 1 , \"b\" : 2 } return knowledge . get ( data , 0 ) model = JSONModel () service = Server () service . register ( model_name = \"json\" , model = model , entrypoint = \"predict\" ) model_name \u548c entrypoint \u662f\u4ec0\u4e48\u610f\u601d? model_name \u4f60\u7ed9\u8fd9\u4e2a\u6a21\u578b\u53d6\u7684\u540d\u5b57\u3002 \u8fd9\u91cc\u6211\u4eec\u53d6\u540d json , \u5bf9\u5e94\u7684\u8fd9\u4e2a\u6a21\u578b\u7684\u5730\u5740\u5c31\u662f http://127.0.0.1:8000/v1/models/json . \u5982\u679c\u5173\u4e8eAPI\u4f60\u6709\u4ec0\u4e48\u4e0d\u6e05\u695a\u7684\uff0c\u4f60\u53ef\u4ee5\u968f\u65f6\u8bbf\u95ee\u4e0b\u9762\u5c06\u8981\u63d0\u5230\u7684\u5728\u7ebfAPI\u6587\u6863\u9875\u9762\u3002 entrypoint predict \u610f\u5473\u7740\u6211\u4eec\u4f1a\u4f7f\u7528 JSON \u6a21\u578b \u7684 predict \u51fd\u6570\u6765\u9884\u6d4b\u6570\u636e\u3002 \u542f\u52a8\u670d\u52a1 \u00b6 $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) \u6253\u5f00\u6d4f\u89c8\u5668\u8bbf\u95ee http://127.0.0.1:8000 , \u73b0\u5728\u4f60\u62e5\u6709\u4e86\u4e00\u4e2a\u81ea\u52a8\u751f\u6210\u7684 API \u6587\u6863\u9875\u9762! FastAPI and Starlette Pinferencia \u57fa\u4e8e FastAPI \uff0c\u5176\u53c8\u57fa\u4e8e Starlette . \u591a\u4e8f\u4e86\u4ed6\u4eec\uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u5e26\u6709 OpenAPI \u89c4\u8303\u7684 API\u3002\u8fd9\u610f\u5473\u7740\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u81ea\u52a8\u6587\u6863\u7f51\u9875\uff0c\u5e76\u4e14\u5ba2\u6237\u7aef\u4ee3\u7801\u4e5f\u53ef\u4ee5\u81ea\u52a8\u751f\u6210\u3002 \u63d0\u793a Pinferencia \u63d0\u4f9b\u4e86\u4e24\u4e2a API \u6587\u6863\u5730\u5740: http://127.0.0.1:8000 or http://127.0.0.1:8000/docs http://127.0.0.1:8000/redoc \u60a8\u53ef\u4ee5\u67e5\u770b API \u89c4\u8303\uff0c\u751a\u81f3\u53ef\u4ee5\u81ea\u5df1 \u8bd5\u7528 API\uff01 \u6d4b\u8bd5 API \u00b6 \u4f7f\u7528\u4e0b\u9762\u7684\u4ee3\u7801\u521b\u5efa\u4e00\u4e2a test.py \u3002 \u63d0\u793a \u4f60\u9700\u8981\u5b89\u88c5 requests . pip install requests test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/json/predict\" , json = { \"data\" : \"a\" }, ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c. $ python test.py {'model_name': 'json', 'data': 1} \u73b0\u5728\u8ba9\u6211\u4eec\u518d\u6dfb\u52a0\u4e24\u4e2a\u8f93\u5165\uff0c\u5e76\u8ba9\u6253\u5370\u66f4\u6f02\u4eae. test.py 1 2 3 4 5 6 7 8 9 10 11 import requests print ( \"| {:^10} | {:^15} |\" . format ( \"Input\" , \"Prediction\" )) print ( \"| {:^10} | {:^15} |\" . format ( \"-\" * 10 , \"-\" * 15 )) for character in [ \"a\" , \"b\" , \"c\" ]: response = requests . post ( url = \"http://localhost:8000/v1/models/json/predict\" , json = { \"data\" : character }, ) print ( f \"| { character : ^10 } | { str ( response . json ()[ 'data' ]) : ^15 } |\" ) \u518d\u6b21\u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py | Input | Prediction | |----------|---------------| | a | 1 | | b | 2 | | c | 0 |","title":"\u542f\u52a8\u4e00\u4e2a\u7b80\u5355\u7684JSON\u6a21\u578b"},{"location":"rc/get-started/serve-a-json-model/#json","text":"\u73b0\u5728\u5148\u8ba9\u6211\u4eec\u5c1d\u8bd5\u4e00\u4e2a\u7b80\u5355\u7684\u4f8b\u5b50\uff0c\u8ba9\u4f60\u6765\u719f\u6089 Pinferecia . \u592a\u957f\u4e0d\u770b \u719f\u6089\u5982\u4f55\u901a\u8fc7 Pinferencia \u6ce8\u518c\u548c\u4e0a\u7ebf\u4e00\u4e2a\u6a21\u578b\u975e\u5e38\u91cd\u8981\u3002 \u4e0d\u8fc7\uff0c\u5982\u679c\u4f60\u60f3\u73b0\u5728\u5c31\u5c1d\u8bd5\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u4f60\u53ef\u4ee5\u79fb\u6b65 \u542f\u52a8 Pytorch MNIST Model","title":"\u542f\u52a8\u4e00\u4e2a JSON \u6a21\u578b"},{"location":"rc/get-started/serve-a-json-model/#json_1","text":"\u8ba9\u6211\u4eec\u5148\u521b\u5efa\u4e00\u4e2a\u6587\u4ef6 app.py . \u4e0b\u9762\u5c31\u662f\u8fd9\u4e2a JSON \u6a21\u578b. \u8f93\u5165\u662f a \u8fd4\u56de 1 , \u8f93\u5165 b \u8fd4\u56de 2 , \u5176\u4ed6\u8f93\u5165\u8fd4\u56de 0 \u3002 app.py 1 2 3 4 class JSONModel : def predict ( self , data ): knowledge = { \"a\" : 1 , \"b\" : 2 } return knowledge . get ( data , 0 )","title":"\u5b9a\u4e49 JSON \u6a21\u578b"},{"location":"rc/get-started/serve-a-json-model/#_1","text":"\u9996\u5148\u4ece pinferencia \u5bfc\u5165 Server , \u7136\u540e\u521b\u5efa\u4e00\u4e2aserver\u5b9e\u4f8b\u5e76\u6ce8\u518c JSON \u6a21\u578b . app.py 1 2 3 4 5 6 7 8 9 10 11 12 from pinferencia import Server class JSONModel : def predict ( self , data : str ) -> int : knowledge = { \"a\" : 1 , \"b\" : 2 } return knowledge . get ( data , 0 ) model = JSONModel () service = Server () service . register ( model_name = \"json\" , model = model , entrypoint = \"predict\" ) model_name \u548c entrypoint \u662f\u4ec0\u4e48\u610f\u601d? model_name \u4f60\u7ed9\u8fd9\u4e2a\u6a21\u578b\u53d6\u7684\u540d\u5b57\u3002 \u8fd9\u91cc\u6211\u4eec\u53d6\u540d json , \u5bf9\u5e94\u7684\u8fd9\u4e2a\u6a21\u578b\u7684\u5730\u5740\u5c31\u662f http://127.0.0.1:8000/v1/models/json . \u5982\u679c\u5173\u4e8eAPI\u4f60\u6709\u4ec0\u4e48\u4e0d\u6e05\u695a\u7684\uff0c\u4f60\u53ef\u4ee5\u968f\u65f6\u8bbf\u95ee\u4e0b\u9762\u5c06\u8981\u63d0\u5230\u7684\u5728\u7ebfAPI\u6587\u6863\u9875\u9762\u3002 entrypoint predict \u610f\u5473\u7740\u6211\u4eec\u4f1a\u4f7f\u7528 JSON \u6a21\u578b \u7684 predict \u51fd\u6570\u6765\u9884\u6d4b\u6570\u636e\u3002","title":"\u521b\u5efa\u670d\u52a1\u5e76\u6ce8\u518c\u6a21\u578b"},{"location":"rc/get-started/serve-a-json-model/#_2","text":"$ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) \u6253\u5f00\u6d4f\u89c8\u5668\u8bbf\u95ee http://127.0.0.1:8000 , \u73b0\u5728\u4f60\u62e5\u6709\u4e86\u4e00\u4e2a\u81ea\u52a8\u751f\u6210\u7684 API \u6587\u6863\u9875\u9762! FastAPI and Starlette Pinferencia \u57fa\u4e8e FastAPI \uff0c\u5176\u53c8\u57fa\u4e8e Starlette . \u591a\u4e8f\u4e86\u4ed6\u4eec\uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u5e26\u6709 OpenAPI \u89c4\u8303\u7684 API\u3002\u8fd9\u610f\u5473\u7740\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u81ea\u52a8\u6587\u6863\u7f51\u9875\uff0c\u5e76\u4e14\u5ba2\u6237\u7aef\u4ee3\u7801\u4e5f\u53ef\u4ee5\u81ea\u52a8\u751f\u6210\u3002 \u63d0\u793a Pinferencia \u63d0\u4f9b\u4e86\u4e24\u4e2a API \u6587\u6863\u5730\u5740: http://127.0.0.1:8000 or http://127.0.0.1:8000/docs http://127.0.0.1:8000/redoc \u60a8\u53ef\u4ee5\u67e5\u770b API \u89c4\u8303\uff0c\u751a\u81f3\u53ef\u4ee5\u81ea\u5df1 \u8bd5\u7528 API\uff01","title":"\u542f\u52a8\u670d\u52a1"},{"location":"rc/get-started/serve-a-json-model/#api","text":"\u4f7f\u7528\u4e0b\u9762\u7684\u4ee3\u7801\u521b\u5efa\u4e00\u4e2a test.py \u3002 \u63d0\u793a \u4f60\u9700\u8981\u5b89\u88c5 requests . pip install requests test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/json/predict\" , json = { \"data\" : \"a\" }, ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c. $ python test.py {'model_name': 'json', 'data': 1} \u73b0\u5728\u8ba9\u6211\u4eec\u518d\u6dfb\u52a0\u4e24\u4e2a\u8f93\u5165\uff0c\u5e76\u8ba9\u6253\u5370\u66f4\u6f02\u4eae. test.py 1 2 3 4 5 6 7 8 9 10 11 import requests print ( \"| {:^10} | {:^15} |\" . format ( \"Input\" , \"Prediction\" )) print ( \"| {:^10} | {:^15} |\" . format ( \"-\" * 10 , \"-\" * 15 )) for character in [ \"a\" , \"b\" , \"c\" ]: response = requests . post ( url = \"http://localhost:8000/v1/models/json/predict\" , json = { \"data\" : character }, ) print ( f \"| { character : ^10 } | { str ( response . json ()[ 'data' ]) : ^15 } |\" ) \u518d\u6b21\u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py | Input | Prediction | |----------|---------------| | a | 1 | | b | 2 | | c | 0 |","title":"\u6d4b\u8bd5 API"},{"location":"rc/handlers/","text":"Handlers \u00b6 BaseHandler \u00b6 BaseHandler \u662f\u4e00\u4e2a\u62bd\u8c61\u57fa\u7840\u7c7b\uff0c\u4f60\u4e0d\u80fd\u76f4\u63a5\u7528\u5b83\u3002 \u4e0d\u8fc7\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u4e0b\u5b83\u7684\u90e8\u5206\u63a5\u53e3\uff0c\u53ef\u4ee5\u8ba9\u6211\u4eec\u62d3\u5c55\u4f7f\u7528: BaseHandler 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class BaseHandler ( abc . ABC ): def preprocess ( self , data : object , parameters : object = None ): return data # (1) def postprocess ( self , data : object , parameters : object = None ): return data # (2) def predict ( self , data : object ): if not getattr ( self , \"model\" , None ): raise Exception ( \"Model is not loaded.\" ) predict_func = ( # (3) getattr ( self . model , self . entrypoint ) if self . entrypoint else self . model ) return predict_func ( data ) @abc . abstractmethod def load_model ( self ): return NotImplemented # (4) \u9ed8\u8ba4\u4ee3\u7801\u5e76\u6ca1\u6709\u505a\u4efb\u4f55\u5904\u7406\uff0c\u4f60\u53ef\u4ee5\u5b9e\u73b0\u81ea\u5df1\u7684\u903b\u8f91\u6765\u505a pre-processing \u5de5\u4f5c\u3002 \u9ed8\u8ba4\u4ee3\u7801\u5e76\u6ca1\u6709\u505a\u4efb\u4f55\u5904\u7406\uff0c\u4f60\u53ef\u4ee5\u5b9e\u73b0\u81ea\u5df1\u7684\u903b\u8f91\u6765\u505a post-processing \u5de5\u4f5c\u3002 \u6839\u636e entrypoint \u548c model \u5bf9\u8c61\uff0c\u627e\u5230\u9884\u6d4b\u51fd\u6570\u3002\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7 self.model \u83b7\u53d6, entrypoint \u53ef\u4ee5\u901a\u8fc7 self.entrypoint \u83b7\u53d6\u3002 \u4f60\u9700\u8981\u5b9e\u73b0\u8fd9\u4e2a\u65b9\u6cd5\u3002 \u6a21\u578b\u8def\u5f84\u53ef\u4ee5\u901a\u8fc7 self.model_path \u83b7\u53d6\u3002 PickleHandler \u00b6 \u9ed8\u8ba4\u7684 handler \u662f PickleHandler . PickleHandler 1 2 3 4 5 6 7 8 class PickleHandler ( BaseHandler ): \"\"\"Pickle Handler for Models Saved through Pickle\"\"\" def load_model ( self ): if not getattr ( self , \"model_path\" , None ): raise Exception ( \"Model path not provided.\" ) with open ( self . model_path , \"rb\" ) as f : return pickle . load ( f )","title":"Handlers"},{"location":"rc/handlers/#handlers","text":"","title":"Handlers"},{"location":"rc/handlers/#basehandler","text":"BaseHandler \u662f\u4e00\u4e2a\u62bd\u8c61\u57fa\u7840\u7c7b\uff0c\u4f60\u4e0d\u80fd\u76f4\u63a5\u7528\u5b83\u3002 \u4e0d\u8fc7\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u4e0b\u5b83\u7684\u90e8\u5206\u63a5\u53e3\uff0c\u53ef\u4ee5\u8ba9\u6211\u4eec\u62d3\u5c55\u4f7f\u7528: BaseHandler 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class BaseHandler ( abc . ABC ): def preprocess ( self , data : object , parameters : object = None ): return data # (1) def postprocess ( self , data : object , parameters : object = None ): return data # (2) def predict ( self , data : object ): if not getattr ( self , \"model\" , None ): raise Exception ( \"Model is not loaded.\" ) predict_func = ( # (3) getattr ( self . model , self . entrypoint ) if self . entrypoint else self . model ) return predict_func ( data ) @abc . abstractmethod def load_model ( self ): return NotImplemented # (4) \u9ed8\u8ba4\u4ee3\u7801\u5e76\u6ca1\u6709\u505a\u4efb\u4f55\u5904\u7406\uff0c\u4f60\u53ef\u4ee5\u5b9e\u73b0\u81ea\u5df1\u7684\u903b\u8f91\u6765\u505a pre-processing \u5de5\u4f5c\u3002 \u9ed8\u8ba4\u4ee3\u7801\u5e76\u6ca1\u6709\u505a\u4efb\u4f55\u5904\u7406\uff0c\u4f60\u53ef\u4ee5\u5b9e\u73b0\u81ea\u5df1\u7684\u903b\u8f91\u6765\u505a post-processing \u5de5\u4f5c\u3002 \u6839\u636e entrypoint \u548c model \u5bf9\u8c61\uff0c\u627e\u5230\u9884\u6d4b\u51fd\u6570\u3002\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7 self.model \u83b7\u53d6, entrypoint \u53ef\u4ee5\u901a\u8fc7 self.entrypoint \u83b7\u53d6\u3002 \u4f60\u9700\u8981\u5b9e\u73b0\u8fd9\u4e2a\u65b9\u6cd5\u3002 \u6a21\u578b\u8def\u5f84\u53ef\u4ee5\u901a\u8fc7 self.model_path \u83b7\u53d6\u3002","title":"BaseHandler"},{"location":"rc/handlers/#picklehandler","text":"\u9ed8\u8ba4\u7684 handler \u662f PickleHandler . PickleHandler 1 2 3 4 5 6 7 8 class PickleHandler ( BaseHandler ): \"\"\"Pickle Handler for Models Saved through Pickle\"\"\" def load_model ( self ): if not getattr ( self , \"model_path\" , None ): raise Exception ( \"Model path not provided.\" ) with open ( self . model_path , \"rb\" ) as f : return pickle . load ( f )","title":"PickleHandler"},{"location":"rc/install/","text":"\u5b89\u88c5 Pinferencia \u00b6 \u63a8\u8350\u65b9\u5f0f \u00b6 \u63a8\u8350\u5c06 Pinferencia \u4e0e uvicorn \u540c\u65f6\u5b89\u88c5\uff0c\u8fd9\u6837\u53ef\u4ee5\u7acb\u523b\u542f\u52a8\u4f60\u7684\u670d\u52a1. $ pip install \"pinferencia[uvicorn]\" ---> 100% \u6216\u8005... \u00b6 \u4f60\u4e5f\u53ef\u4ee5\u9009\u62e9\u4ec5 Pinferencia \u3002\u7a0d\u540e\u4f60\u53ef\u4ee5\u518d\u9009\u62e9\u5176\u5b83\u7684ASGI Server\u3002 $ pip install pinferencia ---> 100%","title":"\u5b89\u88c5"},{"location":"rc/install/#pinferencia","text":"","title":"\u5b89\u88c5 Pinferencia"},{"location":"rc/install/#_1","text":"\u63a8\u8350\u5c06 Pinferencia \u4e0e uvicorn \u540c\u65f6\u5b89\u88c5\uff0c\u8fd9\u6837\u53ef\u4ee5\u7acb\u523b\u542f\u52a8\u4f60\u7684\u670d\u52a1. $ pip install \"pinferencia[uvicorn]\" ---> 100%","title":"\u63a8\u8350\u65b9\u5f0f"},{"location":"rc/install/#_2","text":"\u4f60\u4e5f\u53ef\u4ee5\u9009\u62e9\u4ec5 Pinferencia \u3002\u7a0d\u540e\u4f60\u53ef\u4ee5\u518d\u9009\u62e9\u5176\u5b83\u7684ASGI Server\u3002 $ pip install pinferencia ---> 100%","title":"\u6216\u8005..."},{"location":"rc/ml/huggingface/dependencies/","text":"\u5bf9\u4e8emac\u7528\u6237 \u00b6 \u5982\u679c\u4f60\u50cf\u6211\u4e00\u6837\u5728 M1 Mac \u4e0a\u5de5\u4f5c\uff0c\u4f60\u9700\u8981\u5b89\u88c5 cmake \u548c rust brew install cmake curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh \u5b89\u88c5\u4f9d\u8d56 \u00b6 \u60a8\u53ef\u4ee5\u4f7f\u7528 pip \u5b89\u88c5\u4f9d\u8d56\u9879\u3002 pip install tqdm boto3 requests regex sentencepiece sacremoses \u6216\u8005\u60a8\u53ef\u4ee5\u6539\u7528 docker \u6620\u50cf\uff1a docker run -it -p 8000 :8000 -v $( pwd ) :/opt/workspace huggingface/transformers-pytorch-cpu:4.18.0 bash","title":"\u5b89\u88c5\u4f9d\u8d56"},{"location":"rc/ml/huggingface/dependencies/#mac","text":"\u5982\u679c\u4f60\u50cf\u6211\u4e00\u6837\u5728 M1 Mac \u4e0a\u5de5\u4f5c\uff0c\u4f60\u9700\u8981\u5b89\u88c5 cmake \u548c rust brew install cmake curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh","title":"\u5bf9\u4e8emac\u7528\u6237"},{"location":"rc/ml/huggingface/dependencies/#_1","text":"\u60a8\u53ef\u4ee5\u4f7f\u7528 pip \u5b89\u88c5\u4f9d\u8d56\u9879\u3002 pip install tqdm boto3 requests regex sentencepiece sacremoses \u6216\u8005\u60a8\u53ef\u4ee5\u6539\u7528 docker \u6620\u50cf\uff1a docker run -it -p 8000 :8000 -v $( pwd ) :/opt/workspace huggingface/transformers-pytorch-cpu:4.18.0 bash","title":"\u5b89\u88c5\u4f9d\u8d56"},{"location":"rc/ml/huggingface/pipeline/nlp/bert/","text":"\u4f60\u4eec\u4e2d\u7684\u8bb8\u591a\u4eba\u4e00\u5b9a\u542c\u8bf4\u8fc7\u201cBert\u201d\u6216\u201ctransformers\u201d\u3002 \u4f60\u53ef\u80fd\u8fd8\u77e5\u9053huggingface\u3002 \u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u8ba9\u6211\u4eec\u4f7f\u7528\u5b83\u7684 pytorch \u8f6c\u6362\u5668\u6a21\u578b\u5e76\u901a\u8fc7 REST API \u4e3a\u5b83\u63d0\u4f9b\u670d\u52a1 \u6a21\u578b\u662f\u5982\u4f55\u5de5\u4f5c\u7684\uff1f \u00b6 \u8f93\u5165\u4e00\u4e2a\u4e0d\u5b8c\u6574\u7684\u53e5\u5b50\uff0c\u6a21\u578b\u5c06\u7ed9\u51fa\u5b83\u7684\u9884\u6d4b\uff1a \u8f93\u5165 \u8f93\u51fa Paris is the [MASK] of France. Paris is the capital of France. \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427 \u5148\u51b3\u6761\u4ef6 \u00b6 \u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879 \u670d\u52a1\u6a21\u578b \u00b6 \u5b89\u88c5 Pinferencia \u00b6 \u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[uvicorn]\" \u521b\u5efaapp.py \u00b6 \u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 from transformers import pipeline from pinferencia import Server bert = pipeline ( \"fill-mask\" , model = \"bert-base-uncased\" ) service = Server () service . register ( model_name = \"bert\" , model = lambda text : bert ( text )) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) \u6d4b\u8bd5\u670d\u52a1 \u00b6 curl Python Requests \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/bert/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"Paris is the [MASK] of France.\" }' \u54cd\u5e94 { \"model_name\":\"bert\", \"data\":\"Paris is the capital of France.\" } \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/bert/predict\" , json = { \"data\" : \"Paris is the [MASK] of France.\" }, ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py {'model_name': 'bert', 'data': 'Paris is the capital of France.'}","title":"Bert"},{"location":"rc/ml/huggingface/pipeline/nlp/bert/#_1","text":"\u8f93\u5165\u4e00\u4e2a\u4e0d\u5b8c\u6574\u7684\u53e5\u5b50\uff0c\u6a21\u578b\u5c06\u7ed9\u51fa\u5b83\u7684\u9884\u6d4b\uff1a \u8f93\u5165 \u8f93\u51fa Paris is the [MASK] of France. Paris is the capital of France. \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427","title":"\u6a21\u578b\u662f\u5982\u4f55\u5de5\u4f5c\u7684\uff1f"},{"location":"rc/ml/huggingface/pipeline/nlp/bert/#_2","text":"\u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"rc/ml/huggingface/pipeline/nlp/bert/#_3","text":"","title":"\u670d\u52a1\u6a21\u578b"},{"location":"rc/ml/huggingface/pipeline/nlp/bert/#pinferencia","text":"\u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[uvicorn]\"","title":"\u5b89\u88c5 Pinferencia"},{"location":"rc/ml/huggingface/pipeline/nlp/bert/#apppy","text":"\u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 from transformers import pipeline from pinferencia import Server bert = pipeline ( \"fill-mask\" , model = \"bert-base-uncased\" ) service = Server () service . register ( model_name = \"bert\" , model = lambda text : bert ( text )) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)","title":"\u521b\u5efaapp.py"},{"location":"rc/ml/huggingface/pipeline/nlp/bert/#_4","text":"curl Python Requests \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/bert/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"Paris is the [MASK] of France.\" }' \u54cd\u5e94 { \"model_name\":\"bert\", \"data\":\"Paris is the capital of France.\" } \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/bert/predict\" , json = { \"data\" : \"Paris is the [MASK] of France.\" }, ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py {'model_name': 'bert', 'data': 'Paris is the capital of France.'}","title":"\u6d4b\u8bd5\u670d\u52a1"},{"location":"rc/ml/huggingface/pipeline/nlp/text-generation/","text":"GPT2\u200a-\u200a\u6587\u672c\u751f\u6210\u8f6c\u6362\u5668\uff1a\u5982\u4f55\u4f7f\u7528\u548c\u542f\u52a8\u670d\u52a1 \u00b6 \u4ec0\u4e48\u662f\u6587\u672c\u751f\u6210\uff1f\u8f93\u5165\u4e00\u4e9b\u6587\u672c\uff0c\u6a21\u578b\u5c06\u9884\u6d4b\u540e\u7eed\u6587\u672c\u4f1a\u662f\u4ec0\u4e48\u3002 \u542c\u8d77\u6765\u4e0d\u9519\u3002\u4e0d\u8fc7\u4e0d\u4eb2\u81ea\u5c1d\u8bd5\u6a21\u578b\u600e\u4e48\u53ef\u80fd\u6709\u8da3\uff1f \u5982\u4f55\u4f7f\u7528 \u00b6 \u6a21\u578b\u5c06\u81ea\u52a8\u4e0b\u8f7d from transformers import pipeline , set_seed generator = pipeline ( \"text-generation\" , model = \"gpt2\" ) set_seed ( 42 ) def predict ( text ): return generator ( text , max_length = 50 , num_return_sequences = 3 ) \u5c31\u662f\u8fd9\u6837\uff01 \u8ba9\u6211\u4eec\u5c1d\u8bd5\u4e00\u4e0b\uff1a predict ( \"You look amazing today,\" ) \u7ed3\u679c\uff1a [{'generated_text': 'You look amazing today, guys. If you\\'re still in school and you still have a job where you work in the field\u2026 you\\'re going to look ridiculous by now, you\\'re going to look really ridiculous.\"\\n\\nHe turned to his friends'}, {'generated_text': 'You look amazing today, aren\\'t you?\"\\n\\nHe turned and looked at me. He had an expression that was full of worry as he looked at me. Even before he told me I\\'d have sex, he gave up after I told him'}, {'generated_text': 'You look amazing today, and look amazing in the sunset.\"\\n\\nGarry, then 33, won the London Marathon at age 15, and the World Triathlon in 2007, the two youngest Olympians to ride 100-meters. He also'}] \u8ba9\u6211\u4eec\u770b\u770b\u7b2c\u4e00\u4e2a\u7ed3\u679c\u3002 You look amazing today, guys. If you're still in school and you still have a job where you work in the field\u2026 you're going to look ridiculous by now, you're going to look really ridiculous.\" He turned to his friends \ud83e\udd23 \u8fd9\u5c31\u662f\u6211\u4eec\u8981\u627e\u7684\u4e1c\u897f\uff01\u5982\u679c\u518d\u6b21\u8fd0\u884c\u9884\u6d4b\uff0c\u6bcf\u6b21\u90fd\u4f1a\u7ed9\u51fa\u4e0d\u540c\u7684\u7ed3\u679c\u3002 \u5982\u4f55\u90e8\u7f72 \u00b6 \u5b89\u88c5 Pinferencia \u00b6 $ pip install \"pinferencia[uvicorn]\" ---> 100% \u521b\u5efa\u670d\u52a1 \u00b6 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from transformers import pipeline , set_seed from pinferencia import Server generator = pipeline ( \"text-generation\" , model = \"gpt2\" ) set_seed ( 42 ) def predict ( text ): return generator ( text , max_length = 50 , num_return_sequences = 3 ) service = Server () service . register ( model_name = \"gpt2\" , model = predict ) \u542f\u52a8\u670d\u52a1 \u00b6 $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) \u6d4b\u8bd5\u670d\u52a1 \u00b6 Curl Python requests curl -X 'POST' \\ 'http://127.0.0.1:8000/v1/models/gpt2/predict' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"id\": \"string\", \"parameters\": {}, \"data\": \"You look amazing today,\" }' \u7ed3\u679c: { \"id\" : \"string\" , \"model_name\" : \"gpt2\" , \"data\" : [ { \"generated_text\" : \"You look amazing today, I was in front of my friends. I wanted everyone to see me. But that's all. No one really cares about me in the eyes of the whole world unless I love them.\\\"\\n\\nIn a second Facebook post\" }, { \"generated_text\" : \"You look amazing today, and I know I am going to get the job done! So thank you all for all those donations, money, help, and hugs. I hope to see you again soon.\" }, { \"generated_text\" : \"You look amazing today, but I will have to wait until early June for what will go down as the first NBA championship (a thing I had been expecting). If it's not the biggest, it is also not great. Now let's look at\" } ] } test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/gpt2/predict\" , json = { \"data\" : \"You look amazing today,\" }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) \u8fd0\u884c python test.py \u5e76\u6253\u5370\u7ed3\u679c\uff1a Prediction: [ { \"generated_text\": \"You look amazing today, I was in front of my friends. I wanted everyone to see me. But that's all. No one really cares about me in the eyes of the whole world unless I love them.\\\"\\n\\nIn a second Facebook post\" }, { \"generated_text\": \"You look amazing today, and I know I am going to get the job done! So thank you all for all those donations, money, help, and hugs. I hope to see you again soon.\" }, { \"generated_text\": \"You look amazing today, but I will have to wait until early June for what will go down as the first NBA championship (a thing I had been expecting). If it's not the biggest, it is also not great. Now let's look at\" } ] \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8000 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u4ea4\u4e92\u5f0fUI\u3002 \u60a8\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u6587\u672c\u751f\u6210 - GPT2"},{"location":"rc/ml/huggingface/pipeline/nlp/text-generation/#gpt2-","text":"\u4ec0\u4e48\u662f\u6587\u672c\u751f\u6210\uff1f\u8f93\u5165\u4e00\u4e9b\u6587\u672c\uff0c\u6a21\u578b\u5c06\u9884\u6d4b\u540e\u7eed\u6587\u672c\u4f1a\u662f\u4ec0\u4e48\u3002 \u542c\u8d77\u6765\u4e0d\u9519\u3002\u4e0d\u8fc7\u4e0d\u4eb2\u81ea\u5c1d\u8bd5\u6a21\u578b\u600e\u4e48\u53ef\u80fd\u6709\u8da3\uff1f","title":"GPT2\u200a-\u200a\u6587\u672c\u751f\u6210\u8f6c\u6362\u5668\uff1a\u5982\u4f55\u4f7f\u7528\u548c\u542f\u52a8\u670d\u52a1"},{"location":"rc/ml/huggingface/pipeline/nlp/text-generation/#_1","text":"\u6a21\u578b\u5c06\u81ea\u52a8\u4e0b\u8f7d from transformers import pipeline , set_seed generator = pipeline ( \"text-generation\" , model = \"gpt2\" ) set_seed ( 42 ) def predict ( text ): return generator ( text , max_length = 50 , num_return_sequences = 3 ) \u5c31\u662f\u8fd9\u6837\uff01 \u8ba9\u6211\u4eec\u5c1d\u8bd5\u4e00\u4e0b\uff1a predict ( \"You look amazing today,\" ) \u7ed3\u679c\uff1a [{'generated_text': 'You look amazing today, guys. If you\\'re still in school and you still have a job where you work in the field\u2026 you\\'re going to look ridiculous by now, you\\'re going to look really ridiculous.\"\\n\\nHe turned to his friends'}, {'generated_text': 'You look amazing today, aren\\'t you?\"\\n\\nHe turned and looked at me. He had an expression that was full of worry as he looked at me. Even before he told me I\\'d have sex, he gave up after I told him'}, {'generated_text': 'You look amazing today, and look amazing in the sunset.\"\\n\\nGarry, then 33, won the London Marathon at age 15, and the World Triathlon in 2007, the two youngest Olympians to ride 100-meters. He also'}] \u8ba9\u6211\u4eec\u770b\u770b\u7b2c\u4e00\u4e2a\u7ed3\u679c\u3002 You look amazing today, guys. If you're still in school and you still have a job where you work in the field\u2026 you're going to look ridiculous by now, you're going to look really ridiculous.\" He turned to his friends \ud83e\udd23 \u8fd9\u5c31\u662f\u6211\u4eec\u8981\u627e\u7684\u4e1c\u897f\uff01\u5982\u679c\u518d\u6b21\u8fd0\u884c\u9884\u6d4b\uff0c\u6bcf\u6b21\u90fd\u4f1a\u7ed9\u51fa\u4e0d\u540c\u7684\u7ed3\u679c\u3002","title":"\u5982\u4f55\u4f7f\u7528"},{"location":"rc/ml/huggingface/pipeline/nlp/text-generation/#_2","text":"","title":"\u5982\u4f55\u90e8\u7f72"},{"location":"rc/ml/huggingface/pipeline/nlp/text-generation/#pinferencia","text":"$ pip install \"pinferencia[uvicorn]\" ---> 100%","title":"\u5b89\u88c5Pinferencia"},{"location":"rc/ml/huggingface/pipeline/nlp/text-generation/#_3","text":"app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from transformers import pipeline , set_seed from pinferencia import Server generator = pipeline ( \"text-generation\" , model = \"gpt2\" ) set_seed ( 42 ) def predict ( text ): return generator ( text , max_length = 50 , num_return_sequences = 3 ) service = Server () service . register ( model_name = \"gpt2\" , model = predict )","title":"\u521b\u5efa\u670d\u52a1"},{"location":"rc/ml/huggingface/pipeline/nlp/text-generation/#_4","text":"$ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)","title":"\u542f\u52a8\u670d\u52a1"},{"location":"rc/ml/huggingface/pipeline/nlp/text-generation/#_5","text":"Curl Python requests curl -X 'POST' \\ 'http://127.0.0.1:8000/v1/models/gpt2/predict' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"id\": \"string\", \"parameters\": {}, \"data\": \"You look amazing today,\" }' \u7ed3\u679c: { \"id\" : \"string\" , \"model_name\" : \"gpt2\" , \"data\" : [ { \"generated_text\" : \"You look amazing today, I was in front of my friends. I wanted everyone to see me. But that's all. No one really cares about me in the eyes of the whole world unless I love them.\\\"\\n\\nIn a second Facebook post\" }, { \"generated_text\" : \"You look amazing today, and I know I am going to get the job done! So thank you all for all those donations, money, help, and hugs. I hope to see you again soon.\" }, { \"generated_text\" : \"You look amazing today, but I will have to wait until early June for what will go down as the first NBA championship (a thing I had been expecting). If it's not the biggest, it is also not great. Now let's look at\" } ] } test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/gpt2/predict\" , json = { \"data\" : \"You look amazing today,\" }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) \u8fd0\u884c python test.py \u5e76\u6253\u5370\u7ed3\u679c\uff1a Prediction: [ { \"generated_text\": \"You look amazing today, I was in front of my friends. I wanted everyone to see me. But that's all. No one really cares about me in the eyes of the whole world unless I love them.\\\"\\n\\nIn a second Facebook post\" }, { \"generated_text\": \"You look amazing today, and I know I am going to get the job done! So thank you all for all those donations, money, help, and hugs. I hope to see you again soon.\" }, { \"generated_text\": \"You look amazing today, but I will have to wait until early June for what will go down as the first NBA championship (a thing I had been expecting). If it's not the biggest, it is also not great. Now let's look at\" } ] \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8000 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u4ea4\u4e92\u5f0fUI\u3002 \u60a8\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u6d4b\u8bd5\u670d\u52a1"},{"location":"rc/ml/huggingface/pipeline/nlp/translation/","text":"Google T5 \u7ffb\u8bd1\u5373\u670d\u52a1\uff0c\u53ea\u9700 7 \u884c\u4ee3\u7801 \u00b6 \u4ec0\u4e48\u662fT5\uff1f Google \u7684 Text-To-Text Transfer Transformer (T5) \u63d0\u4f9b\u4e86\u7ffb\u8bd1\u529f\u80fd\u3002 \u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5c06 Google T5 \u6a21\u578b\u90e8\u7f72\u4e3a REST API \u670d\u52a1\u3002 \u96be\u7684\uff1f \u6211\u544a\u8bc9\u4f60\u600e\u4e48\u6837\uff1a\u4f60\u53ea\u9700\u8981\u5199 7 \u884c\u4ee3\u7801\uff1f \u5b89\u88c5\u4f9d\u8d56 \u00b6 HuggingFace \u00b6 pip install \"transformers[pytorch]\" \u5982\u679c\u4e0d\u8d77\u4f5c\u7528\uff0c\u8bf7\u8bbf\u95ee Installation \u5e76\u67e5\u770b\u5176\u5b98\u65b9\u6587\u6863\u3002 Pinferencia \u00b6 pip install \"pinferencia[uvicorn]\" \u5b9a\u4e49\u670d\u52a1 \u00b6 \u9996\u5148\u8ba9\u6211\u4eec\u521b\u5efa app.py \u6765\u5b9a\u4e49\u670d\u52a1\uff1a app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from transformers import pipeline from pinferencia import Server t5 = pipeline ( model = \"t5-base\" , tokenizer = \"t5-base\" ) def translate ( text ): return t5 ( text ) service = Server () service . register ( model_name = \"t5\" , model = translate ) \u542f\u52a8\u670d\u52a1 \u00b6 $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) \u6d4b\u8bd5\u670d\u52a1 \u00b6 Curl Python requests curl -X 'POST' \\ 'http://localhost:8000/v1/models/t5/predict' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"parameters\": {}, \"data\": \"translate English to German: Good morning, my love.\" }' \u7ed3\u679c: { \"model_name\" : \"t5\" , \"data\" : [ { \"translation_text\" : \"Guten Morgen, liebe Liebe.\" } ] } test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/gpt2/predict\" , json = { \"data\" : \"translate English to German: Good morning, my love.\" }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) \u8fd0\u884c python test.py \u5e76\u6253\u5370\u7ed3\u679c\uff1a Prediction: { \"translation_text\": \"Guten Morgen, liebe Liebe.\" } \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8000 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u4ea4\u4e92\u5f0f ui\u3002 \u60a8\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u7ffb\u8bd1 - Google T5"},{"location":"rc/ml/huggingface/pipeline/nlp/translation/#google-t5-7","text":"\u4ec0\u4e48\u662fT5\uff1f Google \u7684 Text-To-Text Transfer Transformer (T5) \u63d0\u4f9b\u4e86\u7ffb\u8bd1\u529f\u80fd\u3002 \u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5c06 Google T5 \u6a21\u578b\u90e8\u7f72\u4e3a REST API \u670d\u52a1\u3002 \u96be\u7684\uff1f \u6211\u544a\u8bc9\u4f60\u600e\u4e48\u6837\uff1a\u4f60\u53ea\u9700\u8981\u5199 7 \u884c\u4ee3\u7801\uff1f","title":"Google T5 \u7ffb\u8bd1\u5373\u670d\u52a1\uff0c\u53ea\u9700 7 \u884c\u4ee3\u7801"},{"location":"rc/ml/huggingface/pipeline/nlp/translation/#_1","text":"","title":"\u5b89\u88c5\u4f9d\u8d56"},{"location":"rc/ml/huggingface/pipeline/nlp/translation/#huggingface","text":"pip install \"transformers[pytorch]\" \u5982\u679c\u4e0d\u8d77\u4f5c\u7528\uff0c\u8bf7\u8bbf\u95ee Installation \u5e76\u67e5\u770b\u5176\u5b98\u65b9\u6587\u6863\u3002","title":"HuggingFace"},{"location":"rc/ml/huggingface/pipeline/nlp/translation/#pinferencia","text":"pip install \"pinferencia[uvicorn]\"","title":"Pinferencia"},{"location":"rc/ml/huggingface/pipeline/nlp/translation/#_2","text":"\u9996\u5148\u8ba9\u6211\u4eec\u521b\u5efa app.py \u6765\u5b9a\u4e49\u670d\u52a1\uff1a app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from transformers import pipeline from pinferencia import Server t5 = pipeline ( model = \"t5-base\" , tokenizer = \"t5-base\" ) def translate ( text ): return t5 ( text ) service = Server () service . register ( model_name = \"t5\" , model = translate )","title":"\u5b9a\u4e49\u670d\u52a1"},{"location":"rc/ml/huggingface/pipeline/nlp/translation/#_3","text":"$ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)","title":"\u542f\u52a8\u670d\u52a1"},{"location":"rc/ml/huggingface/pipeline/nlp/translation/#_4","text":"Curl Python requests curl -X 'POST' \\ 'http://localhost:8000/v1/models/t5/predict' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"parameters\": {}, \"data\": \"translate English to German: Good morning, my love.\" }' \u7ed3\u679c: { \"model_name\" : \"t5\" , \"data\" : [ { \"translation_text\" : \"Guten Morgen, liebe Liebe.\" } ] } test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/gpt2/predict\" , json = { \"data\" : \"translate English to German: Good morning, my love.\" }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) \u8fd0\u884c python test.py \u5e76\u6253\u5370\u7ed3\u679c\uff1a Prediction: { \"translation_text\": \"Guten Morgen, liebe Liebe.\" } \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8000 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u4ea4\u4e92\u5f0f ui\u3002 \u60a8\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u6d4b\u8bd5\u670d\u52a1"},{"location":"rc/ml/huggingface/pipeline/vision/","text":"\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u63a2\u8ba8\u5982\u4f55\u4f7f\u7528 Hugging Face \u7ba1\u9053\uff0c\u4ee5\u53ca\u5982\u4f55\u4f7f\u7528 Pinferencia \u4f5c\u4e3a REST API \u90e8\u7f72\u5b83\u3002 \u5148\u51b3\u6761\u4ef6 \u00b6 \u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879 \u4e0b\u8f7d\u6a21\u578b\u5e76\u9884\u6d4b \u00b6 \u6a21\u578b\u5c06\u81ea\u52a8\u4e0b\u8f7d\u3002 1 2 3 4 5 6 from transformers import pipeline vision_classifier = pipeline ( task = \"image-classification\" ) vision_classifier ( images = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\" ) \u7ed3\u679c: [{ 'label' : 'lynx, catamount' , 'score' : 0.4403027892112732 }, { 'label' : 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor' , 'score' : 0.03433405980467796 }, { 'label' : 'snow leopard, ounce, Panthera uncia' , 'score' : 0.032148055732250214 }, { 'label' : 'Egyptian cat' , 'score' : 0.02353910356760025 }, { 'label' : 'tiger cat' , 'score' : 0.023034192621707916 }] \u8ba9\u6211\u4eec\u5c1d\u8bd5\u53e6\u4e00\u4e2a\u56fe\u50cf\uff0c\u8ba9\u6211\u4eec\u5c1d\u8bd5\u5728\u4e00\u6279\u4e2d\u9884\u6d4b\u4e24\u4e2a\u56fe\u50cf\uff1a 1 2 3 4 image = \"https://cdn.pixabay.com/photo/2018/08/12/16/59/parrot-3601194_1280.jpg\" vision_classifier ( images = [ image , image ] ) \u7ed3\u679c: [[{ 'score' : 0.9489120244979858 , 'label' : 'macaw' }, { 'score' : 0.014800671488046646 , 'label' : 'broom' }, { 'score' : 0.009150494821369648 , 'label' : 'swab, swob, mop' }, { 'score' : 0.0018255198374390602 , 'label' : \"plunger, plumber's helper\" }, { 'score' : 0.0017631321679800749 , 'label' : 'African grey, African gray, Psittacus erithacus' }], [{ 'score' : 0.9489120244979858 , 'label' : 'macaw' }, { 'score' : 0.014800671488046646 , 'label' : 'broom' }, { 'score' : 0.009150494821369648 , 'label' : 'swab, swob, mop' }, { 'score' : 0.0018255198374390602 , 'label' : \"plunger, plumber's helper\" }, { 'score' : 0.0017631321679800749 , 'label' : 'African grey, African gray, Psittacus erithacus' }]] \u51fa\u4e4e\u610f\u6599\u7684\u5bb9\u6613\uff01 \u73b0\u5728\u8ba9\u6211\u4eec\u8bd5\u8bd5\uff1a \u90e8\u7f72\u6a21\u578b \u00b6 \u6ca1\u6709\u90e8\u7f72\uff0c\u673a\u5668\u5b66\u4e60\u6559\u7a0b\u600e\u4e48\u53ef\u80fd\u5b8c\u6574\uff1f \u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[uvicorn]\" \u73b0\u5728\u8ba9\u6211\u4eec\u7528\u4ee3\u7801\u521b\u5efa\u4e00\u4e2a app.py \u6587\u4ef6\uff1a app.py 1 2 3 4 5 6 7 8 9 10 11 from transformers import pipeline from pinferencia import Server vision_classifier = pipeline ( task = \"image-classification\" ) def classify ( data ): return vision_classifier ( images = data ) service = Server () service . register ( model_name = \"vision\" , model = classify ) \u5bb9\u6613\uff0c\u5bf9\u5427\uff1f \u9884\u6d4b \u00b6 Curl Python requests curl --location --request POST 'http://127.0.0.1:8000/v1/models/vision/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"https://cdn.pixabay.com/photo/2018/08/12/16/59/parrot-3601194_1280.jpg\" }' \u7ed3\u679c: Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/vision/predict\" , json = { \"data\" : \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\" # noqa }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) \u8fd0\u884c python test.py \uff0c\u67e5\u770b\u7ed3\u679c\uff1a Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8000 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u4ea4\u4e92\u5f0f ui\u3002 \u60a8\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01 \u8fdb\u4e00\u6b65\u6539\u8fdb \u00b6 \u4f46\u662f\uff0c\u6709\u65f6\u4f7f\u7528\u56fe\u50cf\u7684 url \u6765\u9884\u6d4b\u662f\u4e0d\u5408\u9002\u7684\u3002 \u8ba9\u6211\u4eec\u7a0d\u5fae\u4fee\u6539 app.py \u4ee5\u63a5\u53d7 Base64 Encoded String \u4f5c\u4e3a\u8f93\u5165\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import base64 from io import BytesIO from PIL import Image from transformers import pipeline from pinferencia import Server vision_classifier = pipeline ( task = \"image-classification\" ) def classify ( image_base64_str ): image = Image . open ( BytesIO ( base64 . b64decode ( image_base64_str ))) return vision_classifier ( images = image ) service = Server () service . register ( model_name = \"vision\" , model = classify ) \u518d\u6b21\u9884\u6d4b \u00b6 Curl Python requests curl --location --request POST 'http://127.0.0.1:8000/v1/models/vision/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"...\" }' \u7ed3\u679c\uff1a Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/vision/predict\" , json = { \"data\" : \"...\" # noqa }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) \u8fd0\u884c python test.py \u5e76\u67e5\u770b\u7ed3\u679c\uff1a Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ]","title":"\u56fe\u50cf\u8bc6\u522b"},{"location":"rc/ml/huggingface/pipeline/vision/#_1","text":"\u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"rc/ml/huggingface/pipeline/vision/#_2","text":"\u6a21\u578b\u5c06\u81ea\u52a8\u4e0b\u8f7d\u3002 1 2 3 4 5 6 from transformers import pipeline vision_classifier = pipeline ( task = \"image-classification\" ) vision_classifier ( images = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\" ) \u7ed3\u679c: [{ 'label' : 'lynx, catamount' , 'score' : 0.4403027892112732 }, { 'label' : 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor' , 'score' : 0.03433405980467796 }, { 'label' : 'snow leopard, ounce, Panthera uncia' , 'score' : 0.032148055732250214 }, { 'label' : 'Egyptian cat' , 'score' : 0.02353910356760025 }, { 'label' : 'tiger cat' , 'score' : 0.023034192621707916 }] \u8ba9\u6211\u4eec\u5c1d\u8bd5\u53e6\u4e00\u4e2a\u56fe\u50cf\uff0c\u8ba9\u6211\u4eec\u5c1d\u8bd5\u5728\u4e00\u6279\u4e2d\u9884\u6d4b\u4e24\u4e2a\u56fe\u50cf\uff1a 1 2 3 4 image = \"https://cdn.pixabay.com/photo/2018/08/12/16/59/parrot-3601194_1280.jpg\" vision_classifier ( images = [ image , image ] ) \u7ed3\u679c: [[{ 'score' : 0.9489120244979858 , 'label' : 'macaw' }, { 'score' : 0.014800671488046646 , 'label' : 'broom' }, { 'score' : 0.009150494821369648 , 'label' : 'swab, swob, mop' }, { 'score' : 0.0018255198374390602 , 'label' : \"plunger, plumber's helper\" }, { 'score' : 0.0017631321679800749 , 'label' : 'African grey, African gray, Psittacus erithacus' }], [{ 'score' : 0.9489120244979858 , 'label' : 'macaw' }, { 'score' : 0.014800671488046646 , 'label' : 'broom' }, { 'score' : 0.009150494821369648 , 'label' : 'swab, swob, mop' }, { 'score' : 0.0018255198374390602 , 'label' : \"plunger, plumber's helper\" }, { 'score' : 0.0017631321679800749 , 'label' : 'African grey, African gray, Psittacus erithacus' }]] \u51fa\u4e4e\u610f\u6599\u7684\u5bb9\u6613\uff01 \u73b0\u5728\u8ba9\u6211\u4eec\u8bd5\u8bd5\uff1a","title":"\u4e0b\u8f7d\u6a21\u578b\u5e76\u9884\u6d4b"},{"location":"rc/ml/huggingface/pipeline/vision/#_3","text":"\u6ca1\u6709\u90e8\u7f72\uff0c\u673a\u5668\u5b66\u4e60\u6559\u7a0b\u600e\u4e48\u53ef\u80fd\u5b8c\u6574\uff1f \u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[uvicorn]\" \u73b0\u5728\u8ba9\u6211\u4eec\u7528\u4ee3\u7801\u521b\u5efa\u4e00\u4e2a app.py \u6587\u4ef6\uff1a app.py 1 2 3 4 5 6 7 8 9 10 11 from transformers import pipeline from pinferencia import Server vision_classifier = pipeline ( task = \"image-classification\" ) def classify ( data ): return vision_classifier ( images = data ) service = Server () service . register ( model_name = \"vision\" , model = classify ) \u5bb9\u6613\uff0c\u5bf9\u5427\uff1f","title":"\u90e8\u7f72\u6a21\u578b"},{"location":"rc/ml/huggingface/pipeline/vision/#_4","text":"Curl Python requests curl --location --request POST 'http://127.0.0.1:8000/v1/models/vision/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"https://cdn.pixabay.com/photo/2018/08/12/16/59/parrot-3601194_1280.jpg\" }' \u7ed3\u679c: Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/vision/predict\" , json = { \"data\" : \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\" # noqa }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) \u8fd0\u884c python test.py \uff0c\u67e5\u770b\u7ed3\u679c\uff1a Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8000 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u4ea4\u4e92\u5f0f ui\u3002 \u60a8\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u9884\u6d4b"},{"location":"rc/ml/huggingface/pipeline/vision/#_5","text":"\u4f46\u662f\uff0c\u6709\u65f6\u4f7f\u7528\u56fe\u50cf\u7684 url \u6765\u9884\u6d4b\u662f\u4e0d\u5408\u9002\u7684\u3002 \u8ba9\u6211\u4eec\u7a0d\u5fae\u4fee\u6539 app.py \u4ee5\u63a5\u53d7 Base64 Encoded String \u4f5c\u4e3a\u8f93\u5165\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import base64 from io import BytesIO from PIL import Image from transformers import pipeline from pinferencia import Server vision_classifier = pipeline ( task = \"image-classification\" ) def classify ( image_base64_str ): image = Image . open ( BytesIO ( base64 . b64decode ( image_base64_str ))) return vision_classifier ( images = image ) service = Server () service . register ( model_name = \"vision\" , model = classify )","title":"\u8fdb\u4e00\u6b65\u6539\u8fdb"},{"location":"rc/ml/huggingface/pipeline/vision/#_6","text":"Curl Python requests curl --location --request POST 'http://127.0.0.1:8000/v1/models/vision/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"...\" }' \u7ed3\u679c\uff1a Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/vision/predict\" , json = { \"data\" : \"...\" # noqa }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) \u8fd0\u884c python test.py \u5e76\u67e5\u770b\u7ed3\u679c\uff1a Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ]","title":"\u518d\u6b21\u9884\u6d4b"},{"location":"rc/models/home/","text":"\u6a21\u578b? \u00b6 \u4ec0\u4e48\u662f \u6a21\u578b ? \u6982\u62ec\u7684\u6765\u8bf4\uff0c\u6a21\u578b\u662f\u4e00\u79cd\u8ba1\u7b97\u7684\u65b9\u6cd5\u3002\u901a\u5e38\u6765\u8bf4\uff0c\u6bd4\u4e00\u4e2a\u65b9\u7a0b\u590d\u6742\u4e00\u70b9\u3002 \u90a3\u6a21\u578b\u53ef\u4ee5\u662f\u4e00\u4e2a\u6587\u4ef6\u5417\uff1f\u53ef\u4ee5\u662f\u4e00\u4e2aPython\u5bf9\u8c61\u5417\uff1f\u5f53\u7136\u3002 \u5728 Pinferencia \uff0c \u4e00\u4e2a\u6a21\u578b\u5c31\u662f\u4e00\u4e2a\u53ef\u4ee5\u88ab\u8c03\u7528\u7684\u4ee3\u7801\uff0c\u5b83\u53ef\u4ee5\u662f\u4e00\u4e2a\u51fd\u6570\uff0c\u4e5f\u53ef\u4ee5\u662f\u4e00\u4e2a\u7c7b\u7684\u5b9e\u4f8b\u3002","title":"\u5173\u4e8e\u6a21\u578b"},{"location":"rc/models/home/#_1","text":"\u4ec0\u4e48\u662f \u6a21\u578b ? \u6982\u62ec\u7684\u6765\u8bf4\uff0c\u6a21\u578b\u662f\u4e00\u79cd\u8ba1\u7b97\u7684\u65b9\u6cd5\u3002\u901a\u5e38\u6765\u8bf4\uff0c\u6bd4\u4e00\u4e2a\u65b9\u7a0b\u590d\u6742\u4e00\u70b9\u3002 \u90a3\u6a21\u578b\u53ef\u4ee5\u662f\u4e00\u4e2a\u6587\u4ef6\u5417\uff1f\u53ef\u4ee5\u662f\u4e00\u4e2aPython\u5bf9\u8c61\u5417\uff1f\u5f53\u7136\u3002 \u5728 Pinferencia \uff0c \u4e00\u4e2a\u6a21\u578b\u5c31\u662f\u4e00\u4e2a\u53ef\u4ee5\u88ab\u8c03\u7528\u7684\u4ee3\u7801\uff0c\u5b83\u53ef\u4ee5\u662f\u4e00\u4e2a\u51fd\u6570\uff0c\u4e5f\u53ef\u4ee5\u662f\u4e00\u4e2a\u7c7b\u7684\u5b9e\u4f8b\u3002","title":"\u6a21\u578b?"},{"location":"rc/models/machine-learning/","text":"\u673a\u5668\u5b66\u4e60\u6846\u67b6 \u00b6 \u4e0b\u9762\u662f\u9488\u5bf9\u4e0d\u540c\u673a\u5668\u5b66\u4e60\u6846\u67b6\u7684\u5e38\u89c1\u6a21\u578b\u8f7d\u5165\u65b9\u6cd5\uff1a Scikit-Learn PyTorch Tensorflow \u4efb\u4f55\u6a21\u578b \u4efb\u610f\u51fd\u6570 app.py import joblib from pinferencia import Server # train your model model = \"...\" # or load your model model = joblib . load ( \"/path/to/model.joblib\" ) # (1) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , # (2) ) \u66f4\u591a\u8be6\u60c5\uff0c\u8bf7\u8bbf\u95ee https://scikit-learn.org/stable/modules/model_persistence.html entrypoint \u662f model \u6267\u884c\u9884\u6d4b\u7684\u51fd\u6570\u540d\u3002 \u8fd9\u91cc\u6570\u636e\u5c06\u88ab\u53d1\u9001\u5230 predict \u51fd\u6570\uff1a model.predict(data) \u3002 app.py import torch from pinferencia import Server # train your models model = \"...\" # or load your models (1) # from state_dict model = TheModelClass ( * args , ** kwargs ) model . load_state_dict ( torch . load ( PATH )) # entire model model = torch . load ( PATH ) # torchscript model = torch . jit . load ( 'model_scripted.pt' ) model . eval () service = Server () service . register ( model_name = \"mymodel\" , model = model , ) \u66f4\u591a\u8be6\u60c5\uff0c\u8bf7\u8bbf\u95ee https://pytorch.org/tutorials/beginner/saving_loading_models.html app.py import tensorflow as tf from pinferencia import Server # train your models model = \"...\" # or load your models (1) # saved_model model = tf . keras . models . load_model ( 'saved_model/model' ) # HDF5 model = tf . keras . models . load_model ( 'model.h5' ) # from weights model = create_model () model . load_weights ( './checkpoints/my_checkpoint' ) loss , acc = model . evaluate ( test_images , test_labels , verbose = 2 ) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) \u66f4\u591a\u8be6\u60c5\uff0c\u8bf7\u8bbf\u95ee https://www.tensorflow.org/tutorials/keras/save_and_load app.py from pinferencia import Server class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) app.py from pinferencia import Server def model ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = model , )","title":"\u5176\u5b83\u673a\u5668\u5b66\u4e60\u6846\u67b6"},{"location":"rc/models/machine-learning/#_1","text":"\u4e0b\u9762\u662f\u9488\u5bf9\u4e0d\u540c\u673a\u5668\u5b66\u4e60\u6846\u67b6\u7684\u5e38\u89c1\u6a21\u578b\u8f7d\u5165\u65b9\u6cd5\uff1a Scikit-Learn PyTorch Tensorflow \u4efb\u4f55\u6a21\u578b \u4efb\u610f\u51fd\u6570 app.py import joblib from pinferencia import Server # train your model model = \"...\" # or load your model model = joblib . load ( \"/path/to/model.joblib\" ) # (1) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , # (2) ) \u66f4\u591a\u8be6\u60c5\uff0c\u8bf7\u8bbf\u95ee https://scikit-learn.org/stable/modules/model_persistence.html entrypoint \u662f model \u6267\u884c\u9884\u6d4b\u7684\u51fd\u6570\u540d\u3002 \u8fd9\u91cc\u6570\u636e\u5c06\u88ab\u53d1\u9001\u5230 predict \u51fd\u6570\uff1a model.predict(data) \u3002 app.py import torch from pinferencia import Server # train your models model = \"...\" # or load your models (1) # from state_dict model = TheModelClass ( * args , ** kwargs ) model . load_state_dict ( torch . load ( PATH )) # entire model model = torch . load ( PATH ) # torchscript model = torch . jit . load ( 'model_scripted.pt' ) model . eval () service = Server () service . register ( model_name = \"mymodel\" , model = model , ) \u66f4\u591a\u8be6\u60c5\uff0c\u8bf7\u8bbf\u95ee https://pytorch.org/tutorials/beginner/saving_loading_models.html app.py import tensorflow as tf from pinferencia import Server # train your models model = \"...\" # or load your models (1) # saved_model model = tf . keras . models . load_model ( 'saved_model/model' ) # HDF5 model = tf . keras . models . load_model ( 'model.h5' ) # from weights model = create_model () model . load_weights ( './checkpoints/my_checkpoint' ) loss , acc = model . evaluate ( test_images , test_labels , verbose = 2 ) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) \u66f4\u591a\u8be6\u60c5\uff0c\u8bf7\u8bbf\u95ee https://www.tensorflow.org/tutorials/keras/save_and_load app.py from pinferencia import Server class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) app.py from pinferencia import Server def model ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = model , )","title":"\u673a\u5668\u5b66\u4e60\u6846\u67b6"},{"location":"rc/models/register/","text":"\u6ce8\u518c\u6a21\u578b \u00b6 \u6ce8\u518c\u4e00\u4e2a\u6a21\u578b\u975e\u5e38\u7b80\u5355: 1 2 3 4 5 service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) \u5982\u679c\u6211\u6709\u591a\u4e2a\u6a21\u578b\uff0c\u6216\u8005\u6709\u591a\u4e2a\u7248\u672c\u5462? \u4f60\u53ef\u4ee5\u6ce8\u518c\u591a\u4e2a\u6a21\u578b\uff0c\u6bcf\u4e2a\u6a21\u578b\u53ef\u4ee5\u6709\u4e0d\u540c\u7684\u7248\u672c: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 service . register ( model_name = \"my-model\" , model = my_model , entrypoint = \"predict\" , ) service . register ( model_name = \"my-model\" , model = my_model_v1 , entrypoint = \"predict\" , version_name = \"v1, ) service . register ( model_name = \"your-model\" , model = your_model , entrypoint = \"predict\" , ) service . register ( model_name = \"your-model\" , model = your_model_v1 , entrypoint = \"predict\" , version_name = \"v1, ) service . register ( model_name = \"your-model\" , model = your_model_v2 , entrypoint = \"predict\" , version_name = \"v2, ) \u53c2\u6570 \u00b6 \u53c2\u6570 \u7c7b\u4f3c \u9ed8\u8ba4\u503c\uff08\u5982\u6709\uff09 \u7ec6\u8282 model_name str \u6a21\u578b\u540d\u79f0 model object \u6a21\u578bPython\u5bf9\u8c61\uff0c\u6216\u8005\u6a21\u578b\u6587\u4ef6\u8def\u5f84 version_name str None \u7248\u672c\u540d\u79f0 entrypoint str None \u7528\u6765\u9884\u6d4b\u7684\u51fd\u6570\u540d\u79f0 metadata dict None \u6a21\u578b\u57fa\u7840\u4fe1\u606f handler object None Hanlder \u7c7b load_now bool True \u662f\u5426\u7acb\u523b\u8f7d\u5165\u6a21\u578b \u4f8b\u5b50 \u00b6 Model Name \u00b6 1 2 3 4 5 6 7 8 9 10 11 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , ) Model \u00b6 Model Object Function 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from pinferencia import Server class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict ) 1 2 3 4 5 6 7 8 9 10 11 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , ) Version\u540d\u79f0 \u00b6 \u6ca1\u6709\u63d0\u4f9b\u7248\u672c\u540d\u7684\u6a21\u578b\u4f1a\u7528 default \u7248\u672c\u540d. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pinferencia import Server def add ( data ): return data [ 0 ] + data [ 1 ] def substract ( data ): return data [ 0 ] + data [ 1 ] service = Server () service . register ( model_name = \"mymodel\" , model = add , version_name = \"add\" , # (1) ) service . register ( model_name = \"mymodel\" , model = substract , version_name = \"substract\" , # (2) ) \u9884\u6d4b\u5730\u5740\u5728 http://127.0.0.1/v1/models/mymodel/versions/add/predict \u9884\u6d4b\u5730\u5740\u5728 http://127.0.0.1/v1/models/mymodel/versions/substract/predict Entrypoint \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pinferencia import Server class MyModel : def add ( self , data ): return data [ 0 ] + data [ 1 ] def substract ( self , data ): return data [ 0 ] - data [ 1 ] model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , version_name = \"add\" , # (1) entrypoint = \"add\" , # (3) ) service . register ( model_name = \"mymodel\" , model = model , version_name = \"substract\" , # (2) entrypoint = \"substract\" , # (4) ) \u9884\u6d4b\u5730\u5740\u5728 http://127.0.0.1/v1/models/mymodel/versions/add/predict \u9884\u6d4b\u5730\u5740\u5728 http://127.0.0.1/v1/models/mymodel/versions/substract/predict add \u51fd\u6570\u4f1a\u88ab\u7528\u6765\u9884\u6d4b. substract \u51fd\u6570\u4f1a\u88ab\u7528\u6765\u9884\u6d4b. Metadata \u00b6 \u9ed8\u8ba4API \u00b6 Pinferencia \u9ed8\u8ba4metadata\u67b6\u6784\u652f\u6301 platform \u548c device \u8fd9\u4e9b\u4fe1\u606f\u4ec5\u4f9b\u5c55\u793a\u3002 These are information for display purpose only. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , metadata = { \"platform\" : \"Linux\" , \"device\" : \"CPU+GPU\" , } ) Kserve API \u00b6 Pinferencia \u540c\u65f6\u652f\u6301 Kserve API. \u5bf9\u4e8e Kserve V2, \u6a21\u578bmetadata\u652f\u6301: - platform - inputs - outputs inputs \u548c outputs \u4f1a\u51b3\u5b9a\u6a21\u578b\u6536\u5230\u7684\u6570\u636e\u548c\u8fd4\u56de\u7684\u6570\u636e\u7c7b\u578b. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server ( api = \"kserve\" ) # (1) service . register ( model_name = \"mymodel\" , model = predict , metadata = { \"platform\" : \"mac os\" , \"inputs\" : [ { \"name\" : \"integers\" , # (2) \"datatype\" : \"int64\" , \"shape\" : [ 1 ], \"data\" : [ 1 , 2 , 3 ], } ], \"outputs\" : [ { \"name\" : \"sum\" , \"datatype\" : \"int64\" , \"shape\" : - 1 , \"data\" : 6 }, # (3) { \"name\" : \"product\" , \"datatype\" : \"int64\" , \"shape\" : - 1 , \"data\" : 6 }, ], } ) \u5982\u679c\u8981\u4f7f\u7528 Kserve API \u9700\u8981\u5728\u5b9e\u4f8b\u5316\u670d\u52a1\u65f6\u8bbe\u7f6e api=\"kserve\"\u3002 \u5982\u679c\u8bf7\u6c42\u5305\u542b\u591a\u7ec4\u6570\u636e\uff0c\u53ea\u6709 intergers \u6570\u636e\u4f1a\u88ab\u4f20\u9012\u7ed9\u6a21\u578b\u3002 \u8f93\u51fa\u6570\u636e\u4f1a\u88ab\u8f6c\u6362\u4e3a int64 \u3002 datatype \u5b57\u6bb5\u4ec5\u652f\u6301 numpy \u6570\u636e\u7c7b\u578b. \u5982\u679c\u7c7b\u578b\u8f6c\u6362\u5931\u8d25\uff0c\u54cd\u5e94\u91cc\u4f1a\u591a\u51fa error \u5b57\u6bb5\u3002 Handler \u00b6 \u5173\u4e8eHandler\u7684\u7ec6\u8282\uff0c\u8bf7\u67e5\u770b Handlers . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pinferencia import Server from pinferencia.handlers import PickleHandler class MyPrintHandler ( PickleHandler ): def predict ( self , data ): print ( data ) return self . model . predict ( data ) def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , handler = MyPrintHandler ) Load Now \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import joblib from pinferencia import Server class JoblibHandler ( BaseHandler ): def load_model ( self ): return joblib . load ( self . model_path ) service = Server ( model_dir = \"/opt/models\" ) service . register ( model_name = \"mymodel\" , model = \"/path/to/model.joblib\" , entrypoint = \"predict\" , handler = JoblibHandler , load_now = True , )","title":"\u6ce8\u518c\u6a21\u578b"},{"location":"rc/models/register/#_1","text":"\u6ce8\u518c\u4e00\u4e2a\u6a21\u578b\u975e\u5e38\u7b80\u5355: 1 2 3 4 5 service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) \u5982\u679c\u6211\u6709\u591a\u4e2a\u6a21\u578b\uff0c\u6216\u8005\u6709\u591a\u4e2a\u7248\u672c\u5462? \u4f60\u53ef\u4ee5\u6ce8\u518c\u591a\u4e2a\u6a21\u578b\uff0c\u6bcf\u4e2a\u6a21\u578b\u53ef\u4ee5\u6709\u4e0d\u540c\u7684\u7248\u672c: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 service . register ( model_name = \"my-model\" , model = my_model , entrypoint = \"predict\" , ) service . register ( model_name = \"my-model\" , model = my_model_v1 , entrypoint = \"predict\" , version_name = \"v1, ) service . register ( model_name = \"your-model\" , model = your_model , entrypoint = \"predict\" , ) service . register ( model_name = \"your-model\" , model = your_model_v1 , entrypoint = \"predict\" , version_name = \"v1, ) service . register ( model_name = \"your-model\" , model = your_model_v2 , entrypoint = \"predict\" , version_name = \"v2, )","title":"\u6ce8\u518c\u6a21\u578b"},{"location":"rc/models/register/#_2","text":"\u53c2\u6570 \u7c7b\u4f3c \u9ed8\u8ba4\u503c\uff08\u5982\u6709\uff09 \u7ec6\u8282 model_name str \u6a21\u578b\u540d\u79f0 model object \u6a21\u578bPython\u5bf9\u8c61\uff0c\u6216\u8005\u6a21\u578b\u6587\u4ef6\u8def\u5f84 version_name str None \u7248\u672c\u540d\u79f0 entrypoint str None \u7528\u6765\u9884\u6d4b\u7684\u51fd\u6570\u540d\u79f0 metadata dict None \u6a21\u578b\u57fa\u7840\u4fe1\u606f handler object None Hanlder \u7c7b load_now bool True \u662f\u5426\u7acb\u523b\u8f7d\u5165\u6a21\u578b","title":"\u53c2\u6570"},{"location":"rc/models/register/#_3","text":"","title":"\u4f8b\u5b50"},{"location":"rc/models/register/#model-name","text":"1 2 3 4 5 6 7 8 9 10 11 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , )","title":"Model Name"},{"location":"rc/models/register/#model","text":"Model Object Function 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from pinferencia import Server class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict ) 1 2 3 4 5 6 7 8 9 10 11 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , )","title":"Model"},{"location":"rc/models/register/#version","text":"\u6ca1\u6709\u63d0\u4f9b\u7248\u672c\u540d\u7684\u6a21\u578b\u4f1a\u7528 default \u7248\u672c\u540d. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pinferencia import Server def add ( data ): return data [ 0 ] + data [ 1 ] def substract ( data ): return data [ 0 ] + data [ 1 ] service = Server () service . register ( model_name = \"mymodel\" , model = add , version_name = \"add\" , # (1) ) service . register ( model_name = \"mymodel\" , model = substract , version_name = \"substract\" , # (2) ) \u9884\u6d4b\u5730\u5740\u5728 http://127.0.0.1/v1/models/mymodel/versions/add/predict \u9884\u6d4b\u5730\u5740\u5728 http://127.0.0.1/v1/models/mymodel/versions/substract/predict","title":"Version\u540d\u79f0"},{"location":"rc/models/register/#entrypoint","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pinferencia import Server class MyModel : def add ( self , data ): return data [ 0 ] + data [ 1 ] def substract ( self , data ): return data [ 0 ] - data [ 1 ] model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , version_name = \"add\" , # (1) entrypoint = \"add\" , # (3) ) service . register ( model_name = \"mymodel\" , model = model , version_name = \"substract\" , # (2) entrypoint = \"substract\" , # (4) ) \u9884\u6d4b\u5730\u5740\u5728 http://127.0.0.1/v1/models/mymodel/versions/add/predict \u9884\u6d4b\u5730\u5740\u5728 http://127.0.0.1/v1/models/mymodel/versions/substract/predict add \u51fd\u6570\u4f1a\u88ab\u7528\u6765\u9884\u6d4b. substract \u51fd\u6570\u4f1a\u88ab\u7528\u6765\u9884\u6d4b.","title":"Entrypoint"},{"location":"rc/models/register/#metadata","text":"","title":"Metadata"},{"location":"rc/models/register/#api","text":"Pinferencia \u9ed8\u8ba4metadata\u67b6\u6784\u652f\u6301 platform \u548c device \u8fd9\u4e9b\u4fe1\u606f\u4ec5\u4f9b\u5c55\u793a\u3002 These are information for display purpose only. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , metadata = { \"platform\" : \"Linux\" , \"device\" : \"CPU+GPU\" , } )","title":"\u9ed8\u8ba4API"},{"location":"rc/models/register/#kserve-api","text":"Pinferencia \u540c\u65f6\u652f\u6301 Kserve API. \u5bf9\u4e8e Kserve V2, \u6a21\u578bmetadata\u652f\u6301: - platform - inputs - outputs inputs \u548c outputs \u4f1a\u51b3\u5b9a\u6a21\u578b\u6536\u5230\u7684\u6570\u636e\u548c\u8fd4\u56de\u7684\u6570\u636e\u7c7b\u578b. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server ( api = \"kserve\" ) # (1) service . register ( model_name = \"mymodel\" , model = predict , metadata = { \"platform\" : \"mac os\" , \"inputs\" : [ { \"name\" : \"integers\" , # (2) \"datatype\" : \"int64\" , \"shape\" : [ 1 ], \"data\" : [ 1 , 2 , 3 ], } ], \"outputs\" : [ { \"name\" : \"sum\" , \"datatype\" : \"int64\" , \"shape\" : - 1 , \"data\" : 6 }, # (3) { \"name\" : \"product\" , \"datatype\" : \"int64\" , \"shape\" : - 1 , \"data\" : 6 }, ], } ) \u5982\u679c\u8981\u4f7f\u7528 Kserve API \u9700\u8981\u5728\u5b9e\u4f8b\u5316\u670d\u52a1\u65f6\u8bbe\u7f6e api=\"kserve\"\u3002 \u5982\u679c\u8bf7\u6c42\u5305\u542b\u591a\u7ec4\u6570\u636e\uff0c\u53ea\u6709 intergers \u6570\u636e\u4f1a\u88ab\u4f20\u9012\u7ed9\u6a21\u578b\u3002 \u8f93\u51fa\u6570\u636e\u4f1a\u88ab\u8f6c\u6362\u4e3a int64 \u3002 datatype \u5b57\u6bb5\u4ec5\u652f\u6301 numpy \u6570\u636e\u7c7b\u578b. \u5982\u679c\u7c7b\u578b\u8f6c\u6362\u5931\u8d25\uff0c\u54cd\u5e94\u91cc\u4f1a\u591a\u51fa error \u5b57\u6bb5\u3002","title":"Kserve API"},{"location":"rc/models/register/#handler","text":"\u5173\u4e8eHandler\u7684\u7ec6\u8282\uff0c\u8bf7\u67e5\u770b Handlers . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pinferencia import Server from pinferencia.handlers import PickleHandler class MyPrintHandler ( PickleHandler ): def predict ( self , data ): print ( data ) return self . model . predict ( data ) def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , handler = MyPrintHandler )","title":"Handler"},{"location":"rc/models/register/#load-now","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import joblib from pinferencia import Server class JoblibHandler ( BaseHandler ): def load_model ( self ): return joblib . load ( self . model_path ) service = Server ( model_dir = \"/opt/models\" ) service . register ( model_name = \"mymodel\" , model = \"/path/to/model.joblib\" , entrypoint = \"predict\" , handler = JoblibHandler , load_now = True , )","title":"Load Now"},{"location":"rc/overview/","text":"\u6b22\u8fce\u4f7f\u7528Pinferencia \u00b6 Pinferencia? \u00b6 \u6ca1\u542c\u8bf4\u8fc7 Pinferencia \uff0c\u8fd9\u4e0d\u662f\u4f60\u7684\u9519\u3002\u4e3b\u8981\u6211\u7684\u5ba3\u4f20\u7ecf\u8d39\uff0c\u5b9e\u5728\u662f\u4e0d\u591f\u591a\u3002 \u4f60\u662f\u4e0d\u662f\u8bad\u7ec3\u4e86\u4e00\u5806\u6a21\u578b\uff0c\u7136\u800c\u522b\u4eba\u8c01\u7528\u90fd\u4e0d\u884c\u3002\u4e0d\u662f\u73af\u5883\u641e\u4e0d\u5b9a\uff0c\u5c31\u662fbug\u547d\u592a\u786c\u3002 \u4f60\u60f3: \u8981\u662f\u6211\u80fd\u6709\u4e2aAPI\uff0c\u8c01\u80fd\u4e0d\u9677\u5165\u6211\u7684\u7231\u3002\u4e0d\u7528\u5b89\u88c5\u4e0d\u7528\u7b49\u5f85\uff0c\u53d1\u4e2a\u8bf7\u6c42\u7ed3\u679c\u81ea\u5df1\u5230\u6765\u3002 \u53ef\u662f\u4e16\u4e0aAPI\u5343\u767e\u4e07\uff0c\u5374\u6ca1\u6709\u54ea\u4e2a\u6211\u80fd\u73a9\u5f97\u8f6c\u3002\u7528\u6765\u7528\u53bb\uff0c\u770b\u6765\u8fd8\u662f\u6211\u5fc3\u592a\u8f6f\uff0c\u6709\u4e9b\u4ea7\u54c1\u771f\u7684\u4e0d\u80fd\u60ef\u3002 \u6211\u591a\u60f3\u8fd9\u4e2a\u4e16\u754c\u53d8\u5f97\u7b80\u5355\uff0c\u6211\u7684\u6a21\u578b1\u5206\u949f\u5c31\u80fd\u4e0a\u7ebf\u3002\u7136\u800c\u73b0\u5b9e\u8fd9\u4e48\u6b8b\u9177\uff0c\u4e00\u5929\u4e24\u5929\u8fc7\u53bb\uff0c\u6211\u7684\u773c\u6cea\u54d7\u54d7\u6b62\u4e0d\u4f4f\u3002 \u5230\u5e95\u8c01\u80fd\u7ed9\u4e88\u6211\u8fd9\u4e2a\u6069\u8d50\u554a\uff0c\u770b\u6765\u53ea\u6709Pinferencia\u3002 $ pip install \"pinferencia[uvicorn]\" ---> 100% High\u8d77\u6765\uff01 \u00b6 \u6b22\u6b22\u4e50\u4e50\uff0c\u641e\u5b9aApp \u00b6 Scikit-Learn PyTorch Tensorflow HuggingFace Transformer Any Model Any Function app.py import joblib import uvicorn from pinferencia import Server # train your model model = \"...\" # or load your model model = joblib . load ( \"/path/to/model.joblib\" ) # (1) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , # (2) ) For more details, please visit https://scikit-learn.org/stable/modules/model_persistence.html entrypoint is the function name of the model to perform predictions. Here the data will be sent to the predict function: model.predict(data) . app.py import torch import uvicorn from pinferencia import Server # train your models model = \"...\" # or load your models (1) # from state_dict model = TheModelClass ( * args , ** kwargs ) model . load_state_dict ( torch . load ( PATH )) # entire model model = torch . load ( PATH ) # torchscript model = torch . jit . load ( 'model_scripted.pt' ) model . eval () service = Server () service . register ( model_name = \"mymodel\" , model = model , ) For more details, please visit https://pytorch.org/tutorials/beginner/saving_loading_models.html app.py import tensorflow as tf import uvicorn from pinferencia import Server # train your models model = \"...\" # or load your models (1) # saved_model model = tf . keras . models . load_model ( 'saved_model/model' ) # HDF5 model = tf . keras . models . load_model ( 'model.h5' ) # from weights model = create_model () model . load_weights ( './checkpoints/my_checkpoint' ) loss , acc = model . evaluate ( test_images , test_labels , verbose = 2 ) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) For more details, please visit https://www.tensorflow.org/tutorials/keras/save_and_load app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from transformers import pipeline from pinferencia import Server vision_classifier = pipeline ( task = \"image-classification\" ) def predict ( data ): return vision_classifier ( images = data ) service = Server () service . register ( model_name = \"vision\" , model = predict ) app.py import uvicorn from pinferencia import Server # train your models class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) app.py import uvicorn from pinferencia import Server # train your models def model ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = model , ) \u8d70\u4e00\u4e2a\uff5e \u00b6 $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)","title":"\u6982\u8ff0"},{"location":"rc/overview/#pinferencia","text":"","title":"\u6b22\u8fce\u4f7f\u7528Pinferencia"},{"location":"rc/overview/#pinferencia_1","text":"\u6ca1\u542c\u8bf4\u8fc7 Pinferencia \uff0c\u8fd9\u4e0d\u662f\u4f60\u7684\u9519\u3002\u4e3b\u8981\u6211\u7684\u5ba3\u4f20\u7ecf\u8d39\uff0c\u5b9e\u5728\u662f\u4e0d\u591f\u591a\u3002 \u4f60\u662f\u4e0d\u662f\u8bad\u7ec3\u4e86\u4e00\u5806\u6a21\u578b\uff0c\u7136\u800c\u522b\u4eba\u8c01\u7528\u90fd\u4e0d\u884c\u3002\u4e0d\u662f\u73af\u5883\u641e\u4e0d\u5b9a\uff0c\u5c31\u662fbug\u547d\u592a\u786c\u3002 \u4f60\u60f3: \u8981\u662f\u6211\u80fd\u6709\u4e2aAPI\uff0c\u8c01\u80fd\u4e0d\u9677\u5165\u6211\u7684\u7231\u3002\u4e0d\u7528\u5b89\u88c5\u4e0d\u7528\u7b49\u5f85\uff0c\u53d1\u4e2a\u8bf7\u6c42\u7ed3\u679c\u81ea\u5df1\u5230\u6765\u3002 \u53ef\u662f\u4e16\u4e0aAPI\u5343\u767e\u4e07\uff0c\u5374\u6ca1\u6709\u54ea\u4e2a\u6211\u80fd\u73a9\u5f97\u8f6c\u3002\u7528\u6765\u7528\u53bb\uff0c\u770b\u6765\u8fd8\u662f\u6211\u5fc3\u592a\u8f6f\uff0c\u6709\u4e9b\u4ea7\u54c1\u771f\u7684\u4e0d\u80fd\u60ef\u3002 \u6211\u591a\u60f3\u8fd9\u4e2a\u4e16\u754c\u53d8\u5f97\u7b80\u5355\uff0c\u6211\u7684\u6a21\u578b1\u5206\u949f\u5c31\u80fd\u4e0a\u7ebf\u3002\u7136\u800c\u73b0\u5b9e\u8fd9\u4e48\u6b8b\u9177\uff0c\u4e00\u5929\u4e24\u5929\u8fc7\u53bb\uff0c\u6211\u7684\u773c\u6cea\u54d7\u54d7\u6b62\u4e0d\u4f4f\u3002 \u5230\u5e95\u8c01\u80fd\u7ed9\u4e88\u6211\u8fd9\u4e2a\u6069\u8d50\u554a\uff0c\u770b\u6765\u53ea\u6709Pinferencia\u3002 $ pip install \"pinferencia[uvicorn]\" ---> 100%","title":"Pinferencia?"},{"location":"rc/overview/#high","text":"","title":"High\u8d77\u6765\uff01"},{"location":"rc/overview/#app","text":"Scikit-Learn PyTorch Tensorflow HuggingFace Transformer Any Model Any Function app.py import joblib import uvicorn from pinferencia import Server # train your model model = \"...\" # or load your model model = joblib . load ( \"/path/to/model.joblib\" ) # (1) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , # (2) ) For more details, please visit https://scikit-learn.org/stable/modules/model_persistence.html entrypoint is the function name of the model to perform predictions. Here the data will be sent to the predict function: model.predict(data) . app.py import torch import uvicorn from pinferencia import Server # train your models model = \"...\" # or load your models (1) # from state_dict model = TheModelClass ( * args , ** kwargs ) model . load_state_dict ( torch . load ( PATH )) # entire model model = torch . load ( PATH ) # torchscript model = torch . jit . load ( 'model_scripted.pt' ) model . eval () service = Server () service . register ( model_name = \"mymodel\" , model = model , ) For more details, please visit https://pytorch.org/tutorials/beginner/saving_loading_models.html app.py import tensorflow as tf import uvicorn from pinferencia import Server # train your models model = \"...\" # or load your models (1) # saved_model model = tf . keras . models . load_model ( 'saved_model/model' ) # HDF5 model = tf . keras . models . load_model ( 'model.h5' ) # from weights model = create_model () model . load_weights ( './checkpoints/my_checkpoint' ) loss , acc = model . evaluate ( test_images , test_labels , verbose = 2 ) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) For more details, please visit https://www.tensorflow.org/tutorials/keras/save_and_load app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from transformers import pipeline from pinferencia import Server vision_classifier = pipeline ( task = \"image-classification\" ) def predict ( data ): return vision_classifier ( images = data ) service = Server () service . register ( model_name = \"vision\" , model = predict ) app.py import uvicorn from pinferencia import Server # train your models class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) app.py import uvicorn from pinferencia import Server # train your models def model ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = model , )","title":"\u6b22\u6b22\u4e50\u4e50\uff0c\u641e\u5b9aApp"},{"location":"rc/overview/#_1","text":"$ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)","title":"\u8d70\u4e00\u4e2a\uff5e"},{"location":"rc/pinferencia-is-different/","text":"Pinferencia \u6709\u4ec0\u4e48\u4e0d\u540c? \u00b6 \u4e0d\u540c? \u00b6 \u66f4\u51c6\u786e\u7684\u8bf4\uff0c\u4e0d\u540c\u5e76\u4e0d\u91cd\u8981\uff0c\u91cd\u8981\u7684\u662f\u66f4\u76f4\u63a5\uff0c\u66f4\u7b80\u5355\uff0c\u66f4\u7b26\u5408\u4f60\u7684\u9884\u671f\u3002 \u4f60\u73b0\u5728\u662f\u5982\u4f55\u4e0a\u7ebf\u6a21\u578b\u7684? \u4f60\u662f\u4e0d\u662f\u82b1\u4e86\u5f88\u591a\u65f6\u95f4\uff0c\u5199\u4ee3\u7801\uff0c\u4fdd\u5b58\u6587\u4ef6\uff0c\u4e3a\u4e86\u6ee1\u8db3\u90a3\u4e9b\u90e8\u7f72\u5de5\u5177\u7684\u8981\u6c42\u3002 \u5bf9\uff0c\u4f60\u8fd8\u82b1\u4e86\u5f88\u591a\u65f6\u95f4\u53bb\u7406\u89e3\u8fd9\u4e9b\u8981\u6c42\uff0c\u5f88\u591a\u65f6\u95f4\u77e5\u9053\u600e\u4e48\u505a\u662f\u6b63\u786e\u7684\u3002 \u4e0d\u8fc7\uff0c\u529f\u592b\u4e0d\u8d1f\u6709\u5fc3\u4eba\uff0c\u4f60\u8fd8\u662f\u641e\u5b9a\u4e86\u3002 \u597d\u666f\u4e0d\u957f\uff0c\u8fc7\u4e86\u5927\u534a\u5e74\uff0c\u53c8\u6709\u65b0\u7684\uff0c\u66f4\u590d\u6742\u7684\u6a21\u578b\u8981\u90e8\u7f72\uff0c\u5929\u554a\uff0c\u600e\u4e48\u529e\uff1f \u4f60\u73b0\u5728\u5728\u60f3\u4ec0\u4e48? \u4e0d\u8981\u554a\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01 \u6a21\u578b\u5728\u4f60\u624b\u91cc\uff0c\u4f60\u7528 Python \u8bad\u7ec3\uff0c\u7528 Python \u9884\u6d4b\uff0c\u751a\u81f3\u5199\u4e86\u5f88\u591a\u590d\u6742\u7684\u4ee3\u7801\uff0c\u53bb\u89e3\u51b3\u56f0\u96be\u53c8\u72ec\u7279\u7684\u9700\u6c42\u3002 \u800c\u5982\u4eca\uff0c\u4f60\u53c8\u8981\u591a\u5199\u591a\u5c11\u4ee3\u7801\uff0c\u591a\u505a\u591a\u5c11\u6539\u53d8\uff0c\u624d\u80fd\u8ba9\u4f60\u7684\u6a21\u578b\uff0c\u7528\u8fd9\u4e9b\u5de5\u5177\u6216\u8005\u5e73\u53f0\uff0c\u4ec5\u4ec5\u662f\u542f\u52a8\u4e00\u4e2aAPI\uff1f \u7b54\u6848\u662f\uff1a \u6570\u4e0d\u80dc\u6570 . \u6709\u4e86 Pinferencia \u00b6 \u4f60\u4e0d\u7528\u518d\u62c5\u5fc3\u8fd9\u4e9b\u3002\u4f60\u53ea\u9700\u8981\u8fd8\u662f\u7528\u4f60\u81ea\u5df1\u7684\u6a21\u578b\uff0c\u8fd8\u662f\u7528\u4f60\u81ea\u5df1\u7684\u4ee3\u7801\u3002 \u65e0\u6240\u8c13\u4f60\u7684\u6a21\u578b\u662f: PyTorch \u6a21\u578b Tensorflow \u6a21\u578b \u4efb\u4f55\u673a\u5668\u5b66\u4e60\u6a21\u578b \u4f60\u81ea\u5df1\u7684\u4ee3\u7801\uff0c\u7b97\u6cd5 \u751a\u81f3\u53ea\u662f\u4e00\u4e2a\u7b80\u7b80\u5355\u5355\u7684\u51fd\u6570 \u53ea\u9700\u8981\u7b80\u5355\u7684\u6ce8\u518c\uff0c Pinferencia \u5c31\u662f\u7acb\u523b\u4e0a\u7ebf\u5b83\u6765\u9884\u6d4b\uff0c\u5982\u4f60\u9884\u671f\uff0c\u6ca1\u6709\u60ca\u5413\u3002 \u7b80\u5355\uff0c\u4e14\u5f3a\u5927 \u00b6 Pinferencia \u81f4\u529b\u4e8e\u6210\u4e3a\u6700\u7b80\u5355\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u90e8\u7f72\u5de5\u5177\u3002 \u90e8\u7f72\u6a21\u578b\u4ece\u6765\u6ca1\u6709\u5982\u6b64\u7b80\u5355\u3002 \u5982\u679c\u4f60\u60f3\uff1a \u627e\u5230\u4e00\u4e2a\u7b80\u5355\u4e14\u53ef\u9760\u7684\u65b9\u6cd5\u6765\u4e0a\u7ebf\u4f60\u7684\u6a21\u578b \u7528\u6700\u5c11\u7684\u4ee3\u7801\uff0c\u638c\u63a7\u4f60\u7684\u670d\u52a1 \u8131\u79bb\u90a3\u4e9b\u91cd\u91cf\u7ea7\u3001\u800c\u5f88\u591a\u529f\u80fd\u4f60\u6839\u672c\u4e0d\u5728\u4e4e\u7684\u5de5\u5177\u548c\u5e73\u53f0 \u90a3\u4e48\uff0c\u4f60\u6765\u5bf9\u5730\u65b9\u4e86","title":"Pinferencia\u6709\u4f55\u4e0d\u540c?"},{"location":"rc/pinferencia-is-different/#pinferencia","text":"","title":"Pinferencia \u6709\u4ec0\u4e48\u4e0d\u540c?"},{"location":"rc/pinferencia-is-different/#_1","text":"\u66f4\u51c6\u786e\u7684\u8bf4\uff0c\u4e0d\u540c\u5e76\u4e0d\u91cd\u8981\uff0c\u91cd\u8981\u7684\u662f\u66f4\u76f4\u63a5\uff0c\u66f4\u7b80\u5355\uff0c\u66f4\u7b26\u5408\u4f60\u7684\u9884\u671f\u3002 \u4f60\u73b0\u5728\u662f\u5982\u4f55\u4e0a\u7ebf\u6a21\u578b\u7684? \u4f60\u662f\u4e0d\u662f\u82b1\u4e86\u5f88\u591a\u65f6\u95f4\uff0c\u5199\u4ee3\u7801\uff0c\u4fdd\u5b58\u6587\u4ef6\uff0c\u4e3a\u4e86\u6ee1\u8db3\u90a3\u4e9b\u90e8\u7f72\u5de5\u5177\u7684\u8981\u6c42\u3002 \u5bf9\uff0c\u4f60\u8fd8\u82b1\u4e86\u5f88\u591a\u65f6\u95f4\u53bb\u7406\u89e3\u8fd9\u4e9b\u8981\u6c42\uff0c\u5f88\u591a\u65f6\u95f4\u77e5\u9053\u600e\u4e48\u505a\u662f\u6b63\u786e\u7684\u3002 \u4e0d\u8fc7\uff0c\u529f\u592b\u4e0d\u8d1f\u6709\u5fc3\u4eba\uff0c\u4f60\u8fd8\u662f\u641e\u5b9a\u4e86\u3002 \u597d\u666f\u4e0d\u957f\uff0c\u8fc7\u4e86\u5927\u534a\u5e74\uff0c\u53c8\u6709\u65b0\u7684\uff0c\u66f4\u590d\u6742\u7684\u6a21\u578b\u8981\u90e8\u7f72\uff0c\u5929\u554a\uff0c\u600e\u4e48\u529e\uff1f \u4f60\u73b0\u5728\u5728\u60f3\u4ec0\u4e48?","title":"\u4e0d\u540c?"},{"location":"rc/pinferencia-is-different/#pinferencia_1","text":"\u4f60\u4e0d\u7528\u518d\u62c5\u5fc3\u8fd9\u4e9b\u3002\u4f60\u53ea\u9700\u8981\u8fd8\u662f\u7528\u4f60\u81ea\u5df1\u7684\u6a21\u578b\uff0c\u8fd8\u662f\u7528\u4f60\u81ea\u5df1\u7684\u4ee3\u7801\u3002 \u65e0\u6240\u8c13\u4f60\u7684\u6a21\u578b\u662f: PyTorch \u6a21\u578b Tensorflow \u6a21\u578b \u4efb\u4f55\u673a\u5668\u5b66\u4e60\u6a21\u578b \u4f60\u81ea\u5df1\u7684\u4ee3\u7801\uff0c\u7b97\u6cd5 \u751a\u81f3\u53ea\u662f\u4e00\u4e2a\u7b80\u7b80\u5355\u5355\u7684\u51fd\u6570 \u53ea\u9700\u8981\u7b80\u5355\u7684\u6ce8\u518c\uff0c Pinferencia \u5c31\u662f\u7acb\u523b\u4e0a\u7ebf\u5b83\u6765\u9884\u6d4b\uff0c\u5982\u4f60\u9884\u671f\uff0c\u6ca1\u6709\u60ca\u5413\u3002","title":"\u6709\u4e86 Pinferencia"},{"location":"rc/pinferencia-is-different/#_2","text":"Pinferencia \u81f4\u529b\u4e8e\u6210\u4e3a\u6700\u7b80\u5355\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u90e8\u7f72\u5de5\u5177\u3002 \u90e8\u7f72\u6a21\u578b\u4ece\u6765\u6ca1\u6709\u5982\u6b64\u7b80\u5355\u3002 \u5982\u679c\u4f60\u60f3\uff1a \u627e\u5230\u4e00\u4e2a\u7b80\u5355\u4e14\u53ef\u9760\u7684\u65b9\u6cd5\u6765\u4e0a\u7ebf\u4f60\u7684\u6a21\u578b \u7528\u6700\u5c11\u7684\u4ee3\u7801\uff0c\u638c\u63a7\u4f60\u7684\u670d\u52a1 \u8131\u79bb\u90a3\u4e9b\u91cd\u91cf\u7ea7\u3001\u800c\u5f88\u591a\u529f\u80fd\u4f60\u6839\u672c\u4e0d\u5728\u4e4e\u7684\u5de5\u5177\u548c\u5e73\u53f0 \u90a3\u4e48\uff0c\u4f60\u6765\u5bf9\u5730\u65b9\u4e86","title":"\u7b80\u5355\uff0c\u4e14\u5f3a\u5927"},{"location":"rc/restapi/","text":"REST API \u00b6 \u6982\u8ff0 \u00b6 Pinferencia \u6709\u4e24\u4e2a\u5185\u7f6e API\uff1a \u9ed8\u8ba4 API Kserve API from pinferencia import Server service = Server () # or service = Server ( api = \"default\" ) from pinferencia import Server service = Server ( api = \"kserve\" ) \u60a8\u73b0\u5728\u6b63\u5728\u4f7f\u7528\u5176\u4ed6\u6a21\u578b\u670d\u52a1\u5de5\u5177\u5417\uff1f \u5982\u679c\u60a8\u8fd8\u4f7f\u7528\u5176\u4ed6\u6a21\u578b\u670d\u52a1\u5de5\u5177\uff0c\u4ee5\u4e0b\u662f\u8fd9\u4e9b\u5de5\u5177\u652f\u6301\u7684 Kserve API \u7248\u672c\uff1a \u540d\u79f0 API Pinferencia Kserve V1 & V2 TF Serving Kserve V1 TorchServe Kserve V1 or V2 Triton Kserve V2 KServe Kserve V1 \u6ca1\u6709\u75db\u82e6\uff0c\u53ea\u6709\u6536\u83b7 \u00b6 \u5982\u4f60\u770b\u5230\u7684 \u60a8\u53ef\u4ee5\u5728 Pinferencia \u548c\u5176\u4ed6\u5de5\u5177\u4e4b\u95f4\u5207\u6362\uff0c\u51e0\u4e4e\u65e0\u9700\u5728\u5ba2\u6237\u7aef\u66f4\u6539\u4ee3\u7801\u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528 Pinferencia \u8fdb\u884c\u539f\u578b\u8bbe\u8ba1\u548c\u5ba2\u6237\u7aef\u6784\u5efa\uff0c\u7136\u540e\u5728\u751f\u4ea7\u4e2d\u4f7f\u7528\u5176\u4ed6\u5de5\u5177\u3002 \u60a8\u53ef\u4ee5\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u5c06 Pinferencia \u4e0e\u5177\u6709\u76f8\u540c API \u96c6\u7684\u5176\u4ed6\u5de5\u5177\u4e00\u8d77\u4f7f\u7528\u3002 \u5982\u679c\u60a8\u8981\u4ece Kserve V1 \u5207\u6362\u5230 Kserve V2\uff0c\u5e76\u4e14\u5728\u8fc7\u6e21\u671f\u95f4\u9700\u8981\u652f\u6301\u8fd9\u4e24\u8005\u7684\u670d\u52a1\u5668\uff0c\u90a3\u4e48\u60a8\u5c31\u53ef\u4ee5\u4f7f\u7528 Pinferencia \u3002 \u6240\u4ee5\uff0c\u6ca1\u6709\u75db\u82e6\uff0c\u53ea\u6709\u6536\u83b7\u3002 \u9ed8\u8ba4 API \u00b6 Path Method Summary /v1/healthz GET \u670d\u52a1\u5065\u5eb7 /v1/models GET \u6a21\u578b\u5217\u8868 /v1/models/{model_name} GET \u6a21\u578b\u7248\u672c\u5217\u8868 /v1/models/{model_name}/ready GET \u6a21\u578b\u662f\u5426\u53ef\u7528 /v1/models/{model_name}/versions/{version_name}/ready GET \u6a21\u578b\u7248\u672c\u662f\u5426\u53ef\u7528 /v1/models/{model_name}/load POST \u52a0\u8f7d\u6a21\u578b /v1/models/{model_name}/versions/{version_name}/load POST \u52a0\u8f7d\u7248\u672c /v1/models/{model_name}/unload POST \u5378\u8f7d\u6a21\u578b /v1/models/{model_name}/versions/{version_name}/unload POST \u5378\u8f7d\u7248\u672c /v1/models/{model_name}/predict POST \u6a21\u578b\u9884\u6d4b /v1/models/{model_name}/versions/{version_name}/predict POST \u6a21\u578b\u7248\u672c\u9884\u6d4b Kserve API \u00b6 Path Method Summary /v1/healthz GET \u670d\u52a1\u5065\u5eb7 /v1/models GET \u6a21\u578b\u5217\u8868 /v1/models/{model_name} GET \u6a21\u578b\u7248\u672c\u5217\u8868 /v1/models/{model_name}/ready GET \u6a21\u578b\u662f\u5426\u53ef\u7528 /v1/models/{model_name}/versions/{version_name}/ready GET \u6a21\u578b\u7248\u672c\u662f\u5426\u53ef\u7528 /v1/models/{model_name}/load POST \u52a0\u8f7d\u6a21\u578b /v1/models/{model_name}/versions/{version_name}/load POST \u52a0\u8f7d\u7248\u672c /v1/models/{model_name}/unload POST \u5378\u8f7d\u6a21\u578b /v1/models/{model_name}/versions/{version_name}/unload POST \u5378\u8f7d\u7248\u672c /v1/models/{model_name}/infer POST \u6a21\u578b\u9884\u6d4b /v1/models/{model_name}/versions/{version_name}/infer POST \u6a21\u578b\u7248\u672c\u9884\u6d4b /v2/healthz GET \u670d\u52a1\u5065\u5eb7 /v2/models GET \u6a21\u578b\u5217\u8868 /v2/models/{model_name} GET \u6a21\u578b\u7248\u672c\u5217\u8868 /v2/models/{model_name}/ready GET \u6a21\u578b\u662f\u5426\u53ef\u7528 /v2/models/{model_name}/versions/{version_name}/ready GET \u6a21\u578b\u7248\u672c\u662f\u5426\u53ef\u7528 /v2/models/{model_name}/load POST \u52a0\u8f7d\u6a21\u578b /v2/models/{model_name}/versions/{version_name}/load POST \u52a0\u8f7d\u7248\u672c /v2/models/{model_name}/unload POST \u5378\u8f7d\u6a21\u578b /v2/models/{model_name}/versions/{version_name}/unload POST \u5378\u8f7d\u7248\u672c /v2/models/{model_name}/infer POST \u6a21\u578b\u9884\u6d4b /v2/models/{model_name}/versions/{version_name}/infer POST \u6a21\u578b\u7248\u672c\u9884\u6d4b","title":"REST API"},{"location":"rc/restapi/#rest-api","text":"","title":"REST API"},{"location":"rc/restapi/#_1","text":"Pinferencia \u6709\u4e24\u4e2a\u5185\u7f6e API\uff1a \u9ed8\u8ba4 API Kserve API from pinferencia import Server service = Server () # or service = Server ( api = \"default\" ) from pinferencia import Server service = Server ( api = \"kserve\" ) \u60a8\u73b0\u5728\u6b63\u5728\u4f7f\u7528\u5176\u4ed6\u6a21\u578b\u670d\u52a1\u5de5\u5177\u5417\uff1f \u5982\u679c\u60a8\u8fd8\u4f7f\u7528\u5176\u4ed6\u6a21\u578b\u670d\u52a1\u5de5\u5177\uff0c\u4ee5\u4e0b\u662f\u8fd9\u4e9b\u5de5\u5177\u652f\u6301\u7684 Kserve API \u7248\u672c\uff1a \u540d\u79f0 API Pinferencia Kserve V1 & V2 TF Serving Kserve V1 TorchServe Kserve V1 or V2 Triton Kserve V2 KServe Kserve V1","title":"\u6982\u8ff0"},{"location":"rc/restapi/#_2","text":"\u5982\u4f60\u770b\u5230\u7684 \u60a8\u53ef\u4ee5\u5728 Pinferencia \u548c\u5176\u4ed6\u5de5\u5177\u4e4b\u95f4\u5207\u6362\uff0c\u51e0\u4e4e\u65e0\u9700\u5728\u5ba2\u6237\u7aef\u66f4\u6539\u4ee3\u7801\u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528 Pinferencia \u8fdb\u884c\u539f\u578b\u8bbe\u8ba1\u548c\u5ba2\u6237\u7aef\u6784\u5efa\uff0c\u7136\u540e\u5728\u751f\u4ea7\u4e2d\u4f7f\u7528\u5176\u4ed6\u5de5\u5177\u3002 \u60a8\u53ef\u4ee5\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u5c06 Pinferencia \u4e0e\u5177\u6709\u76f8\u540c API \u96c6\u7684\u5176\u4ed6\u5de5\u5177\u4e00\u8d77\u4f7f\u7528\u3002 \u5982\u679c\u60a8\u8981\u4ece Kserve V1 \u5207\u6362\u5230 Kserve V2\uff0c\u5e76\u4e14\u5728\u8fc7\u6e21\u671f\u95f4\u9700\u8981\u652f\u6301\u8fd9\u4e24\u8005\u7684\u670d\u52a1\u5668\uff0c\u90a3\u4e48\u60a8\u5c31\u53ef\u4ee5\u4f7f\u7528 Pinferencia \u3002 \u6240\u4ee5\uff0c\u6ca1\u6709\u75db\u82e6\uff0c\u53ea\u6709\u6536\u83b7\u3002","title":"\u6ca1\u6709\u75db\u82e6\uff0c\u53ea\u6709\u6536\u83b7"},{"location":"rc/restapi/#api","text":"Path Method Summary /v1/healthz GET \u670d\u52a1\u5065\u5eb7 /v1/models GET \u6a21\u578b\u5217\u8868 /v1/models/{model_name} GET \u6a21\u578b\u7248\u672c\u5217\u8868 /v1/models/{model_name}/ready GET \u6a21\u578b\u662f\u5426\u53ef\u7528 /v1/models/{model_name}/versions/{version_name}/ready GET \u6a21\u578b\u7248\u672c\u662f\u5426\u53ef\u7528 /v1/models/{model_name}/load POST \u52a0\u8f7d\u6a21\u578b /v1/models/{model_name}/versions/{version_name}/load POST \u52a0\u8f7d\u7248\u672c /v1/models/{model_name}/unload POST \u5378\u8f7d\u6a21\u578b /v1/models/{model_name}/versions/{version_name}/unload POST \u5378\u8f7d\u7248\u672c /v1/models/{model_name}/predict POST \u6a21\u578b\u9884\u6d4b /v1/models/{model_name}/versions/{version_name}/predict POST \u6a21\u578b\u7248\u672c\u9884\u6d4b","title":"\u9ed8\u8ba4 API"},{"location":"rc/restapi/#kserve-api","text":"Path Method Summary /v1/healthz GET \u670d\u52a1\u5065\u5eb7 /v1/models GET \u6a21\u578b\u5217\u8868 /v1/models/{model_name} GET \u6a21\u578b\u7248\u672c\u5217\u8868 /v1/models/{model_name}/ready GET \u6a21\u578b\u662f\u5426\u53ef\u7528 /v1/models/{model_name}/versions/{version_name}/ready GET \u6a21\u578b\u7248\u672c\u662f\u5426\u53ef\u7528 /v1/models/{model_name}/load POST \u52a0\u8f7d\u6a21\u578b /v1/models/{model_name}/versions/{version_name}/load POST \u52a0\u8f7d\u7248\u672c /v1/models/{model_name}/unload POST \u5378\u8f7d\u6a21\u578b /v1/models/{model_name}/versions/{version_name}/unload POST \u5378\u8f7d\u7248\u672c /v1/models/{model_name}/infer POST \u6a21\u578b\u9884\u6d4b /v1/models/{model_name}/versions/{version_name}/infer POST \u6a21\u578b\u7248\u672c\u9884\u6d4b /v2/healthz GET \u670d\u52a1\u5065\u5eb7 /v2/models GET \u6a21\u578b\u5217\u8868 /v2/models/{model_name} GET \u6a21\u578b\u7248\u672c\u5217\u8868 /v2/models/{model_name}/ready GET \u6a21\u578b\u662f\u5426\u53ef\u7528 /v2/models/{model_name}/versions/{version_name}/ready GET \u6a21\u578b\u7248\u672c\u662f\u5426\u53ef\u7528 /v2/models/{model_name}/load POST \u52a0\u8f7d\u6a21\u578b /v2/models/{model_name}/versions/{version_name}/load POST \u52a0\u8f7d\u7248\u672c /v2/models/{model_name}/unload POST \u5378\u8f7d\u6a21\u578b /v2/models/{model_name}/versions/{version_name}/unload POST \u5378\u8f7d\u7248\u672c /v2/models/{model_name}/infer POST \u6a21\u578b\u9884\u6d4b /v2/models/{model_name}/versions/{version_name}/infer POST \u6a21\u578b\u7248\u672c\u9884\u6d4b","title":"Kserve API"}]}